<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>[How to] 2. Hexo+Github+NexT</title>
    <url>/howto2/</url>
    <content><![CDATA[<p>果然还是要从底层去理解，不然上层建筑一换，不结实的地基就很有大问题。 <a id="more"></a></p>
<h2 id="博客迁移的心路历程">1. 博客迁移的心路历程</h2>
<ul>
<li><p>最近因为想魔改Wordpress的主题，又不懂php代码，只能加上一些插件，导致后台运行有点儿慢。又想到服务器要续费了，过去半年服务器就挂了个网站不太值。还是一狠心换到Github Page上了。</p></li>
<li><p>这也导致了个问题，就是数学公式的写作。在用Wordpress的时候，自己是用Typora写md文件，然后导出成html文件，复制到Wordpress的对应格子里。用Hexo，原理是一样的，从md到html。但是因为Hexo原生转译器过于简陋+Typora过于强大，自己的一个糟糕的书写格式使得我得把文章再编辑一边才能正常显示。</p>
<ul>
<li>当然Typora面对字数较多时（5000+）就太弱小</li>
</ul></li>
<li><p>当然Hexo作为静态网页还是有些麻烦，一些功能需要修改。当然和魔改Wordpress比还是简单一些。等某一天有自己的服务器的时候，可能还是会回去。</p></li>
<li><p>之前觉得自己还是蛮喜欢做不需要脑子的事情去消磨时间，但是重新编辑文章真是痛苦到我了。人还是多用脑子比较好。</p></li>
</ul>
<h2 id="基本配置">2. 基本配置</h2>
<blockquote>
<p>Hexo: v5.3.0</p>
<p>npm: v6.14.9</p>
<p>NexT: v8.0</p>
</blockquote>
<h2 id="功能与优化">3. 功能与优化</h2>
<p>:green_book:：已完成</p>
<p>:open_book:：未完成</p>
<h3 id="数学公式的转译">数学公式的转译</h3>
<h3 id="字数统计">字数统计</h3>
<h3 id="讨论">讨论</h3>
<h3 id="green_book去除table-of-content的自动编号">:green_book:去除Table of Content的自动编号</h3>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">toc:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Automatically add list number to toc.</span></span><br><span class="line">  <span class="attr">number:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h3 id="green_book菜单栏添加自定义菜单">:green_book:菜单栏添加自定义菜单</h3>
<ol type="1">
<li>tags, categories 要设置<code>type: tags</code>, <code>type: categories</code></li>
<li>tagcloud字体设置在<code>\source\ _data\styles.styl, line48</code></li>
</ol>
<h3 id="已运行的时间">已运行的时间</h3>
<h3 id="文末添加结束语">文末添加结束语</h3>
<h3 id="green_book邮件链接">:green_book:邮件链接</h3>
<p>在邮件地址前要加上mailto:，点进去才是期望效果。</p>
<h3 id="添加emoji表情">添加Emoji表情</h3>
<h3 id="green_book修改新建文章的模板-1">:green_book:修改新建文章的模板 [1]</h3>
<p>修改<code>\scaffolds\post.md</code>文件 <code>hexo n post 'new post'</code>创建新文章</p>
<h3 id="标签云-tag-cloud特效">标签云 tag cloud特效</h3>
<h3 id="green_book文章统一添加全文结束语-2">:green_book:文章统一添加全文结束语 [2]</h3>
<ol type="1">
<li><code>\themes\next\layout\_macro</code> 中新建 <code>passage-end-tag.njk</code> 文件</li>
<li><code>\themes\next\layout\_macro\post.njk</code>文件中，在post-body 之后，post-footer 之前<code>Line 70</code>添加代码。</li>
</ol>
<h3 id="绑定自己域名">绑定自己域名</h3>
<ol type="1">
<li>获取域名</li>
<li>解析域名</li>
<li>在<code>\source\</code>底下新建<code>CNAME</code>文件。填写上域名。
<ul>
<li>填写WWW.{}.{}和去掉www是不同的。添加上www。</li>
</ul></li>
</ol>
<h2 id="reference">Reference</h2>
<ol type="1">
<li><a href="https://blog.csdn.net/tuckenough/article/details/107383201">hexo theme next7.8 主题美化</a></li>
<li><a href="https://segmentfault.com/a/1190000009544924">hexo的next主题个性化配置教程</a></li>
</ol>
]]></content>
      <categories>
        <category>How to</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Hexo-NexT</tag>
      </tags>
  </entry>
  <entry>
    <title>[统计数学方法] 8. 提升方法 Boosting</title>
    <url>/slm008/</url>
    <content><![CDATA[<p>三个臭皮匠，顶过诸葛亮。 <a id="more"></a></p>
<h2 id="reference">Reference</h2>
<p><a href="https://www.bilibili.com/video/BV1Cs411c7Zt?p=1">【报告】Boosting 25年（2014周志华）</a></p>
<h2 id="提升方法的基本思路">1. 提升方法的基本思路</h2>
<p>臭皮匠易得，诸葛亮是少有的。当我们有3个臭皮匠的时候也要考虑一下到底能不能变成一个诸葛亮。这就是提升方法的理论基础。</p>
<h3 id="强可学习与弱可学习">1.1 强可学习与弱可学习</h3>
<blockquote>
<p>定义1.01（强弱学习） ：在概率近似正确(probably approximately correct, PAC) 学习的框架中， 一个概念（ 一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的； 一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。</p>
</blockquote>
<ul>
<li>:book: PAC</li>
</ul>
<blockquote>
<p>定理1.01（强弱学习的关系） 在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。</p>
</blockquote>
<ul>
<li>:book: Proof</li>
</ul>
<p><strong>提升步骤</strong>：给定一个训练数据集，反复使用弱学习算法，得到一系列的弱分类器，把这些弱分类器组合成一个强分类器。<strong>通常我们通过调整训练数据的概率分布（权值分布），调用同一个弱学习算法来学习一系列的弱分类器。</strong></p>
<p><strong>问题</strong>：</p>
<ol type="1">
<li>如何在每一轮中改变训练数据的概率分布？
<ul>
<li>AdaBoost：提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。使得误分类的数据得到后一轮弱分类器更多的关注。使得分类问题被<strong>分而治之</strong>。</li>
</ul></li>
<li>如何把一系列弱分类器组合成一个强分类器？
<ul>
<li>AdaBoost：加权多数的表决方法。即加大分类误差率小的弱分类器的权值，使其在表决中起较人的作川；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</li>
</ul></li>
</ol>
<h3 id="adaboost算法">1.2 AdaBoost算法</h3>
<blockquote>
<p>算法1.1（AdaBoost）</p>
<p>输入：训练数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中 <span class="math inline">\(x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}, y_{i} \in\)</span> <span class="math inline">\(\mathcal{Y}=\{-1,+1\} ;\)</span> 弱学习算法;</p>
<p>输出：最终分类器 <span class="math inline">\(G(x)\)</span> 。</p>
<p>（1）初始化训练数据的权值分布 <span class="math display">\[
\begin{align}
D_{1}=\left(w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N
\end{align}
\]</span> （2）对 <span class="math inline">\(m=1,2, \cdots, M\)</span></p>
<p><span class="math inline">\(\quad\)</span>（a）使用具有权值分布 <span class="math inline">\(D_{m}\)</span> 的训练数据集学习，得到基本分类器 <span class="math display">\[
\begin{align}
G_{m}(x): \mathcal{X} \rightarrow\{-1,+1\}
\end{align}
\]</span> <span class="math inline">\(\quad\)</span>（b）计算 <span class="math inline">\(G_{m}(x)\)</span> 在训练数据集上的分类误差率 <span class="math display">\[
\begin{align}
e_{m}&amp;=\sum_{i=1}^{N} P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)\\
&amp;=\sum_{G_{m}\left(x_{i}\right) \neq y_{i}} w_{m i}
\end{align}
\]</span> <span class="math inline">\(\quad\)</span>（c）计算 <span class="math inline">\(G_{m}(x)\)</span> 的系数 <span class="math display">\[
\begin{align}
\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}
\end{align}
\]</span> <span class="math inline">\(\quad\)</span><span class="math inline">\(\quad\)</span><span class="math inline">\(\quad\)</span>这里的对数是自然对数。</p>
<p><span class="math inline">\(\quad\)</span>（d）更新训练数据集的权值分布 <span class="math display">\[
\begin{align}
D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right) \\
w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \quad i=1,2, \cdots, N
\end{align}
\]</span> <span class="math inline">\(\quad\)</span><span class="math inline">\(\quad\)</span><span class="math inline">\(\quad\)</span>这里， <span class="math inline">\(Z_{m}\)</span> 是规范化因子 <span class="math display">\[
\begin{align}
Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)
\end{align}
\]</span> <span class="math inline">\(\quad\)</span><span class="math inline">\(\quad\)</span><span class="math inline">\(\quad\)</span>它使 <span class="math inline">\(D_{m+1}\)</span> 成为一个概率分布。</p>
<p>（3）构建基本分类器的线性组合 <span class="math display">\[
\begin{align}
f(x)=\sum_{m=1}^{M} \alpha_{m} G_{m}(x)
\end{align}
\]</span> <span class="math inline">\(\quad\)</span><span class="math inline">\(\quad\)</span>得到最终分类器 <span class="math display">\[
\begin{align}
G(x) &amp;=\operatorname{sign}(f(x)) \\
&amp;=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
\end{align}
\]</span></p>
</blockquote>
<ul>
<li><p>（1）假设原始数据的每一个训练样本在基本分类器的学习中作用相同</p></li>
<li><p>（2）一共进行<span class="math inline">\(m=M\)</span>次学习。（如何确定<span class="math inline">\(M\)</span>？没有错误的分类or因为没有过拟合所以选个偏大的数？）</p>
<ul>
<li><p>（a）通过<span class="math inline">\(D_m\)</span>加权得到基本分类器<span class="math inline">\(G_m(x)\)</span></p>
<ul>
<li>如何学习？</li>
</ul></li>
<li><p><strong>（b）<span class="math inline">\(G_m(x)\)</span>在加权的训练数据集的分类误差率<span class="math inline">\(e_m\)</span>是被<span class="math inline">\(G_m(x)\)</span>误分类样本的权值之和</strong></p></li>
<li><p>（c）用分类误差率<span class="math inline">\(e_m\)</span>计算基本分类器<span class="math inline">\(G_m(x)\)</span>的系数<span class="math inline">\(\alpha_m\)</span>，用来表示<span class="math inline">\(G_m(x)\)</span>在最终分类器中的重要程度。</p></li>
<li><p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-8%20function.png" alt="slm-8 function" style="zoom:50%;" /></p>
<p>用图可得，当 <span class="math inline">\(e_{m} \leqslant \frac{1}{2}\)</span> 时， <span class="math inline">\(\alpha_{m} \geqslant 0\)</span>，并且 <span class="math inline">\(\alpha_{m}\)</span> 随着 <span class="math inline">\(e_{m}\)</span> 的减小而增大。因此，<strong>分类误差率越小的基本分类器在最终分类器中的作用越大</strong>。</p></li>
<li><p>（d）即问题1的解答做法。等式（7）可以写成 <span class="math display">\[
w_{m+1, i}=\left\{\begin{array}{ll}
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, &amp; G_{m}\left(x_{i}\right)=y_{i} \\
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, &amp; G_{m}\left(x_{i}\right) \neq y_{i}
\end{array}\right.\nonumber
\]</span> <strong>误分类样本的权值增大<span class="math inline">\(\mathrm{e}^{2 \alpha_{m}}=\frac{1-e_{m}}{e_{m}}\)</span>倍，正确分类样本的权值减小</strong>。在下一轮中，误分类样本起到更大的作用。<span class="math inline">\(Z_m\)</span>归一化。</p></li>
</ul></li>
<li><p>（3）即问题2的解答做法。所有<span class="math inline">\(\alpha_m\)</span>之和不为一。<span class="math inline">\(f(x)\)</span>符号觉得实例<span class="math inline">\(x\)</span>的类，<span class="math inline">\(|f(x)|\)</span>表示分类的确信度。</p></li>
</ul>
<p>:book: （d）中增大倍数，权值变化与系数<span class="math inline">\(\alpha_m\)</span>构造的关系。</p>
<h3 id="book-1.3-adaboost-example">:book: 1.3 AdaBoost example</h3>
<h2 id="adaboost算法的训练误差分析">2. AdaBoost算法的训练误差分析</h2>
<p>AdaBoost 最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。</p>
]]></content>
      <categories>
        <category>study</category>
        <category>《统计学习方法》</category>
      </categories>
      <tags>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>[统计数学方法] 7. 支持向量机 SVM</title>
    <url>/slm007/</url>
    <content><![CDATA[<p>支持向量机(Support Vector Machines)是一种<strong>二元分类</strong>模型。<strong>基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。</strong>基本模型是<strong>特征空间上的间隔最大</strong>的线性分类器，区别于感知机。学习策略为间隔最大化，可化为求解凸二次规划convex quadratic programming。学习算法为求解凸二次规划的最优算法。 <a id="more"></a></p>
<h2 id="线性可分支持向量机与硬间隔最大化">1. 线性可分支持向量机与硬间隔最大化</h2>
<h3 id="model">1.1 Model</h3>
<blockquote>
<p>定义 7.1 (线性可分支持向量机 ) <span class="math inline">\(\quad\)</span> 给定<strong>线性可分</strong>训练数据集 <span class="math display">\[
\begin{align}
T=&amp;\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}), \cdots,\left(x_{N}, y_{N}\right)\right\}\\
x_{i} \in \mathcal{X}=&amp;\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N
\end{align}
\]</span> 通过<strong>间隔最大化或等价地求解相应的凸二次规划问题</strong>学习得到的分离超平面为 <span class="math display">\[
w^{*} \cdot x+b^{*}=0
\]</span> 以及相应的分类决策函数 <span class="math display">\[
f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\]</span> 称为线性可分支持向量机。</p>
</blockquote>
<ul>
<li>对线性可分数据集，存在无穷超平面分离数据。感知机用误分类最小策略，得到的解无穷多个。SVM用间隔最大化策略，得到的解唯一。</li>
</ul>
<h3 id="间隔">1.2 间隔</h3>
<ol type="1">
<li><p>点到分离超平面的远近表示对分类预测的确信程度。</p></li>
<li><p><span class="math inline">\(y_{pre}\)</span>与<span class="math inline">\(\hat{y}\)</span>的符号一致与否表示分类预测的准确性。</p></li>
</ol>
<h4 id="函数间隔-functional-margin">1.2.1 函数间隔 Functional Margin</h4>
<blockquote>
<p>定义 7.2 (函数间隔) <span class="math inline">\(\quad\)</span> 对于给定的训练数据集 <span class="math inline">\(T\)</span> 和超平面 <span class="math inline">\((w, b),\)</span> 定义超平面<span class="math inline">\((w, b)\)</span> <strong>关于样本点 <span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span> 的函数间隔</strong>为 <span class="math display">\[
\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)
\]</span> 定义超平面 <span class="math inline">\((w, b)\)</span> <strong>关于训练数据集 <span class="math inline">\(T\)</span> 的函数间隔</strong>为超平面 <span class="math inline">\((w, b)\)</span> 对于 <span class="math inline">\(T\)</span> 中所有样本点 <span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span> 的函数间隔之最小值，即 <span class="math display">\[
\begin{align}
\hat{\gamma}=\min _{i=1, \cdots, N} \hat{\gamma}_{i}
\end{align}
\]</span></p>
</blockquote>
<ul>
<li><p>(3)表示分类的准确性与准确程度</p></li>
<li><p>对超平面<span class="math inline">\(w \cdot x+b=0\)</span>，成倍的改变<span class="math inline">\(w,\;b\)</span>不会改变该平面，但是会成倍的改变函数间隔，且<strong>倍数相等</strong></p></li>
</ul>
<h4 id="几何间隔-geometric-margin">1.2.2 几何间隔 Geometric Margin</h4>
<p>对分离超平面<span class="math inline">\(w \cdot x+b=0\)</span>的法向量<span class="math inline">\(w\)</span>进行正规化，使得<span class="math inline">\(||w&#39;||=\frac{w}{\|w\|}\)</span>（同时对<span class="math inline">\(b\)</span>也是）。</p>
<blockquote>
<p>定义 7.3 (几何间隔) <span class="math inline">\(\quad\)</span> 对于给定的训练数据集 <span class="math inline">\(T\)</span> 和超平面 <span class="math inline">\((w, b),\)</span> 定义超平面<span class="math inline">\((w, b)\)</span> 关于样本,点 <span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span> 的几何间隔为 <span class="math display">\[
\gamma_{i}=y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)
\]</span> 定义超平面 <span class="math inline">\((w, b)\)</span> 对于训练数据集 <span class="math inline">\(T\)</span> 的几何间隔为超平面 <span class="math inline">\((w, b)\)</span> 关千 <span class="math inline">\(T\)</span> 中所有样本点 <span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span> 的几何间隔之最小值，即 <span class="math display">\[
\gamma=\min _{i=1, \cdots, N} \gamma_{i}
\]</span></p>
</blockquote>
<ul>
<li>几何间隔不随参数的改变而改变</li>
</ul>
<h3 id="间隔最大化">1.3 间隔最大化</h3>
<ul>
<li>对线性可分数据集，线性可分分离超平面有无穷多个（即感知机），但几何间隔最大的分离超平面唯一。</li>
<li>对线性可分训练集，间隔最大化又称硬间隔最大化。</li>
<li>间隔最大化，即以充分大的确信度，对训练数据进行分类；也就是说，在正负实例分开的同时，对离超平面最近的点也能有足够大的确信度。</li>
</ul>
<h4 id="algorithm">1.3.1 Algorithm</h4>
<p>由几何间隔的定义可知，硬间隔最大化可以表示为 <span class="math display">\[
\begin{align}
\max _{w, b} &amp; \gamma \\
\text { s.t. } &amp; y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma&gt;0, \quad i=1,2, \cdots, N
\end{align}
\]</span> 由几何间隔和函数间隔的定义，(7)转化成 <span class="math display">\[
\begin{align}
\max _{u \cdot b}&amp; \frac{\hat{\gamma}}{\|w\|} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N
\end{align}
\]</span> 我们要求<span class="math inline">\(w,\;b\)</span>，使得<span class="math inline">\(\frac{\hat{\gamma}}{\|w\|}\)</span>最大化，在这个目标函数与约束条件中，<span class="math inline">\(\hat{y}\)</span>的取值对最后的超平面没有影响。于是我们设<span class="math inline">\(\hat{y}=1\)</span>，当成单位1。同时最大化<span class="math inline">\(\frac{1}{\|w\|}\)</span> 和最小化 <span class="math inline">\(\frac{1}{2}\|w\|^{2}\)</span>等价。(8)转化成 <span class="math display">\[
\begin{align}
\min _{w, b} &amp; \frac{1}{2}\|w\|^{2} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{align}
\]</span></p>
<ul>
<li>(8-9)为凸二次规划问题convex quadratic programming</li>
</ul>
<p>求出(8-9)的解<span class="math inline">\(w^*,\;b^*\)</span>，我们可以得出最大间隔分离超平面<span class="math inline">\(w^{*} \cdot x+b^{*}=0\)</span> 及分类决策函数<span class="math inline">\(f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)\)</span>，即线性可分向量机模型。</p>
<blockquote>
<p>算法 7.1 (线性可分支持向量机学习算法——最大间隔法)</p>
<p>输入: 线性可分训练数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}\)</span>，<span class="math inline">\(x_{i}\in \mathcal{X}=\mathbf{R}^{n}\)</span>,<span class="math inline">\(y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N\)</span></p>
<p>输出：最大间隔分离超平面和分类决策函数。</p>
<p>（1）构造并求解约束最优化问题: <span class="math display">\[
\begin{align}
\min _{w, b} &amp; \frac{1}{2}\|w\|^{2} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{align}
\]</span> 求得最优解 <span class="math inline">\(w^{*}, b^{*}\)</span> 。</p>
<p>（2）由此得到分离超平面: <span class="math display">\[
w^{*} \cdot x+b^{*}=0
\]</span> 分类决策函数 <span class="math display">\[
f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\]</span></p>
</blockquote>
<h4 id="解的存在性与唯一性">1.3.2 解的存在性与唯一性</h4>
<blockquote>
<p>定理 7.1 (最大间隔分离超平面的存在唯一性) <span class="math inline">\(\quad\)</span> 若训练数据集 <span class="math inline">\(T\)</span> 线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</p>
</blockquote>
<h4 id="支持向量-support-vector-间隔边界">1.3.3 支持向量 Support Vector &amp; 间隔边界</h4>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png" alt="image-20201216151045483" style="zoom:80%;" /></p>
<p>对线性可分数据集：</p>
<ul>
<li>支持向量：训练样本点中与分离超平面距离最近的<strong>实例们<span class="math inline">\((x_i,\;b_i)\)</span></strong>，使得约束调节(10)取等号。即位于<span class="math inline">\(H_{1,2}: w \cdot x+b=\pm1\)</span>上的两（或更多）点。</li>
<li>间隔边界：<span class="math inline">\(H_1\)</span>和<span class="math inline">\(H_2\)</span></li>
</ul>
<p><strong>此模型的结果只由这些少数个支持向量决定</strong>，故此得名。</p>
<h3 id="对偶算法-dual-problem">1.4 对偶算法 Dual Problem</h3>
<p>求解最优化问题(10)，我们可以把其当成原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。</p>
<ol type="1">
<li>对偶问题更容易求解。高维甚至无限维的最优化问题难求解。</li>
<li>引入核函数</li>
</ol>
<h4 id="对偶问题的导出">1.4.1 对偶问题的导出</h4>
<p>对于拘束最优化问题 <span class="math display">\[
\begin{align}
\min _{w, b} &amp; \frac{1}{2}\|w\|^{2} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{align}
\]</span> 的每一个约束条件，我们引入一个拉格朗日乘子 Lagrange multiplier <span class="math inline">\(\alpha_{i} \geqslant 0\)</span>, <span class="math inline">\(i=1,2, \cdots, N\)</span>，定义拉格朗日函数 Generalized Lagrange Function，<span class="math inline">\(\alpha=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}\)</span> 为拉格朗日乘子向量。 <span class="math display">\[
\begin{align}
L(w, b, \alpha)&amp;=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i}(y_{i}\left(w \cdot x_{i}+b\right)-1)\nonumber
\\&amp;=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}
\end{align}
\]</span> 带拘束最优化问题(12-13)可以变成无拘束优化问题(15-16) <span class="math display">\[
\begin{align}
\min _{w, b} \max _{\lambda} \mathcal{L}(w, b, \alpha) \\
\text { s.t. } \lambda_{i} \geqslant 0
\end{align}
\]</span></p>
<p>由强对偶关系，拘束问题(15-16)可以化为无拘束优化问题 <span class="math display">\[
\begin{align}
\max _{\lambda} \min _{w, b} \mathcal{L}(w, b, \alpha) \\
\text { s.t. } \lambda_{i} \geqslant 0
\end{align}
\]</span></p>
<p>由上面的拉格朗日（强）对偶性可得，原始问题(12-13)的对偶问题是极大极小问题(17-18)</p>
<ul>
<li>详细推导见：【拉格朗日对偶，等价对偶以及KKT条件】（文件没保存，有缘再续）</li>
</ul>
<h4 id="对偶问题的计算">1.4.2 对偶问题的计算</h4>
<p><strong>1.4.2.1: 求<span class="math inline">\(\min _{w, b} L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}\)</span></strong></p>
<p>计算<span class="math inline">\(\min _{w, b} L(w, b, \alpha)\)</span>对变量<span class="math inline">\(w,\;b\)</span>的偏导为0 <span class="math display">\[
\begin{align}
\nabla_{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0 \\
\nabla_{b} L(w, b, \alpha)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0\nonumber\\
\end{align}
\]</span> 得 <span class="math display">\[
\begin{align}
w&amp;=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}\\
\sum_{i=1}^{N} \alpha_{i} y_{i}&amp;=0
\end{align}
\]</span> 把(20-21)带入到(14)，得 <span class="math display">\[
\begin{aligned}
L(w, b, \alpha) &amp;=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\
&amp;=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{aligned}
\]</span> 得到结果 <span class="math display">\[
\min _{w, b} L(w, b, \alpha)=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\]</span> <strong>求<span class="math inline">\(\min _{w, b} L(w, b, \alpha) \text { 对 } \alpha \text { 的极大 }\)</span></strong> <span class="math display">\[
\begin{aligned}
\max _{\alpha} &amp; -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{aligned}
\]</span> 调整符号，得与原始问题等价的对偶最优化问题 <span class="math display">\[
\begin{align}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{align}
\]</span></p>
<ul>
<li><span class="math inline">\(\sum_{i=1}^{N} \alpha_{i} y_{i}=0\)</span>意味着很大部分的<span class="math inline">\(\alpha=0\)</span>，即决定超平面的只是很小一部分的支持向量。</li>
</ul>
<h4 id="对偶问题的解">1.4.3 对偶问题的解</h4>
<blockquote>
<p>定理 7.2 设 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{l}^{*}\right)^{\mathrm{T}}\)</span> 是对偶最优化问题的解，则存在下标 <span class="math inline">\(j,\)</span> 使得 <span class="math inline">\(\alpha_{j}^{*}&gt;0,\)</span> 并可按下式求得原始最优化问题的解 <span class="math inline">\(w^{*}, b^{*}\)</span> : <span class="math display">\[
\begin{align}
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)
\end{align}
\]</span></p>
</blockquote>
<p>分离超平面为<span class="math inline">\(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x \cdot x_{i}\right)+b^{*}=0\)</span></p>
<p>分类决策函数为<span class="math inline">\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x \cdot x_{i}\right)+b^{*}\right)\)</span></p>
<h4 id="algorithm-1">1.4.4 Algorithm</h4>
<blockquote>
<p>算法 7.2 (线性可分支持向量机学习算法)</p>
<p>输入: 线性可分训练集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中 <span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}\)</span>, <span class="math inline">\(y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N\)</span></p>
<p>输出：分离超平面和分类决策函数。</p>
<p>（1）构造并求解约束最优化问题 <span class="math display">\[
\begin{align}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{align}
\]</span> 求得最优解 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}\)</span> 。 (2) 计算 <span class="math display">\[
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}
\]</span> 并选择 <span class="math inline">\(\alpha^{*}\)</span> 的一个正分量 <span class="math inline">\(\alpha_{j}^{*}&gt;0,\)</span> 计算 <span class="math display">\[
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)
\]</span> （3）求得分离超平面 <span class="math display">\[
w^{*} \cdot x+b^{*}=0
\]</span> 分类决策函数: <span class="math display">\[
f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\]</span></p>
</blockquote>
<p><br><br></p>
<h2 id="线性支持向量机与软间隔最大化">2. 线性支持向量机与软间隔最大化</h2>
<p>对线性不可分数据集，存在一些特异点（outlier）使得不等式不能全部满足，即函数间隔大于1。</p>
<p>因此我们引入松弛变量<span class="math inline">\(\xi_i \geq0\)</span>，使得函数间隔大于等于<span class="math inline">\(1-\xi_i\)</span>。同时对松弛变量付出一个惩罚参数。</p>
<ul>
<li><p>约束条件 <span class="math display">\[
y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}
\]</span></p></li>
<li><p>目标函数 <span class="math display">\[
\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}
\]</span></p>
<ul>
<li><span class="math inline">\(C\)</span>为惩罚参数。<span class="math inline">\(C\)</span>越大，对误分类的惩罚越大</li>
<li>最小化目标函数，即使<span class="math inline">\(\frac{1}{2}\|w\|^{2}\)</span>尽可能小，间隔尽可能大</li>
</ul></li>
</ul>
<h3 id="原始问题">2.1 原始问题</h3>
<p><span class="math display">\[
\begin{align}
\min _{w, b, \xi} &amp; \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{align}
\]</span></p>
<ul>
<li>凸二次规划问题</li>
<li><span class="math inline">\(w\)</span>的解唯一</li>
<li><span class="math inline">\(b\)</span>的解可能不唯一，而是一个区间</li>
</ul>
<h3 id="拉格朗日函数">2.2 拉格朗日函数</h3>
<p><span class="math display">\[
L(w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}\\
\alpha_i\geq 0,\;\mu_i\geq0
\]</span></p>
<h3 id="对偶问题">2.3 对偶问题</h3>
<p>原始问题(30-32)的对偶问题使拉格朗日函数的极大极小问题。 <span class="math display">\[
\begin{align}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{align}
\]</span></p>
<h3 id="解的等价性">2.4 解的等价性</h3>
<blockquote>
<p>定理 7.3 设 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}\)</span> 是对偶问题的一个解，若存在 <span class="math inline">\(\alpha^{*}\)</span> 的一个分量 <span class="math inline">\(\alpha_{j}^{*}, 0&lt;\alpha_{j}^{*}&lt;C,\)</span> 则原始问题的解 <span class="math inline">\(w^{*}, b^{*}\)</span> 可按下式 求得: <span class="math display">\[
\begin{align}
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)
\end{align}
\]</span></p>
</blockquote>
<blockquote>
<p>算法 7.3 (线性支持向量机学习算法)</p>
<p>输入: 训练数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中， <span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}\)</span>, <span class="math inline">\(y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N\)</span></p>
<p>输出：分离超平面和分类决策函数。</p>
<p>（1）选择惩罚参数 <span class="math inline">\(C&gt;0,\)</span> 构造并求解凸二次规划问题 <span class="math display">\[
\begin{align}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{align}
\]</span> 求得最优解 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}\)</span></p>
<p>（2） 计算 <span class="math inline">\(w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}\)</span> 选择 <span class="math inline">\(\alpha^{*}\)</span> 的一个分量 <span class="math inline">\(\alpha_{j}^{*}\)</span> 适合条件 <span class="math inline">\(0&lt;\alpha_{j}^{*}&lt;C\)</span> ，计算 <span class="math display">\[
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)
\]</span> （3）求得分离超平面 <span class="math display">\[
w^{*} \cdot x+b^{*}=0
\]</span> 分类决策函数: <span class="math display">\[
f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\]</span></p>
</blockquote>
<ul>
<li>第二部中，每一个合适的<span class="math inline">\(\alpha\)</span>都会得出一个<span class="math inline">\(b\)</span>，所以<span class="math inline">\(b\)</span>不唯一</li>
</ul>
<h3 id="支持向量">2.4 支持向量</h3>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E8%BD%AF%E9%97%B4%E9%9A%94%E5%88%86%E7%A6%BB%E5%90%91%E9%87%8F.png" alt="image-20201216151045483" style="zoom:50%;" /></p>
<p>虚线是间隔边界，实线是分离超平面。对每一个实例点<span class="math inline">\((x_i,y_i)\)</span>和他们的<span class="math inline">\(\alpha _i^*,\;\xi_i\)</span>，我们有</p>
<ul>
<li><span class="math inline">\(\alpha_{i}^{*}&lt;C\)</span>，则 <span class="math inline">\(\xi_{i}=0\)</span></li>
<li><span class="math inline">\(\alpha_{i}^{*}=C, 0&lt;\xi_{i}&lt;1\)</span>，则</li>
<li><span class="math inline">\(\alpha_{i}^{*}=C, \xi_{i}=1\)</span>，则</li>
<li><span class="math inline">\(\alpha_{i}^{*}=C, \xi_{i}&gt;1\)</span>，则</li>
<li>没有<span class="math inline">\(\alpha_{i}^{*}=C, 0&lt;\xi_{i}&lt;1\)</span></li>
</ul>
<h3 id="合页损失函数-hinge-loss-function">2.5 合页损失函数 Hinge Loss Function</h3>
<blockquote>
<p>定理 7.4 线性支持向量机原始最优化问题: <span class="math display">\[
\begin{align}
\min _{w, b, \xi} &amp; \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{align}
\]</span> 等价于最优化问题 <span class="math display">\[
\min _{w, b} \sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
\]</span></p>
</blockquote>
<ul>
<li><span class="math inline">\([z]_{+}=\left\{\begin{array}{ll}z, &amp; z&gt;0 \\ 0, &amp; z \leqslant 0\end{array}\right.\)</span>表示取正值的函数。</li>
<li><span class="math inline">\(L(y(w \cdot x+b))=[1-y(w \cdot x+b)]_{+}\)</span> 经验损失。但实例点被正确分类且函数间隔大于1，损失才是0。位于间隔边界上的实例点损失不为0。</li>
<li><span class="math inline">\(\lambda\|w\|^{2}\)</span> 正则化项</li>
</ul>
<h4 id="损失函数对比">2.5.1 损失函数对比</h4>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt="image-20201216155233841" style="zoom:50%;" /></p>
<p>虚线为感知机的损失函数<span class="math inline">\(\left[-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}\)</span></p>
<ul>
<li>0-1损失函数为二元分类真正的损失函数。<strong>不过因其不连续可导，无法直接优化它所构成的函数</strong>。</li>
<li>合页损失函数是0-1损失函数的上界。<strong>线性支持向量机是优化由0-1损失函数的上界构成的目标函数</strong>。</li>
<li>合页损失函数对学习有更高要求。不但需要分类正确，还要确信度（距离）足够高才为0损失。</li>
</ul>
<p><br><br></p>
<h2 id="非线性支持向量机与核函数">3. 非线性支持向量机与核函数</h2>
<p>对于一个非线性数据集，首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。</p>
<p>由于特征空间是高维甚至无限维，加上对偶问题中多组高维向量计算内积。因此引入核函数减少计算量。</p>
<h3 id="核技巧-kernel-trick">3.1 核技巧 Kernel Trick</h3>
<p>核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间 (欧氏空间 <span class="math inline">\(\mathbf{R}^{n}\)</span> 或离散集合) 对应于一个特征空间 (希尔伯特空间 <span class="math inline">\(\mathcal{H}\)</span> )，使得在输入空间 <span class="math inline">\(\mathbf{R}^{n}\)</span> 中的超曲面模型对应于特征空间 <span class="math inline">\(\mathcal{H}\)</span> 中的超平面模型支持向量机）。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。</p>
<blockquote>
<p>定义 7.6 (核函数) <span class="math inline">\(\quad\)</span> 设 <span class="math inline">\(\mathcal{X}\)</span> 是输入空间 ( 欧氏空间 <span class="math inline">\(\mathbf{R}^{n}\)</span> 的子集或离散集合 <span class="math inline">\(),\)</span> 又设 <span class="math inline">\(\mathcal{H}\)</span> 为特征空间 <span class="math inline">\((\)</span> 希 尔伯特空间 <span class="math inline">\(),\)</span> 如果存在一个从 <span class="math inline">\(\mathcal{X}\)</span> 到 <span class="math inline">\(\mathcal{H}\)</span> 的映射 <span class="math display">\[
\phi(x): \mathcal{X} \rightarrow \mathcal{H}
\]</span> 使得对所有 <span class="math inline">\(x, z \in \mathcal{X},\)</span> 函数 <span class="math inline">\(K(x, z)\)</span> 满足条件 <span class="math display">\[
K(x, z)=\phi(x) \cdot \phi(z)
\]</span> 则称 <strong><span class="math inline">\(K(x, z)\)</span> 为核函数， <span class="math inline">\(\phi(x)\)</span> 为映射函数</strong>，式中 <span class="math inline">\(\phi(x) \cdot \phi(z)\)</span> 为 <span class="math inline">\(\phi(x)\)</span> 和 <span class="math inline">\(\phi(z)\)</span> 的内积。</p>
</blockquote>
<ul>
<li><strong>在使用过程中只定义核函数<span class="math inline">\(K(x,z)\)</span>，不显式定义映射函数<span class="math inline">\(\phi\)</span></strong></li>
<li>特征空间<span class="math inline">\(\mathcal{H}\)</span>一般是高维甚至无限维</li>
<li>给定核<span class="math inline">\(K(x,z)\)</span>，特征空间和映射函数不唯一</li>
</ul>
<p>在支持向量机中，通过映射函数<span class="math inline">\(\phi\)</span>把输入空间变换到新的特征空间，在新的特征空间中训练线性支持向量机。此时对偶问题的目标函数成为<span class="math inline">\(W(\alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\)</span>，分类决策函数成为<span class="math inline">\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \phi\left(x_{i}\right) \cdot \phi(x)+b^{*}\right)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} K\left(x_{i}, x\right)+b^{*}\right)\)</span></p>
<h3 id="正定核">3.2 正定核</h3>
<blockquote>
<p>定理 7.5 (正定核的充要条件) <span class="math inline">\(\quad\)</span> 设 <span class="math inline">\(K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbf{R}\)</span> 是对称函数，则 <span class="math inline">\(K(x, z)\)</span> 为正 定核函数的充要条件是对任意 <span class="math inline">\(x_{i} \in \mathcal{X}, i=1,2, \cdots, m, K(x, z)\)</span> 对应的 Gram 矩阵: <span class="math display">\[
K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}
\]</span> 是半正定矩阵。</p>
</blockquote>
<blockquote>
<p>定义 7.7 (正定核的等价定义) <span class="math inline">\(\quad\)</span> 设 <span class="math inline">\(\mathcal{X} \subset \mathbf{R}^{n}, K(x, z)\)</span> 是定义在 <span class="math inline">\(\mathcal{X} \times \mathcal{X}\)</span> 上的对称 函数, 如果对任意 <span class="math inline">\(x_{i} \in \mathcal{X}, i=1,2, \cdots, m, K(x, z)\)</span> 对应的 Gram 矩阵 <span class="math display">\[
K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}
\]</span></p>
</blockquote>
<h3 id="常用核函数">3.3 常用核函数</h3>
<h4 id="多项式核函数-polynomial-kernel-function">3.3.1 多项式核函数 Polynomial Kernel Function</h4>
<p><span class="math display">\[
K(x, z)=(x \cdot z+1)^{p}
\]</span></p>
<p>对应的支持向量机是一个 <span class="math inline">\(p\)</span> 次多项式分类器。在此情形下，分类决策函数成为 <span class="math display">\[
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i}\left(x_{i} \cdot x+1\right)^{p}+b^{*}\right)
\]</span></p>
<h4 id="高斯核函数-gaussian-kernel-function">3.3.2 高斯核函数 Gaussian Kernel Function</h4>
<p><span class="math display">\[
K(x, z)=\exp \left(-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)
\]</span></p>
<p>对应的支持向量机是高斯径向基函数（radial basis function）分类器。在此情形下，分类决策函数成为 <span class="math display">\[
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \exp \left(-\frac{\left\|x-x_{i}\right\|^{2}}{2 \sigma^{2}}\right)+b^{*}\right)
\]</span></p>
<h4 id="字符串核函数-string-kernel-function">3.3.3 字符串核函数 String Kernel Function</h4>
<p>定义在离散数据集合上。</p>
<h3 id="非线性支持向量机">3.4 非线性支持向量机</h3>
<blockquote>
<p>定义 7.8 (非线性支持向量机 <span class="math inline">\() \quad\)</span> 从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划(46)学习得到的分类决策函数 <span class="math display">\[
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x, x_{i}\right)+b^{*}\right)
\]</span> 称为非线性支持向量机, <span class="math inline">\(K(x, z)\)</span> 是正定核涵数。</p>
</blockquote>
<blockquote>
<p>算法 7.4 (非线性支持向量机学习算法)</p>
<p>输入: 训练数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中 <span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in\)</span><span class="math inline">\(\mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N\)</span></p>
<p>输出：分类决策函数。</p>
<p>（1）选取适当的核函数 <span class="math inline">\(K(x, z)\)</span> 和适当的参数 <span class="math inline">\(C,\)</span> 构造并求解最优化问题 <span class="math display">\[
\begin{align}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{align}
\]</span> 求得最优解 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}\)</span> 。 （2）选择 <span class="math inline">\(\alpha^{*}\)</span> 的一个正分量 <span class="math inline">\(0&lt;\alpha_{j}^{*}&lt;C,\)</span> 计算 <span class="math display">\[
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x_{i}, x_{j}\right)
\]</span> （3）构造决策函数: <span class="math display">\[
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x, x_{i}\right)+b^{*}\right)
\]</span></p>
</blockquote>
<ul>
<li><span class="math inline">\(K(x,z)\)</span>为正定核函数是，问题(46)是二次规划问题，解存在。</li>
</ul>
<p><br><br></p>
<h2 id="smo-algorithm-sequential-minimal-optimization">4. SMO Algorithm (sequential minimal optimization)</h2>
<p>序列最小算法用来求解凸二次规划的对偶问题 <span class="math display">\[
\begin{align}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{align}
\]</span></p>
<p><br><br></p>
<h2 id="reference">5. Reference</h2>
<p><a href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>
]]></content>
      <categories>
        <category>study</category>
        <category>《统计学习方法》</category>
      </categories>
      <tags>
        <tag>Dual Problem</tag>
        <tag>SVM</tag>
        <tag>Kernel Method</tag>
        <tag>Maximum Margin</tag>
        <tag>Hinge Loss Function</tag>
      </tags>
  </entry>
  <entry>
    <title>[统计数学方法] 6. 逻辑回归LR和最大熵模型MEM</title>
    <url>/slm006/</url>
    <content><![CDATA[<p>Logistic Regression 是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型( maximum entropy model) 。逻辑回归模型与最大熵模型都属于对数线性模型。 <a id="more"></a></p>
<h2 id="logistics-regression">1. Logistics Regression</h2>
<h3 id="logistic-distribution">1.1 Logistic Distribution</h3>
<blockquote>
<p>定义 6.1 （logistic分布）$ $ 设 <span class="math inline">\(X\)</span>是连续随机变量， <span class="math inline">\(X\)</span>服从逻辑斯締分布是指<span class="math inline">\(X\)</span>具有下列分布函数和密度函数: <span class="math display">\[
\begin{align}
F(x)&amp;=P(X \leqslant x)=\frac{1}{1+\mathrm{e}^{-(x-\mu) / \gamma}} \\
f(x)&amp;=F^{\prime}(x)=\frac{\mathrm{e}^{-(x-\mu) / \gamma}}{\gamma\left(1+\mathrm{e}^{-(x-\mu) / \gamma}\right)^{2}}
\end{align}
\]</span> 式中， <span class="math inline">\(\mu\)</span> 为位置参数, <span class="math inline">\(\gamma&gt;0\)</span> 为形状参数。</p>
</blockquote>
<ul>
<li><p>logistic function，也叫sigmoid function为 <span class="math display">\[
\begin{align}
f(x)&amp;=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1},\;x\in(- \infty,+ \infty),\;f(x)\in[0,1]\\
f&#39;(x)&amp;=f(1-f)=\frac{1}{\left(1+e^{-x}\right)^{2}} e^{-x}
\end{align}
\]</span></p></li>
<li><p><span class="math inline">\(f(x=0)=0.5\)</span>，凭借这个性质，我们可以把<span class="math inline">\(x&gt;0\)</span>时的<span class="math inline">\(y\)</span>分类成1。</p></li>
</ul>
<h3 id="二项logistic回归模型">1.2 二项logistic回归模型</h3>
<p>Binomial Logistic Regression Model是二元分类模型。由条件概率分布<span class="math inline">\(P(Y|X)\)</span>表示，形式为参数化的逻辑分布，此时随机变量<span class="math inline">\(X\in \mathrm{R},\; Y=0,1\)</span>。通过监督学习的方法估计模型参数<span class="math inline">\(w,b\)</span></p>
<blockquote>
<p>定义 6.2 (logistic回归模型) <span class="math inline">\(\quad\)</span> 二项逻辑回归模型是如下的条件概率分布: <span class="math display">\[
\begin{align}
P(Y=1 \mid x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)} \\
P(Y=0 \mid x)=\frac{1}{1+\exp (w \cdot x)}
\end{align}
\]</span> 这里, <span class="math inline">\(x=\left(x^{(i)}, x^{(2)}, \cdots, x^{(n)}, 1\right)^{\mathrm{T}}\in \mathbf{R}^{n+1}\)</span> 是输入, <span class="math inline">\(Y \in\{0,1\}\)</span> 是输出， <span class="math inline">\(w =(\left.w^{(1）},w^{(2)}, \cdots, w^{(n)}, b\right)^{\mathrm{T}}\in \mathbf{R}^{n+1}\)</span> 和 <span class="math inline">\(b \in \mathbf{R}\)</span> 是参数， <span class="math inline">\(w\)</span> 称为权值向量， <span class="math inline">\(b\)</span> 称为偏置， <span class="math inline">\(w \cdot x\)</span> 为 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(x\)</span> 的内积。</p>
</blockquote>
<ul>
<li>对给定的<span class="math inline">\(x\)</span>，计算2个概率分布的大小，把<span class="math inline">\(x\)</span>分类到概率较大的那一类</li>
</ul>
<blockquote>
<p>定义6.01（几率odds与概率） 该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是<span class="math inline">\(p\)</span>, 那么该事件的几率是<span class="math inline">\(\frac{p}{1-p}\)</span>，该事件的对数几率log odds是 <span class="math display">\[
\begin{align}
\operatorname{logit}(p)=\log \frac{p}{1-p}
\end{align}
\]</span></p>
</blockquote>
<p>根据（5）（6）（7），我们可以得到 <span class="math display">\[
\begin{align}
\log \frac{P(Y=1 \mid x)}{1-P(Y=1 \mid x)}=w \cdot x
\end{align}
\]</span></p>
<ul>
<li>（8）表示了输出为<span class="math inline">\(Y=1\)</span>的对数几率是输入<span class="math inline">\(x\)</span>的线性函数</li>
<li><strong>通过几率这一关于概率的函数，把线性函数<span class="math inline">\(w\cdot x\)</span>与概率联系起来，互相转化。</strong></li>
</ul>
<h3 id="参数估计">1.3 参数估计</h3>
<p><img src='https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-6%20%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.png'></p>
<p>利用极大似然估计法估计参数<span class="math inline">\(w\)</span>，即求<span class="math inline">\(L(w)\)</span>的极大值。 <span class="math display">\[
\begin{align}
maxL(w) =max(\sum_{i=1}^{N}\left[y_{i}\left(w \cdot x_{i}\right)-\log \left(1+\exp \left(w \cdot x_{i}\right)\right]\right)
\end{align}
\]</span> 求解对数似然函数为目标函数的最优化问题，采样梯度下降法与牛顿法。最后得到回归参数<span class="math inline">\(\hat{w}\)</span>和回归模型 <span class="math display">\[
\begin{align}
P(Y=1 \mid x)=\frac{\exp (\hat{w} \cdot x)}{1+\exp (\hat{w} \cdot x)} \\
P(Y=0 \mid x)=\frac{1}{1+\exp (\hat{w} \cdot x)}
\end{align}
\]</span></p>
<h3 id="多项逻辑回归模型">1.4 多项逻辑回归模型</h3>
<p>但<span class="math inline">\(Y\)</span>有多项分类时，模型为 <span class="math display">\[
\begin{align}
P(Y=k \mid x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1
\end{align}
\]</span></p>
<p><br><br></p>
<h2 id="最大熵模型-maximum-entropy-model">2. 最大熵模型 Maximum Entropy Model</h2>
<h3 id="最大熵原理">2.1 最大熵原理</h3>
<p>最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。</p>
<ul>
<li><p>假设离散随机变量 <span class="math inline">\(X\)</span> 的概率分布是 <span class="math inline">\(P(X)\)</span>，熵为 <span class="math display">\[
\begin{align}
H( P)=-\sum_{x} P(x) \log P(x)
\end{align}
\]</span></p></li>
<li><p><span class="math inline">\(0 \leqslant H( P) \leqslant \log |X|\)</span>，<strong><span class="math inline">\(X\)</span>服从均匀分布（等可能）时，熵最大，右边等式取等</strong></p></li>
<li><p>直观上，模型的选择在满足约束条件的情况下，认为不确定的部分为”等可能“的。保留尽可能多的不确定性，即在没有更多信息的时候，不擅自做假设。</p></li>
<li><p>用熵来表示“等可能”程度的数值指标。</p></li>
</ul>
<h3 id="最大熵模型的定义">2.2 最大熵模型的定义</h3>
<p>最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型。</p>
<ul>
<li>假设分类模型是一个条件概率分布 <span class="math inline">\(P(Y \mid X)\)</span>
<ul>
<li>$ X $ <span class="math inline">\(\in \mathcal{X}\subseteq \mathbf{R}^n\)</span> 表示输入, <span class="math inline">\(Y \in \mathcal{Y}\)</span> 表示输出, <span class="math inline">\(\mathcal{X}\)</span> 和 <span class="math inline">\(\mathcal{Y}\)</span> 分别是输入和输出的集合。这个模型表示的是对于给定的输入 <span class="math inline">\(X,\)</span> 以条件概率 <span class="math inline">\(P(Y \mid X)\)</span> 输出 <span class="math inline">\(Y\)</span></li>
</ul></li>
<li>给定训练数据集<span class="math inline">\(T={\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)}\)</span></li>
<li>学习目标：用最大熵原理选择最好的分类模型</li>
</ul>
<h4 id="确定约束条件来确定概率模型的集合">2.2.1 确定约束条件来确定概率模型的集合</h4>
<p>从训练数据T中抽取若干特征,然后要求这些特征在<span class="math inline">\(T\)</span>上关于经验分布的期望与它们在模型中关于<span class="math inline">\(P(x,y)\)</span>的数学期望相等，这样，一个特征就对应一个约束。</p>
<h5 id="经验分布">2.2.1.1 经验分布</h5>
<p>在训练集<span class="math inline">\(T\)</span>中，我们可以确定联合分布和边缘分布的经验分布为 <span class="math display">\[
\begin{align}
\tilde{P}(X=x, Y=y)=\frac{\nu(X=x, Y=y)}{N}
\end{align}
\]</span> <span class="math display">\[
\begin{align}
\tilde{P}(X=x)=\frac{\nu(X=x)}{N}
\end{align}
\]</span> <span class="math display">\[
\begin{align}
v:\text{频数},\;N:训练样本容量
\end{align}
\]</span></p>
<ul>
<li>与模型中的条件概率分布<span class="math inline">\(P(Y|X)\)</span>可以写出贝叶斯定理</li>
</ul>
<h5 id="特征函数">2.2.1.2 特征函数</h5>
<p>用特征函数形式化表示<span class="math inline">\(X\)</span>的相关知识信息。</p>
<p>用特征函数描述输入 <span class="math inline">\(x\)</span> 和输出 <span class="math inline">\(y\)</span> 之间的某一个事实。 其定义是 <span class="math display">\[
f(x, y)=\left\{\begin{array}{ll}
1, &amp; x \text { 与 } y \text { 满足某一事实 } \\
0, &amp; \text { 否则 }
\end{array}\right.
\]</span> 不同的输入对输出的影响是不一样的，我们要提取，衡量这些输入输出的关系</p>
<p>特征函数可以是任意实值函数</p>
<h5 id="期望相等">2.2.1.3 期望相等</h5>
<p>特征函数 <span class="math inline">\(f(x, y)\)</span> 关于经验分布 <span class="math inline">\(\tilde{P}(X, Y)\)</span> 的期望值 <span class="math display">\[
\begin{align}
E_{\tilde{P}}(f)=\sum_{x, y} \tilde{P}(x, y) f(x, y)
 \end{align}
\]</span> 特征函数 <span class="math inline">\(f(x, y)\)</span> 关于模型 <span class="math inline">\(P(Y \mid X)\)</span> 与经验分布 <span class="math inline">\(\tilde{P}(X)\)</span> 的期望值, 用 <span class="math inline">\(E_{P}(f)\)</span> 表示: <span class="math display">\[
\begin{align}
E_{P}(f)=\sum_{x, y} \tilde{P}(x) P(y \mid x) f(x, y)
 \end{align}
\]</span> 如果一个模拟能够获取到训练集中的信息，那么这2个期望应该是相等的，即</p>
<p><span class="math display">\[
\begin{align}
 E_{P}(f)&amp;=E_{\tilde{P}}(f)\\
 \sum_{x, y} \tilde{P}(x) P(y \mid x) f(x, y)&amp;=\sum_{x, y} \tilde{P}(x, y) f(x, y)
\end{align}
\]</span> (13)是某一特征函数对于模型学习的约束条件。如果有<span class="math inline">\(n\)</span>个特征函数<span class="math inline">\(f_{i}(x, y), i=1,2, \cdots, n,\)</span> 那么就有 <span class="math inline">\(n\)</span> 个约束条件。</p>
<h4 id="模型的条件熵">2.2.2 模型的条件熵</h4>
<blockquote>
<p>定义 6.3 (最大熵模型） <span class="math inline">\(\quad\)</span> 假设满足所有约束条件的模型集合为 <span class="math display">\[
\mathcal{C} \equiv\left\{P \in \mathcal{P} \mid E_{P}\left(f_{i}\right)=E_{\tilde{P}}\left(f_{i}\right), \quad i=1,2, \cdots, n\right\}
\]</span> 定义在条件概率分布 <span class="math inline">\(P(Y \mid X)\)</span> 上的条件熵为 <span class="math display">\[
\begin{align}
H(P)=-\sum_{x, y} \tilde{P}(x) P(y \mid x) \log P(y \mid x)
\end{align}
\]</span> 则模型集合 <span class="math inline">\(\mathcal{C}\)</span> 中条件熵 <span class="math inline">\(H(P)\)</span> 最大的模型称为最大熵模型。式中的对数为自然对数。</p>
</blockquote>
<h3 id="模型的学习">2.3 模型的学习</h3>
<h4 id="对偶问题转化">2.3.1 对偶问题转化</h4>
<p><strong>原始问题</strong></p>
<p>给定训练数据集<span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}\)</span>与特征函数<span class="math inline">\(f_{i}(x, y), i=1,2, \cdots, n\)</span>。最大熵模型的学习可以转化为约束最优化问题</p>
<p><span class="math display">\[
\begin{align}
\min _{P \in \mathbf{C}} &amp; -H(P)=\sum_{x, y} \tilde{P}(x) P(y \mid x) \log P(y \mid x) \\
\text { s.t. } &amp; E_{P}\left(f_{i}\right)-E_{\tilde{P}}\left(f_{i}\right)=0, \quad i=1,2, \cdots, n \\
&amp; \sum_{y} P(y \mid x)=1
\end{align}
\]</span></p>
<p><strong>原始问题的等价问题</strong></p>
<p>设拉格朗日乘子<span class="math inline">\(w_{0}, w_{1}, w_{2}, \cdots, w_{n},\)</span> 定义拉格朗日函数 <span class="math inline">\(L(P, w)\)</span> <span class="math display">\[
\begin{array}{1}
L(P, w)&amp; \equiv -H(P)+w_{0}\left(1-\sum_{y} P(y \mid x)\right)+\sum_{i=1}^{n} w_{i}\left(E_{\tilde{P}}\left(f_{i}\right)-E_{P}\left(f_{i}\right)\right) \\
&amp;= \sum_{x, y} \tilde{P}(x) P(y \mid x) \log P(y \mid x)+w_{0}\left(1-\sum_{y} P(y \mid x)\right)+\\
&amp; \quad \,\sum_{i=1}^{n} w_{i}\left(\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P(y \mid x) f_{i}(x, y)\right)
\end{array}
\]</span> 原始问题的等价问题为 <span class="math display">\[
\min _{P \in \mathbf{C}} \max _{w} L(P, w)
\]</span> <strong>对偶问题</strong></p>
<p>对偶问题为 <span class="math display">\[
\max _{w} \min _{P \in \mathrm{C}} L(P, w)
\]</span> 因为<span class="math inline">\(L(P, w)\)</span>是<span class="math inline">\(P\)</span>的凸函数。强对偶关系，解相等。</p>
<h4 id="对偶问题求解">2.3.2 对偶问题求解</h4>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-6%20%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A31.png"></p>
<h4 id="极大化与mle">2.3.3 极大化与MLE</h4>
<p>对极小值<span class="math inline">\(\Phi\)</span>的极大化等价于对其的MLE</p>
<p><img src='https://github.com/ChongfengLing/typora-picBed/blob/main/img/SLM-6%20%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A32.png?raw=true'></p>
<p><br><br></p>
<h2 id="lr和mem的关系">3. LR和MEM的关系</h2>
<p>最大熵模型为 <span class="math display">\[
\begin{align}
P_{w}(y \mid x)&amp;=\frac{1}{Z_{w}(x)} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
Z_{w}(x)&amp;=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{align}
\]</span> 假设当前类别 <span class="math inline">\(y\)</span> 的取值只有两个, 分别是 <span class="math inline">\(y_{0}, y_{1}\)</span> 。假设输入向量 <span class="math inline">\(\mathbf{x}\)</span> 具有 <span class="math inline">\(n\)</span> 个维度, 我们定义 <span class="math inline">\(n\)</span> 个特征函数: <span class="math display">\[
f_{i}(\mathbf{x}, y)=\left\{\begin{array}{ll}
x_{i}, &amp; \text { if } y=y_{1} \\
0, &amp; \text { 其他 }
\end{array}\right.
\]</span> 那么我们对于2个输出<span class="math inline">\(y\)</span>有 <span class="math display">\[
\begin{align}
P\left(y_{1} \mid \mathbf{x}\right)&amp;=\frac{e^{\mathbf{w}^{T} \mathbf{x}}}{1+e^{\mathbf{w}^{T} \mathbf{x}}}\\
P\left(y_{0} \mid \mathbf{x}\right)&amp;=\frac{1}{1+e^{\mathbf{w}^{T} \mathbf{x}}}
\end{align}
\]</span></p>
<ul>
<li>(27)(28)即是二项逻辑回归模型</li>
<li>LR的本质为二项分类下选取了某个特征函数的最大熵模型</li>
<li>LR模型采用了Sigmoid函数以及用MLE求参数的原因</li>
</ul>
<p><br><br></p>
<h2 id="模型学习的最优化算法">4. 模型学习的最优化算法</h2>
<h3 id="改进的迭代尺度算法-improved-iterative-scaling">4.1改进的迭代尺度算法 Improved Iterative Scaling</h3>
<p>最大熵模型 <span class="math display">\[
\begin{align}
P_{w}(y \mid x)&amp;=\frac{1}{Z_{w}(x)} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
Z_{w}(x)&amp;=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{align}
\]</span> 对数似然函数为 <span class="math display">\[
\begin{align}
L(w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
 \end{align}
\]</span> 目标：求对数似然函数的极大值<span class="math inline">\(\hat{w}\)</span></p>
<p><br><br></p>
<h2 id="reference">Reference</h2>
<p>待补充</p>
]]></content>
      <categories>
        <category>study</category>
        <category>《统计学习方法》</category>
      </categories>
      <tags>
        <tag>MLE</tag>
        <tag>Logistic model</tag>
        <tag>Maximum Entropy Model</tag>
        <tag>Dual Problem</tag>
      </tags>
  </entry>
  <entry>
    <title>[统计数学方法] 5. 决策树 Decision Tree</title>
    <url>/slm005/</url>
    <content><![CDATA[<p>决策树是一种基本的分类与回归方法，这里主要讨论分类问题。<strong>他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</strong>学习过程主要包括3个步骤：特征选择、决策树的生成、决策树的修建，并根据损失函数最小化的原则建立决策树模型。 <a id="more"></a></p>
<h2 id="model">1. Model</h2>
<blockquote>
<p>定义 5.1 (决策树) <span class="math inline">\(\quad\)</span> 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点 ( node ) 和有向边 ( directed edge ) 组成。结点有两种类型: 内部结点 ( internal node ) 和叶结,点 ( leaf node ) 。内部结点表示一个特征或属性, 叶结点表示一个类。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">		A--是--&gt;C[叶结点]</span><br><span class="line">		A((根节点))--否--&gt;B((内部结点))</span><br><span class="line">		B--是--&gt;D[叶结点]</span><br><span class="line">		B--否--&gt;E[叶结点]</span><br></pre></td></tr></table></figure>
<h3 id="决策树与if-then规则">1.1 决策树与if-then规则</h3>
<ul>
<li>根结点到叶结点的每一条路径是一条规则</li>
<li>路径上的内部节点对应规则的条件</li>
<li>叶结点的类对应规则的结论</li>
<li>if-then规则是<strong>互斥且完备</strong>，即每一个实例有且仅有被一条路径/规则覆盖。</li>
</ul>
<h3 id="决策树与给定特征条件下的类条件概率分布">1.2 决策树与给定特征条件下的类条件概率分布</h3>
<p><strong>决策树的生成等价于对特征空间的划分(partition)</strong>，从而划分成互不相交的单元(cell)/区域(region)，再每一个单元定义一个类的概率分布就构成的一个条件概率分布。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。</p>
<p><span class="math inline">\(P(Y|X),\;X:特征的随机变量\;\;\;\;Y:类的随机变量\)</span></p>
<p>条件概率<span class="math inline">\(P(Y|X)\)</span>往往偏大于某一类<span class="math inline">\(y\)</span>。分类时把该节点实例强行分到条件概率大的那一类。</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLm-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87.png" alt="image-20201202210309048" style="zoom:50%;" /></p>
<h3 id="决策树的学习">1.3 决策树的学习</h3>
<p>学习本质：为训练数据集归纳出一组分类规则</p>
<ul>
<li>正确分类训练数据集的决策树可能有很多个，可能没有</li>
<li>需要找到一个与训练数据集的误差较小、泛化能力高的决策树</li>
</ul>
<p>损失函数与策略：最小化正则化后的极大似然函数</p>
<p>算法：递归的选择最优特征。ID3、C4.5、CART</p>
<p>剪枝：树枝多，复杂，泛化能力低。自下而上进行剪枝。树的生成考虑局部最优，树的剪枝考虑全局最优。</p>
<h2 id="特征选择-feature-selection">2. 特征选择 Feature Selection</h2>
<p>通过信息增益（比）来选取具有分类能力的特征（指那些与随机分类有较大查别的特征），从而提高决策树的学习效率。</p>
<h3 id="信息增益-information-gain">2.1 信息增益 Information Gain</h3>
<h4 id="熵-entropy信息熵">2.1.1 熵 Entropy、信息熵</h4>
<blockquote>
<p>定义 5.01（熵） 熵是表示随机变量不确定性的度量。设<span class="math inline">\(X\)</span>为离散随机变量，概率分布为 <span class="math display">\[
\begin{align}
P\left(X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n 
\end{align}
\]</span> 随机变量<span class="math inline">\(X\)</span>的熵为 <span class="math display">\[
\begin{align}
H(X)=H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
\end{align}
\]</span></p>
<p><span class="math display">\[
对p_i=0，定义0log0=0\nonumber
\]</span></p>
</blockquote>
<ul>
<li>在(2)中，对数底为2/<span class="math inline">\(e\)</span>时，熵单位为比特bit/纳特nat</li>
<li>熵只和<span class="math inline">\(X\)</span>的分布有关，与取值<span class="math inline">\(x_i\)</span>无关</li>
<li>熵越大，随机变量的不确定性就越大</li>
<li><span class="math inline">\(0 \leqslant H(p) \leqslant \log n\)</span></li>
</ul>
<h4 id="条件熵-conditional-entropy">2.1.2 条件熵 Conditional Entropy</h4>
<blockquote>
<p>定义 5.02（条件熵） 对随机变量(X,Y)，联合概率分布为 <span class="math display">\[
P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, n ; \quad j=1,2, \cdots, m\nonumber
\]</span> 条件熵<span class="math inline">\(H(Y|X)\)</span>表示一直随机变量<span class="math inline">\(X\)</span>的情况下随机变量<span class="math inline">\(Y\)</span>的不确定性。<strong>定义为给定<span class="math inline">\(X\)</span>后<span class="math inline">\(Y\)</span>的条件概率分布的熵对<span class="math inline">\(X\)</span>的数学期望</strong>，即 <span class="math display">\[
H(Y \mid X)=\sum_{i=1}^{n} p_{i} H\left(Y \mid X=x_{i}\right)\\
p_{i}=P\left(X=x_{i}\right), i=1,2, \cdots, n
\]</span></p>
</blockquote>
<p>当熵和条件熵的概率有极大似然估计得到时，对应的为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy)</p>
<h4 id="信息增益">2.1.3 信息增益</h4>
<p><strong>信息增益表示得知特征<span class="math inline">\(X\)</span>的信息从而使得类<span class="math inline">\(Y\)</span>的信息的不确定性减少程度。</strong></p>
<blockquote>
<p>定义 5.2 (信息增益) <span class="math inline">\(\quad\)</span> 特征 <span class="math inline">\(A\)</span> 对训练数据集<span class="math inline">\(D\)</span>的信息增益 <span class="math inline">\(g(D, A),\)</span> 定义为集合<span class="math inline">\(D\)</span>的经验熵<span class="math inline">\(H(D)\)</span> 与特征 <span class="math inline">\(A\)</span> 给定条件下<span class="math inline">\(D\)</span>的经验条件熵 <span class="math inline">\(H(D \mid A)\)</span> 之差，即 <span class="math display">\[
g(D, A)=H(D)-H(D \mid A)\nonumber
\]</span></p>
</blockquote>
<ul>
<li>熵和条件熵之差称为互信息 mutual information，在决策树中即为信息增益</li>
<li><span class="math inline">\(D\)</span>一般为标签</li>
</ul>
<h4 id="信息增益比-information-gain-ratio">2.1.4 信息增益比 information gain ratio</h4>
<p>在训练集里，某一个特征较多时，信息增益会偏大。因此采用信息增益比来校正。</p>
<blockquote>
<p>定义 5.3 (信息增益比) 特征 <span class="math inline">\(A\)</span> 对训练数据集 <span class="math inline">\(D\)</span> 的信息增益比 <span class="math inline">\(g_{R}(D, A)\)</span> 定义为其信息增益 <span class="math inline">\(g(D, A)\)</span> 与训练数据集 <span class="math inline">\(D\)</span> 关于特征 <span class="math inline">\(A\)</span> 的值的熵 <span class="math inline">\(H_{A}(D)\)</span> 之比，即 <span class="math display">\[
g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}
\]</span> 其中 <span class="math inline">\(, H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}, n\)</span> 是特征 <span class="math inline">\(A\)</span> 取值的个数。</p>
</blockquote>
<h4 id="特征增益的算法">2.1.5 特征增益的算法</h4>
<p>对数据集<span class="math inline">\(D\)</span>，有<span class="math inline">\(k\)</span>个类<span class="math inline">\(C_k\)</span>，对特征<span class="math inline">\(A\)</span>有<span class="math inline">\(n\)</span>个取值<span class="math inline">\(D_n\)</span>，<span class="math inline">\(D_{i k}=D_{i} \cap C_{k}\)</span>，<span class="math inline">\(|D|\)</span>为数据集中的样本个数</p>
<blockquote>
<p>算法 5.1 (信息增益的算法) 输入: 训练数据集 <span class="math inline">\(D\)</span> 和特征 <span class="math inline">\(A\)</span>; 输出: 特征 <span class="math inline">\(A\)</span> 对训练数据集 <span class="math inline">\(D\)</span> 的信息增益 <span class="math inline">\(g(D, A)\)</span> 。</p>
<p>(1)计算数据集 <span class="math inline">\(D\)</span> 的经验熵<span class="math inline">\(H(D)\)</span> <span class="math display">\[
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}\nonumber
\]</span></p>
<p>(2)计算特征 <span class="math inline">\(A\)</span> 对数据集 <span class="math inline">\(D\)</span> 的经验条件熵 <span class="math inline">\(H(D \mid A)\)</span> <span class="math display">\[
H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}\nonumber
\]</span></p>
<p>(3)计算信息增益 <span class="math display">\[
g(D, A)=H(D)-H(D \mid A)\nonumber
\]</span></p>
</blockquote>
<ul>
<li>没有特征B！A是特征的符号</li>
</ul>
<h2 id="id3-algorithm">3. ID3 Algorithm</h2>
<p>决策树各个节点熵应用信息增益准则选择特征，递归构建决策树。</p>
<p>从根结点开始，计算所有可能的特征，选取信息增益最大的特征作为节点特征。并由该特征的不同取值点建立子节点。递归调用。</p>
<blockquote>
<p>算法 5.2 (ID3 算法) 输入: 训练数据集 <span class="math inline">\(D,\)</span> 特征集 <span class="math inline">\(A\)</span> 间值 <span class="math inline">\(\varepsilon ;\)</span><span class="math inline">\(\\\)</span></p>
<p>输出：决策树 <span class="math inline">\(T\)</span> 。</p>
<ol type="1">
<li><p>若 <span class="math inline">\(D\)</span> 中所有实例属于同一类 <span class="math inline">\(C_{k},\)</span> 则 <span class="math inline">\(T\)</span> 为单结点树，并将类 <span class="math inline">\(C_{k}\)</span> 作为该结点 的类标记，返回 <span class="math inline">\(T\)</span>;</p></li>
<li><p>若 <span class="math inline">\(A=\varnothing,\)</span> 则 <span class="math inline">\(T\)</span> 为单结点树，并将 <span class="math inline">\(D\)</span> 中实例数最大的类 <span class="math inline">\(C_{k}\)</span> 作为该结点的类标记，返回 <span class="math inline">\(T\)</span> :</p></li>
<li><p>否则，按算法 5.1 计算 <span class="math inline">\(A\)</span> 中各特征对 <span class="math inline">\(D\)</span> 的信息增益，选择信息增益最大的特征 <span class="math inline">\(A_{g}\)</span> :</p></li>
<li><p>如果 <span class="math inline">\(A_{g}\)</span> 的信息增益小于间值 <span class="math inline">\(\varepsilon,\)</span> 则置 <span class="math inline">\(T\)</span> 为单结点树，并将 <span class="math inline">\(D\)</span> 中实例数最大 的类 <span class="math inline">\(C_{k}\)</span> 作为该结点的类标记，返回 <span class="math inline">\(T\)</span> :</p></li>
<li><p>否则，对 <span class="math inline">\(A_{g}\)</span> 的每一可能值 <span class="math inline">\(a_{i},\)</span> 依 <span class="math inline">\(A_{g}=a_{i}\)</span> 将 <span class="math inline">\(D\)</span> 分割为若干非空子集 <span class="math inline">\(D_{i},\)</span> 将 <span class="math inline">\(D_{i}\)</span> 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 <span class="math inline">\(T,\)</span> 返回 <span class="math inline">\(T\)</span>;</p></li>
<li><p>对第 <span class="math inline">\(i\)</span> 个子结点，以 <span class="math inline">\(D_{i}\)</span> 为训练集，以 <span class="math inline">\(A-\left\{A_{g}\right\}\)</span> 为特征集，逆归地调用步 (1)<span class="math inline">\(\sim\)</span> 步 <span class="math inline">\((5),\)</span> 得到子树 <span class="math inline">\(T_{i},\)</span> 返回 <span class="math inline">\(T_{i} \circ\)</span></p></li>
</ol>
</blockquote>
<ul>
<li>极大似然法进行概率估计？</li>
<li>只有树的生成，没有剪枝，容易过拟合</li>
</ul>
<h2 id="c4.5-algorithm">4. C4.5 Algorithm</h2>
<p>ID3算法的改进，用信息增益比来选择特征</p>
<blockquote>
<p>算法 5.3 (C4.5 的生成算法)</p>
<p>输入: 训练数据集 <span class="math inline">\(D\)</span>, 特征集 <span class="math inline">\(A\)</span> 间值 <span class="math inline">\(\varepsilon\)</span>;</p>
<p>输出：决策树 <span class="math inline">\(T\)</span> 。</p>
<p>(1)如果 <span class="math inline">\(D\)</span> 中所有实例属于同一类 <span class="math inline">\(C_{k},\)</span> 则置 <span class="math inline">\(T\)</span> 为单结点树，并将 <span class="math inline">\(C_{k}\)</span> 作为该结 点的类, 返回 <span class="math inline">\(T\)</span>;</p>
<p>(2)如果 <span class="math inline">\(A=\varnothing,\)</span> 则置 <span class="math inline">\(T\)</span> 为单结点树，并将 <span class="math inline">\(D\)</span> 中实例数最大的类 <span class="math inline">\(C_{k}\)</span> 作为该结点 的类, 返回 <span class="math inline">\(T\)</span>;</p>
<p>(3)否则，按式 (5.10) 计算 <span class="math inline">\(A\)</span> 中各特征对 <span class="math inline">\(D\)</span> 的<strong><em>信息增益比</em></strong>, 选择信息增益比最大 的特征 <span class="math inline">\(A_{g}\)</span>;</p>
<p>(4)如果 <span class="math inline">\(A_{g}\)</span> 的信息增益比小于间值 <span class="math inline">\(\varepsilon,\)</span> 则置 <span class="math inline">\(T\)</span> 为单结点树，并将 <span class="math inline">\(D\)</span> 中实例数最 大的类 <span class="math inline">\(C_{k}\)</span> 作为该结点的类, 返回 <span class="math inline">\(T\)</span>;</p>
<p>(5)否则, 对 <span class="math inline">\(A_{g}\)</span> 的每一可能值 <span class="math inline">\(a_{i},\)</span> 依 <span class="math inline">\(A_{g}=a_{i}\)</span> 将 <span class="math inline">\(D\)</span> 分割为子集若干非空 <span class="math inline">\(D_{i},\)</span> 将 <span class="math inline">\(D_{i}\)</span> 中实例数最大的类作为标记，构建子结点, 由结点及其子结点构成树 <span class="math inline">\(T,\)</span> 返回 <span class="math inline">\(T\)</span>;</p>
<p>(6)对结点 <span class="math inline">\(i\)</span>, 以 <span class="math inline">\(D_{i}\)</span> 为训练集，以 <span class="math inline">\(A-\left\{A_{g}\right\}\)</span> 为特征集，递归地调用步 (1) 步 <span class="math inline">\((5),\)</span> 得到子树 <span class="math inline">\(T_{i},\)</span> 返回 <span class="math inline">\(T_{i}\)</span> 。</p>
</blockquote>
<h2 id="pruning">5. Pruning</h2>
<p>考虑树的复杂度，对生成的决策树进行剪枝，减掉子树或叶结点</p>
<p><strong>策略：极小化决策树的损失函数</strong>，即正则化的极大似然估计</p>
<p>设树<span class="math inline">\(T\)</span>，叶节点个数<span class="math inline">\(|T|\)</span>，树<span class="math inline">\(T\)</span>的叶结点<span class="math inline">\(t\)</span>，此叶结点的样本点个数<span class="math inline">\(N_t\)</span>；此叶结点有<span class="math inline">\(K\)</span>类样本点的个数<span class="math inline">\(N_{tk}；\;k=1,2,...,K\)</span>；此叶结点的经验熵<span class="math inline">\(H_t(T)\)</span>，函数参数<span class="math inline">\(\alpha \geq0\)</span>。此决策树的损失函数定义为 <span class="math display">\[
C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|
\]</span> 经验熵<span class="math inline">\(H_t(T)\)</span>为 <span class="math display">\[
H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}
\]</span> 设<span class="math inline">\(C(T)\)</span>为 <span class="math display">\[
C(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)=-\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{t k} \log \frac{N_{t k}}{N_{t}}
\]</span> 最后得到 <span class="math display">\[
C_{\alpha}(T)=C(T)+\alpha|T|
\]</span></p>
<ul>
<li><span class="math inline">\(C(T)\)</span>为样本点个数与经验熵的积，表示模型对训练数据的预测误差</li>
<li><span class="math inline">\(|T|\)</span>为模型复杂度
<ul>
<li>越大说明叶结点越多，树越复杂</li>
<li><span class="math inline">\(\alpha\)</span>越大，选择较简单的模型树</li>
</ul></li>
</ul>
<h3 id="pruning-algorithm">5.1 Pruning algorithm</h3>
<blockquote>
<p>算法 5.4 (树的剪枝算法) 输入: 生成算法产生的整个树 <span class="math inline">\(T\)</span>, 参数 <span class="math inline">\(\alpha\)</span>; 输出：修剪后的子树 <span class="math inline">\(T_{\alpha} \circ\)</span></p>
<p>（1）计算每个结点的经验熵。</p>
<p>（2）递归地从树的叶结点向上回缩。</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D.png" alt="image-20201216151045483" style="zoom:80%;" /> ​ 设一组叶结点回缩到其父结点之前与之后的整体树分别为 <span class="math inline">\(T_{B}\)</span> 与 <span class="math inline">\(T_{A}\)</span>, 其对应的 损失函数值分别是 <span class="math inline">\(C_{\alpha}\left(T_{B}\right)\)</span> 与 <span class="math inline">\(C_{\alpha}\left(T_{A}\right),\)</span> 如果 <span class="math display">\[
C_{\alpha}\left(T_{A}\right) \leqslant C_{\alpha}\left(T_{B}\right)
\]</span> 则进行剪枝，即将父结点变为新的叶结点。 (3) 返回 ( 2 )，直至不能继续为止，得到损失函数最小的子树 <span class="math inline">\(T_{\alpha^{\circ}}\)</span></p>
</blockquote>
<ul>
<li>动态规划的算法实现</li>
</ul>
<h2 id="cart-algorithm">6. CART Algorithm</h2>
<p>分类与回归树(classification and regression tree)是给定随机变量<span class="math inline">\(X\)</span>的条件下给出随机变量<span class="math inline">\(Y\)</span>的条件概率分布的学习方法。决策树是二叉树，左分支是“是”分支，右否。<strong>通过递归的二分每个特征，使得特征空间划分成有限个单元</strong>，在这些单元上确定概率分布。</p>
<h3 id="cart的生成">6.1 CART的生成</h3>
<h4 id="回归树生成">6.1.1 回归树生成</h4>
<h5 id="model-1">6.1.1.1 Model</h5>
<p>用<strong>平方误差最小化准则</strong>，进行特征选择，递归的构建二叉决策树。</p>
<p>对数据集<span class="math inline">\(D=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}\)</span>，<span class="math inline">\(Y\)</span>是连续变量，生成回归树。</p>
<p>回归树对应的是<strong>特征空间的划分</strong>以及<strong>划分单元上的输出值</strong>。</p>
<blockquote>
<p>定义 5.03 （回归树模型）</p>
<p>假设已将输入空间划分为 <span class="math inline">\(M\)</span> 个单元 <span class="math inline">\(R_{1}, R_{2}, \cdots, R_{M},\)</span> 并且在每个单元 <span class="math inline">\(R_{m}\)</span> 上 有一个固定的输出值 <span class="math inline">\(c_{m},\)</span> 于是回归树模型可表示为 <span class="math display">\[
f(x)=\sum_{m=1}^{M} c_{m} I\left(x \in R_{m}\right)
\]</span></p>
</blockquote>
<ul>
<li><p><span class="math inline">\(x\)</span>是特征向量</p></li>
<li><p>当输入空间划分确定，利用平方误差最小准则可以确定<span class="math inline">\(c_m\)</span>的最优值</p>
<ul>
<li><p>平方误差定义为<span class="math inline">\(\sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}\)</span>。注意<span class="math inline">\(x_i\in R_m\)</span></p></li>
<li><p>利用平方误差最小原则，单元 <span class="math inline">\(R_{m}\)</span> 上的 <span class="math inline">\(c_{m}\)</span> 的最优值 <span class="math inline">\(\hat{c}_{m}\)</span> 是 <span class="math inline">\(R_{m}\)</span> 上的所有输入实例 <span class="math inline">\(x_{i}\)</span> 对应的输出 <span class="math inline">\(y_{i}\)</span> 的均值，即 <span class="math display">\[
\hat{c}_{m}=\operatorname{ave}\left(y_{i} \mid x_{i} \in R_{m}\right)\nonumber
\]</span></p></li>
</ul></li>
</ul>
<p><strong>问题1：如何划分特征空间，即选择划分点</strong></p>
<p>通过启发式算法，选择第 <span class="math inline">\(j\)</span> 个变量 <span class="math inline">\(x^{(j)}\)</span>和它取的特征值 <span class="math inline">\(s\)</span>, 作为切分变量（splitting variable）和切分点 (splitting point)，并定义两个区域 <span class="math display">\[
\begin{align}
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\} \quad \text { 和 } \quad R_{2}(j, s)=\left\{x \mid x^{(j)}&gt;s\right\}
\end{align}
\]</span> 在一开始的选择基础上，寻找最优切分变量<span class="math inline">\(j\)</span>和最优切分点<span class="math inline">\(s\)</span>。具体的，求解 <span class="math display">\[
\begin{align}
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
\end{align}
\]</span></p>
<ul>
<li>(3)中，<span class="math inline">\(x^{(j)}\)</span>是第<span class="math inline">\(j\)</span>个特征，<span class="math inline">\(s\)</span>是此特征维度上的值。<span class="math inline">\(x_n\)</span>是第<span class="math inline">\(n\)</span>个输入向量。<a href="https://blog.csdn.net/u012328159/article/details/93667566">ref</a></li>
<li>(4)中，括号里的对于<span class="math inline">\(c_1,c_2\)</span>取极小值是已知的。</li>
<li>划分特征空间时是把高维矩形划分为2个子高维矩形。</li>
</ul>
<p><strong>问题2：如何确定好输出值<span class="math inline">\(c_m\)</span></strong></p>
<p>特征空间切分好后，根据最小化平方误差准则，得到相应的最优输出值 <span class="math display">\[
\hat{c}_{1}=\operatorname{ave}\left(y_{i} \mid x_{i} \in R_{1}(j, s)\right) \quad \text { 和 } \quad \hat{c}_{2}=\operatorname{ave}\left(y_{i} \mid x_{i} \in R_{2}(j, s)\right)
\]</span> <strong>总结：</strong>一开始，我们遍历所有输入特征，找到最优的切分特征变量<span class="math inline">\(j\)</span>，构成一对<span class="math inline">\((j,s)\)</span>，根据这个构成的超平面，讲特征空间划分成2个区域。对每个子区域重复过程，直到满足停止条件。这种回归树称为最小二乘回归树(least squares regression tree)</p>
<h5 id="algorithm">6.1.1.2 Algorithm</h5>
<blockquote>
<p>算法 5.5 (最小二乘回归树生成算法) 输入: 训练数据集 <span class="math inline">\(D\)</span>; 输出：回归树 <span class="math inline">\(f(x)\)</span> 。 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树:</p>
<p>（1）选择最优切分变量 <span class="math inline">\(j\)</span> 与切分点 <span class="math inline">\(s\)</span>, 求解 <span class="math display">\[
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
\]</span> 遍历变量 <span class="math inline">\(j\)</span>, 对固定的切分变量 <span class="math inline">\(j\)</span> 扫描切分点 <span class="math inline">\(s\)</span>, 选择使式 (14) 达到最小值的对<span class="math inline">\((j, s)\)</span></p>
<p>（2）用选定的对 <span class="math inline">\((j, s)\)</span> 划分区域并决定相应的输出值: <span class="math display">\[
\begin{array}{c}
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\}, \quad R_{2}(j, s)=\left\{x \mid x^{(j)}&gt;s\right\} \\
\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, \quad x \in R_{m}, \quad m=1,2
\end{array}
\]</span></p>
<p>（3）继续对两个子区域调用步骤 <span class="math inline">\((1),(2),\)</span> 直至满足停止条件。</p>
<p>（4）将输入空间划分为 <span class="math inline">\(M\)</span> 个区域 <span class="math inline">\(R_{1}, R_{2}, \cdots, R_{M},\)</span> 生成决策树: <span class="math display">\[
f(x)=\sum_{m=1}^{M} \hat{c}_{m} I\left(x \in R_{m}\right)
\]</span></p>
</blockquote>
<h5 id="example">6.1.1.3 Example</h5>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><span class="math inline">\(x^1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(x^2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(x^3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(x_1\)</span></td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(x_2\)</span></td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(x_3\)</span></td>
<td style="text-align: center;">1.6.</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2.75</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(x_4\)</span></td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2.25</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p><strong>Q：</strong>对以上数据集基于最小化平方误差生成二叉回归树</p>
<ol type="1">
<li><p>设<span class="math inline">\(j=x^1\)</span>，<span class="math inline">\(s=1.5\)</span>时</p>
<p><span class="math inline">\(c_1=1,\;c_2=2.5\)</span></p>
<p><span class="math inline">\(\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}= [(1-1)^2+(1-1)^2]+[(2-2.5)^2+(3-2.5)^2]=0.5\)</span></p>
<p><span class="math inline">\((j,s)=(x^1,1.6),\;0.67\)</span></p>
<p><span class="math inline">\((j,s)=(x^1,1.2),\;(j,s)=(x^1,1.8)\)</span>的结果一定偏大</p>
<p>对固定<span class="math inline">\(j=x^1\)</span>，<span class="math inline">\(s=1.5\)</span>是最佳切分点，<span class="math inline">\(error=0.5\)</span></p></li>
<li><p>设<span class="math inline">\(j=x^2\)</span></p>
<p>最佳切分为<span class="math inline">\((j,s)=(x^2,4),\;error=0.5\)</span></p></li>
<li><p>设<span class="math inline">\(j=x^3\)</span></p>
<p>最佳切分为<span class="math inline">\((j,s)=(x^3,2.75),\;error=2\)</span></p></li>
<li><p><span class="math inline">\(\min _{j, s}=\min[0.5,\;0.5,\;2]=0.5\)</span>，选择<span class="math inline">\((j,s)=(x^1,1.5)\)</span>作为最优划分。划分后的子集<span class="math inline">\(R_1,\;R_2\)</span>为</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><strong><span class="math inline">\(R_1\)</span>左分支</strong></th>
<th style="text-align: center;"><span class="math inline">\(x^1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(x^2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(x^3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(x_1\)</span></td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(x_2\)</span></td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong><span class="math inline">\(R_2\)</span>右分支</strong></td>
<td style="text-align: center;"><span class="math inline">\(x^1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(x^2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(x^3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(x_3\)</span></td>
<td style="text-align: center;">1.6.</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2.75</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(x_4\)</span></td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2.25</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(\hat{c}_1=1,\;\hat{c}_2=2.5\)</span></p></li>
<li><p>对左右分支继续迭代1-4的步骤，直到满足停止条件</p></li>
</ol>
<h4 id="分类树的生成">6.1.2 分类树的生成</h4>
<p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<h5 id="gini-index">6.1.2.1 Gini Index</h5>
<blockquote>
<p>定义 5.4 (基尼指数) <span class="math inline">\(\quad\)</span> 分类问题中，假设有 <span class="math inline">\(K\)</span> 个类，样本点属于第 <span class="math inline">\(k\)</span> 类的概率 为 <span class="math inline">\(p_{k}\)</span>, 则棍率分布的基尼指数定义为 <span class="math display">\[
\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
\]</span> 对于二类分类问题, 若样本点属于第 1 个类的概率是 <span class="math inline">\(p,\)</span> 则概率分布的基尼指数为 <span class="math display">\[
\operatorname{Gini}(p)=2 p(1-p)
\]</span> 对于给定的样本集合 <span class="math inline">\(D,\)</span> 其基尼指数为 <span class="math display">\[
\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}
\]</span> 这里, <span class="math inline">\(C_{k}\)</span> 是 <span class="math inline">\(D\)</span> 中属于第 <span class="math inline">\(k\)</span> 类的样本子集， <span class="math inline">\(K\)</span> 是类的个数。</p>
<p>如果样本集合 <span class="math inline">\(D\)</span> 根据<strong>特征 <span class="math inline">\(A\)</span></strong> 是否取某一可能值 <span class="math inline">\(a\)</span> 被分割成 <span class="math inline">\(D_{1}\)</span> 和 <span class="math inline">\(D_{2}\)</span> 两部分，即 <span class="math display">\[
D_{1}=\{(x, y) \in D \mid A(x)=a\}, \quad D_{2}=D-D_{1}\nonumber
\]</span> 则在特征 <span class="math inline">\(A\)</span> 的条件下，集合 <span class="math inline">\(D\)</span> 的基尼指数定义为 <span class="math display">\[
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
\]</span></p>
</blockquote>
<ul>
<li>基尼指数表示集合的不确定性。基尼指数越大，集合的不确定性越大。和熵相似。</li>
<li>二元分类中基尼指数、单位比特熵和分类误差的关系。x轴：概率p。y轴：损失。<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%9F%BA%E5%B0%BC%E3%80%81%E7%86%B5%E4%B8%8E%E5%88%86%E7%B1%BB%E8%AF%AF%E5%B7%AE.png" alt="image-20201208142218988" style="zoom:50%;" /></li>
</ul>
<h5 id="algorithm-1">6.1.2.2 Algorithm</h5>
<blockquote>
<p>算法 5.6 (CART 生成算法)</p>
<p>输入: 训练数据集 <span class="math inline">\(D,\)</span> 停止计算的条件; 输出: CART 决策树。</p>
<p>根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树:</p>
<p>（1）设结点的训练数据集为 <span class="math inline">\(D\)</span>, 计算现有特征对该数据集的基尼指数。此时，对每一个特征 <span class="math inline">\(A,\)</span> 对其可能取的每个值 <span class="math inline">\(a,\)</span> 根据样本点对 <span class="math inline">\(A=a\)</span> 的测试为“是”或“否”，将<span class="math inline">\(D\)</span> 分割成 <span class="math inline">\(D_{1}\)</span> 和 <span class="math inline">\(D_{2}\)</span> 两部分，利用式 (20) 计算 <span class="math inline">\(A=a\)</span> 时的基尼指数。</p>
<p>（2）在所有可能的特征 <span class="math inline">\(A\)</span> 以及它们所有可能的切分点 <span class="math inline">\(a\)</span> 中，选择基尼指数最小的 持征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现 结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。</p>
<p>（3）对两个子结点递归地调用 ( 1 )， (2) , 直至满足停止条件。</p>
<p>（4）生成 CART 决策树。</p>
</blockquote>
<h3 id="cart的剪枝">6.2 CART的剪枝</h3>
<p><strong>步骤：</strong></p>
<ol type="1">
<li>对生成算法产生的决策树<span class="math inline">\(T_0\)</span>底端开始不断剪枝，直到<span class="math inline">\(T_0\)</span>的根结点，形成子树序列<span class="math inline">\(\left\{T_{0}, T_{1}, \cdots, T_{n}\right\}\)</span></li>
<li>通过交叉验证法，在独立的验证数据集上对字数序列进行测试，选择最优子书。</li>
</ol>
<h4 id="剪枝成一个子树序列">6.2.1 剪枝成一个子树序列</h4>
<p>子树的损失函数为 <span class="math display">\[
C_{\alpha}(T)=C(T)+\alpha|T|\nonumber
\]</span></p>
<ul>
<li><p><span class="math inline">\(C(T)=\sum_{t=1}^{|T|} N_{t}\left(1-\sum_{k=1}^{K}\left(\frac{N_{t k}}{N_{t}}\right)^{2}\right),|T|\)</span> 是叶结点个数，<span class="math inline">\(K\)</span> 是类别个数</p></li>
<li><p>定义推导同上</p></li>
<li><p>对固定<span class="math inline">\(\alpha\)</span>，<strong>唯一存在</strong>最优子树<span class="math inline">\(T_{\alpha}\)</span>，使得损失函数<span class="math inline">\(C_{\alpha}(T)\)</span>最小。</p>
<ul>
<li>此处“最优”的意义是指使得损失函数最小</li>
<li><span class="math inline">\(\alpha\)</span>越大，最优子树<span class="math inline">\(T_{\alpha}\)</span>越小。当 <span class="math inline">\(\alpha \rightarrow \infty\)</span> 时，叶结点不断被剪，根结点组成的单结点树是最优的。</li>
</ul></li>
</ul>
<p>对一个整体树<span class="math inline">\(T_0\)</span>，它的子树是<strong>有限个</strong>的。因此，对一个连续参数<span class="math inline">\(\alpha\)</span>，我们得到了最优子树<span class="math inline">\(T&#39;(\alpha)\)</span>。<span class="math inline">\(\alpha\)</span>不断增大，在增大到<strong>跳跃点<span class="math inline">\(\alpha&#39;\)</span></strong>之前，<span class="math inline">\(T&#39;(\alpha)\)</span>依然是最优子树。即<span class="math inline">\(T&#39;(\alpha)=T&#39;(\alpha+\Delta\alpha)\)</span>。再跳跃点之后，易知最优子树<span class="math inline">\(T&#39;(\alpha’)\in T&#39;(\alpha)\)</span>。</p>
<p>上面可以表述为：将 <span class="math inline">\(\alpha\)</span> 从小增大, <span class="math inline">\(0=\alpha_{0}&lt;\)</span><span class="math inline">\(\alpha_{1}&lt;\cdots&lt;\alpha_{n}&lt;+\infty,\)</span> 产生一系列的区间 <span class="math inline">\(\left[\alpha_{i}, \alpha_{i+1}\right), i=0,1, \cdots, n ;\)</span> 剪枝得到的子树序列对应着区间 <span class="math inline">\(\alpha \in\left[\alpha_{i}, \alpha_{i+1}\right), i=0,1, \cdots, n\)</span> 的最优子树序列 <span class="math inline">\(\left\{T_{0}, T_{1}, \cdots, T_{n}\right\},\)</span> <strong>序列中的子树是嵌套的</strong>。</p>
<p>具体来说，从整体树<span class="math inline">\(T_0\)</span>，<span class="math inline">\(\alpha=0\)</span>开始剪枝。对<span class="math inline">\(T_0\)</span>内的任意内部结点<span class="math inline">\(t\)</span>：</p>
<p>以<span class="math inline">\(t\)</span>为单结点树的损失函数为 <span class="math display">\[
C_{\alpha}(t)=C(t)+\alpha
\]</span> 以 <span class="math inline">\(t\)</span> 为根结点的子树 <span class="math inline">\(T_{t}\)</span> 的损失函数为 <span class="math display">\[
C_{\alpha}\left(T_{t}\right)=C\left(T_{t}\right)+\alpha\left|T_{t}\right|
\]</span> 当 <span class="math inline">\(\alpha=0\)</span> 及 <span class="math inline">\(\alpha\)</span> 充分小时，有不等式 <span class="math display">\[
C_{\alpha}\left(T_{t}\right)&lt;C_{\alpha}(t)
\]</span> 当 <span class="math inline">\(\alpha\)</span> 增大时，在某一 <span class="math inline">\(\alpha\)</span> 有 <span class="math display">\[
\begin{aligned}
C_{\alpha}\left(T_{t}\right) &amp;=C_{\alpha}(t)\\
\alpha &amp;=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
\end{aligned}
\]</span></p>
<p><strong>此时<span class="math inline">\(T_t\)</span>和<span class="math inline">\(t\)</span>有相同的损失函数值，因为<span class="math inline">\(t\)</span>的结点更少，所以取<span class="math inline">\(t\)</span>，剪去以 <span class="math inline">\(t\)</span> 为根结点的子树 <span class="math inline">\(T_{t}\)</span> </strong></p>
<p>根据这个性质，我们可以找到系列区间以及对应的最优子树序列</p>
<p>对<span class="math inline">\(T_0\)</span>的每一个内部节点<span class="math inline">\(t\)</span>，计算 <span class="math display">\[
g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
\]</span> <span class="math inline">\(g(t)\)</span>表示剪枝后整体损失函数减小的程度。在 <span class="math inline">\(T_{0}\)</span> 中剪去 <span class="math inline">\(g(t)\)</span> 最小的 <span class="math inline">\(T_{t},\)</span> 将得到的子树作为 <span class="math inline">\(T_{1},\)</span> 同时将最小的 <span class="math inline">\(g(t)\)</span> 设为 <span class="math inline">\(\alpha_{1}\)</span>。$ T_{1}$ 为区间 <span class="math inline">\(\left[\alpha_{1}, \alpha_{2}\right)\)</span> 的最优子树。循环直到得到根结点。</p>
<ul>
<li>在这个过程中，<span class="math inline">\(g(t)=\alpha\)</span>是不断增大的？</li>
</ul>
<h4 id="交叉验证">6.2.2 交叉验证</h4>
<p>利用独立的验证数据集，测试子树序列 <span class="math inline">\(\left\{T_{0}, T_{1}, \cdots, T_{n}\right\}\)</span>中每个子树的平方误差/基尼指数，选择最优决策树<span class="math inline">\(T_{\alpha}\)</span>。</p>
<ul>
<li>子树序列 <span class="math inline">\(\left\{T_{0}, T_{1}, \cdots, T_{n}\right\}\)</span>在剪枝的时候是对应<span class="math inline">\(\alpha\)</span>的最优子序列</li>
</ul>
<h4 id="algorithm-2">6.2.3 Algorithm</h4>
<blockquote>
<p>算法 5.7 (CART 剪枝算法) 输入: CART 算法生成的决策树 <span class="math inline">\(T_{0}\)</span>; 输出：最优决策树 <span class="math inline">\(T_{\alpha \circ}\)</span></p>
<p>(1)设 <span class="math inline">\(k=0, T=T_{0}\)</span> 。</p>
<p>(2)设 <span class="math inline">\(\alpha=+\infty\)</span> 。</p>
<p>(3)自下而上地对各内部结点 <span class="math inline">\(t\)</span> 计算 <span class="math inline">\(C\left(T_{t}\right),\left|T_{t}\right|\)</span> 以及 <span class="math display">\[
\begin{aligned}
g(t) &amp;=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1} \\
\alpha &amp;=\min (\alpha, g(t))
\end{aligned}
\]</span> 这里, <span class="math inline">\(T_{t}\)</span> 表示以 <span class="math inline">\(t\)</span> 为根结点的子树, <span class="math inline">\(C\left(T_{t}\right)\)</span> 是对训练数据的预测误差, <span class="math inline">\(\left|T_{t}\right|\)</span> 是 <span class="math inline">\(T_{t}\)</span>的叶结点个数。</p>
<p>(4)对 <span class="math inline">\(g(t)=\alpha\)</span> 的内部结点 <span class="math inline">\(t\)</span> 进行剪枝，并对叶结点 <span class="math inline">\(t\)</span> 以多数表决法决定其类，得到树 <span class="math inline">\(T\)</span> 。</p>
<p>(5)设 <span class="math inline">\(k=k+1, \alpha_{k}=\alpha, T_{k}=T_{\circ}\)</span></p>
<p>(6)如果 <span class="math inline">\(T_{k}\)</span> 不是由根结点及两个叶结点构成的树，则回到步骤 (2)<span class="math inline">\(;\)</span> 否则令 <span class="math inline">\(T_{k}=T_{n}\)</span></p>
<p>(7)采用交义验证法在子树序列 <span class="math inline">\(T_{0}, T_{1}, \cdots, T_{n}\)</span> 中选取最优子树 <span class="math inline">\(T_{\alpha^{\circ}}\)</span></p>
</blockquote>
<h2 id="question">7. Question</h2>
<ol type="1">
<li>为什么<span class="math inline">\(C(T)\)</span>能表示模型对训练数据的预测误差</li>
<li><del>正则化的极大似然估计？</del></li>
<li><del><strong>1.2</strong>中的'构成一个条件概率分布'，不是叶结点咋办？</del></li>
<li><del>6.1.1 启发式算法？</del></li>
<li>CART决策时有没有可能对一个特征二叉再接个二叉，成<span class="math inline">\(2^2\)</span>个叉？</li>
</ol>
<h2 id="code">8. Code</h2>
<h3 id="id3-create-test-visualization">8.1 ID3: create, test, visualization</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">CARTClassification</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">CARTRegression</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ID3_create</span>(<span class="params">self, train_set, features, labels, tol=[<span class="number">0.1</span>, <span class="number">2</span>], visible=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        create ID3 tree</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray. m: samples, n: features</span></span><br><span class="line"><span class="string">        :param features: n size vector</span></span><br><span class="line"><span class="string">        :param labels: m size ndarray</span></span><br><span class="line"><span class="string">        :param tol: tolerate for pre-pruning</span></span><br><span class="line"><span class="string">        :return: ID3 tree in dict type</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        three conditions to stop iteration:</span></span><br><span class="line"><span class="string">          1. all labelss are the same</span></span><br><span class="line"><span class="string">          2. no feature</span></span><br><span class="line"><span class="string">          3. info_gain &lt; tol[0], samples &gt; tol[1]. pre-pruning</span></span><br><span class="line"><span class="string">          4. same training values but different labels</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        train_set = np.array(train_set)</span><br><span class="line">        labels = np.array(labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(np.unique(labels)) == <span class="number">1</span>:  <span class="comment"># condition 1</span></span><br><span class="line">            <span class="keyword">return</span> labels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> train_set.shape[<span class="number">1</span>] == <span class="number">0</span>:  <span class="comment"># condition 2</span></span><br><span class="line">            <span class="keyword">return</span> np.sort(labels)[-<span class="number">1</span>]  <span class="comment"># return the most frequency value</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># condition 3 &amp; 4</span></span><br><span class="line">        <span class="comment"># not finished</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get best feature</span></span><br><span class="line">        best_feature_index, best_info_gain = self.ID3_best_feature(train_set, labels)</span><br><span class="line">        best_feature_name = features[best_feature_index]</span><br><span class="line">        print(<span class="string">&#x27;best selected feature is &#x27;</span>, best_feature_name, <span class="string">&#x27;, its information gain is &#x27;</span>, best_info_gain)</span><br><span class="line"></span><br><span class="line">        ID3Tree = &#123;best_feature_name: &#123;&#125;&#125;  <span class="comment"># return feature name as a dict key</span></span><br><span class="line">        <span class="comment"># return unique values under the feature and as the node(key)</span></span><br><span class="line">        tree_nodes = np.unique(train_set.T[best_feature_index])</span><br><span class="line">        <span class="comment"># small feature set for dealing feature set depending on its index</span></span><br><span class="line">        features = np.delete(features, best_feature_index)</span><br><span class="line">        <span class="comment"># iteration in these nodes</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> tree_nodes:</span><br><span class="line">            train_sample_index = train_set.T[best_feature_index] == node</span><br><span class="line">            node_labels = labels[train_sample_index]</span><br><span class="line">            <span class="comment"># small train set with node feature&#x27;s column equal node value</span></span><br><span class="line">            node_train_set = self.spilt_dataset(train_set, best_feature_index, node)</span><br><span class="line">            <span class="comment"># small train set without node feature</span></span><br><span class="line">            node_train_set = np.delete(node_train_set, best_feature_index, axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># iteration</span></span><br><span class="line">            ID3Tree[best_feature_name][node] = self.ID3_create(node_train_set, features, node_labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> visible <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            treePlotter.ID3_Tree(ID3Tree)</span><br><span class="line">        <span class="keyword">return</span> ID3Tree</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">array</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        calculate bit entropy</span></span><br><span class="line"><span class="string">        :param array: 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: entropy in bit</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        count_array = np.unique(array, return_counts=<span class="literal">True</span>)[<span class="number">1</span>]  <span class="comment"># unique values and its occurrences</span></span><br><span class="line">        probability = count_array / array.size  <span class="comment"># probability of values</span></span><br><span class="line">        h_p = np.dot(-probability, np.log2(probability))  <span class="comment"># entropy</span></span><br><span class="line">        <span class="keyword">return</span> h_p</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conditional_entropy</span>(<span class="params">Y, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        get conditional entropy H(Y|X)</span></span><br><span class="line"><span class="string">        :param Y: random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :param X: given random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: conditional entropy of Y given X, H(Y|X)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        Y = np.array(Y)</span><br><span class="line">        X = np.array(X)</span><br><span class="line">        hY_X = <span class="number">0</span>  <span class="comment"># initialization</span></span><br><span class="line">        X_value, X_count = np.unique(X, return_counts=<span class="literal">True</span>)  <span class="comment"># unique values and its occurrences</span></span><br><span class="line">        <span class="keyword">for</span> xi <span class="keyword">in</span> X_value:</span><br><span class="line">            index = np.argwhere(X == xi)  <span class="comment"># get index of X=xi</span></span><br><span class="line">            p_xi = index.size / X.size  <span class="comment"># P(X=xi)</span></span><br><span class="line">            Yi = Y[index]  <span class="comment"># get yi given xi</span></span><br><span class="line">            hYi_xi = DecisionTree.entropy(np.array(Yi))  <span class="comment"># H(Y|X=xi)</span></span><br><span class="line">            hY_X += p_xi * hYi_xi</span><br><span class="line">        <span class="keyword">return</span> hY_X</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain</span>(<span class="params">Y, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        get information gain G(Y,X)</span></span><br><span class="line"><span class="string">        :param Y: random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :param X: given random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: information gain of Y given X, G(Y|X)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> DecisionTree.entropy(Y) - DecisionTree.conditional_entropy(Y, X)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spilt_dataset</span>(<span class="params">dataset, colume, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dataset with small samples</span></span><br><span class="line"><span class="string">        :param dataset: m*n ndarray</span></span><br><span class="line"><span class="string">        :param colume:  axis</span></span><br><span class="line"><span class="string">        :param value: compared value</span></span><br><span class="line"><span class="string">        :return: l*n ndarray, l&lt;m</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dataset = pd.DataFrame(dataset)</span><br><span class="line">        df = dataset[dataset[colume] == value]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.array(df)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ID3_best_feature</span>(<span class="params">train_set, labels</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        return the feature with the highest infomation gain</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray. m: samples, n: features</span></span><br><span class="line"><span class="string">        :param labels: m size ndarray.</span></span><br><span class="line"><span class="string">        :return: best feature index and its infomation gain</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        features = train_set.shape[<span class="number">1</span>]  <span class="comment"># number of features</span></span><br><span class="line">        tmp = np.ones(features) * -<span class="number">1</span>  <span class="comment"># store info gain</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(features):  <span class="comment"># calculate info gain of each features</span></span><br><span class="line">            feature_list = train_set.T[i]</span><br><span class="line">            gain = DecisionTree.info_gain(labels, feature_list)</span><br><span class="line">            tmp[i] = gain</span><br><span class="line">            print(<span class="string">&quot;the info gain of %d th feature in ID3 is: %.3f&quot;</span> % (i, gain))</span><br><span class="line">        best_feature = np.argmax(tmp)</span><br><span class="line">        best_info_gain = tmp[best_feature]</span><br><span class="line">        <span class="keyword">return</span> best_feature, best_info_gain</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">tree, sample, features</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param tree: dict</span></span><br><span class="line"><span class="string">        :param sample: 1-d ndarray</span></span><br><span class="line"><span class="string">        :param features: 1-d ndarray</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        first_str = <span class="built_in">list</span>(tree.keys())[<span class="number">0</span>] <span class="comment"># root name</span></span><br><span class="line">        small_tree = tree[first_str] <span class="comment">#</span></span><br><span class="line">        feature_index = features.index(first_str)</span><br><span class="line">        label = <span class="string">&#x27;None&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> small_tree.keys():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(sample[feature_index]) == key:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">type</span>(small_tree[key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:</span><br><span class="line">                    label = DecisionTree.classify(small_tree[key], sample, features)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    label = small_tree[key]</span><br><span class="line">        <span class="keyword">return</span> label</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<h2 id="reference">9. Reference</h2>
<p><a href="https://blog.csdn.net/u012328159/article/details/93667566">分类与回归树（classification and regression tree，CART）之回归</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/101721467">回归树</a></p>
<p><a href="https://blog.csdn.net/u012328159/article/details/93667566">分类与回归树（classification and regression tree，CART）之回归</a></p>
<p><a href="https://github.com/Erikfather/Decision_tree-python">Erikfather/Decision_tree-python</a></p>
<p><a href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>
]]></content>
      <categories>
        <category>study</category>
        <category>《统计学习方法》</category>
      </categories>
      <tags>
        <tag>C4.5</tag>
        <tag>CART</tag>
        <tag>Decision Tree</tag>
        <tag>Entropy</tag>
        <tag>Gini Index</tag>
        <tag>ID3</tag>
        <tag>Info Gain</tag>
      </tags>
  </entry>
  <entry>
    <title>[统计数学方法] 4. 朴素贝叶斯 Naive Bayes</title>
    <url>/slm004/</url>
    <content><![CDATA[<p>朴素贝叶斯 Naive Bayes是基于Bayes Theorem和特征条件独立假设的回归和分类方法，普遍用于垃圾邮件检测等，本文主要介绍分类方法。 <a id="more"></a></p>
<h2 id="bayes-theorem">0. Bayes Theorem</h2>
<p><span class="math display">\[
\underbrace{P(X|Y)}_{posterior}=\frac{\overbrace{P(Y|X)}^{likelihood}\overbrace{P(X)}^{prior}}{\underbrace{P(Y)}_{evidence}}=\frac{\overbrace{P(Y|X)}^{likelihood}\overbrace{P(X)}^{prior}}{\underbrace{\sum\limits_x P(Y|X)P(X)}_{evidence}}
\]</span></p>
<ul>
<li><span class="math inline">\(P(X)\)</span>：先验概率prior</li>
<li><span class="math inline">\(P(X|Y)\)</span>：后验概率posterior</li>
<li><span class="math inline">\(P(Y|X)\)</span>：似然likelihood。</li>
</ul>
<h2 id="model">1. Model</h2>
<ul>
<li><p>输入空间<span class="math inline">\(\mathcal{X} \subseteq \mathbf{R}^{n}\)</span> 为 <span class="math inline">\(n\)</span> 维向量的集合，输出空间为类集合<span class="math inline">\(\mathcal{Y}=\left\{c_{1}, c_{2}, \cdots, c_{K}\right\}\)</span></p></li>
<li><p>输入特征向量<span class="math inline">\(x\)</span>，输出类标记<span class="math inline">\(y\)</span></p></li>
<li><p><span class="math inline">\(X\)</span>是输入空间上的随机向量，<span class="math inline">\(Y\)</span>是输出空间上的随机变量</p></li>
<li><p>训练数据集 <span class="math display">\[
\begin{aligned}
T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}
\end{aligned}
\]</span> 由 <span class="math inline">\(P(X, Y)\)</span> 独立同分布产生。</p>
<ul>
<li>这边的独立指的是向量<span class="math inline">\(x_1\)</span>与<span class="math inline">\(x_2\)</span>等之间的独立，不是特征条件独立。</li>
</ul></li>
</ul>
<h3 id="学习联合概率分布pxy">1.1 学习联合概率分布<span class="math inline">\(P(X,Y)\)</span></h3>
<ol type="1">
<li><p><span class="math inline">\(P(X, Y)=P(Y) P(X \mid Y)=P(X) P(Y \mid X)\)</span>，用第一个等式</p></li>
<li><p><strong>先验概率分布</strong> <span class="math display">\[
P\left(Y=c_{k}\right), \quad k=1,2, \cdots, K
\]</span></p>
<ul>
<li>由测试集可知，有多少概率/比例的<span class="math inline">\(c_1\)</span>标签</li>
</ul></li>
<li><p><strong>条件概率分布</strong> <span class="math display">\[
P\left(X=x \mid Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right),\\ \quad k=1,2, \cdots, K
\]</span></p>
<ul>
<li><p>特征向量相等<span class="math inline">\(\Longrightarrow\)</span>每一维度上都相等</p></li>
<li><p>朴素贝叶斯法对条件概率做条件独立性假设，即 <span class="math display">\[
\begin{align}
P\left(X=x \mid Y=c_{k}\right) &amp;=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right)\nonumber \\
&amp;=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
\end{align}
\]</span></p></li>
<li><p>如果不加独立分布假设，那么(1,1,1,1)和(1,1,1,2)独立，没有可以利用的信息。需要更大的测试集。</p></li>
</ul></li>
<li><p><strong>重复“词语”的处理</strong></p>
<p>我们得到一个垃圾邮件向量<span class="math inline">\(x=[&#39;发票&#39;，‘发票’，‘发票‘，‘充值’，’购物‘]\)</span>，如何处理出现3次的“发票”</p>
<ol type="1">
<li>多项式模型
<ul>
<li>每一个“发票”都是独立的，统计多次，即<span class="math inline">\(P(X=(&#39;发票&#39;，‘发票’，‘发票‘)|Y=c_k)=P^3(&#39;发票|Y=c_k&#39;)\)</span></li>
</ul></li>
<li>伯努利模型
<ul>
<li>只分成出现，不出现的二元情况，即<span class="math inline">\(P(X=(&#39;发票&#39;，‘发票’，‘发票‘)|Y=c_k)=P(&#39;发票|Y=c_k&#39;)\)</span></li>
</ul></li>
<li>混合模型
<ul>
<li>计算单一句子的概率时用伯努利模型</li>
<li>计算全文词语的概率时用多项式模型</li>
</ul></li>
<li>高斯模型
<ul>
<li>适用于连续变量</li>
<li>假设：在给定一个类别<span class="math inline">\(c_k\)</span>后，各个特征符合正态分布</li>
<li><span class="math inline">\(P\left(x^{(i)} \mid y=c_k\right)=\frac{1}{\sqrt{2 \pi \sigma_{y}^{2}}} \exp \left(-\frac{\left(x^{(i)}-\mu_{y}\right)^{2}}{2 \sigma_{y}^{2}}\right)\)</span>
<ul>
<li><span class="math inline">\(\mu_y:\)</span> 在类别为 <span class="math inline">\(y\)</span> 的样本中，特征 <span class="math inline">\(x^{(i)}\)</span> 的均值。 <span class="math inline">\(\sigma_{y}:\)</span> 在类别为 <span class="math inline">\(y\)</span> 的样本中，特征 <span class="math inline">\(x^{(i)}\)</span> 的标准差。</li>
</ul></li>
</ul></li>
</ol></li>
</ol>
<h3 id="计算后验分布">1.2 计算后验分布</h3>
<p>通过贝叶斯定理，得到在给定了一个特征向量的情况下，标签为<span class="math inline">\(c_k\)</span>的概率。 <span class="math display">\[
\begin{align}
P(Y \mid X)&amp;=\frac{P(X, Y)}{P(X)}=\frac{P(Y) P(X \mid Y)}{\sum_{Y} P(Y) P(X \mid Y)}\\
P\left(Y=c_{k} \mid X=x\right)&amp;=\frac{P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}\\
&amp;=\frac{P\left(Y=c_{k}\right) \prod P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum_{k} (P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right))}, \quad k=1,2, \cdots, K
\end{align}
\]</span></p>
<ul>
<li>把(1)带入到(3)，得到等式(4)</li>
</ul>
<h3 id="后验概率最大化">1.3 后验概率最大化</h3>
<p>得到了概率表达式之后，我们便去寻找使得概率<span class="math inline">\(P\left(Y=c_{k} \mid X=x\right)\)</span>最大的标签<span class="math inline">\(c_k\)</span></p>
<p>朴素贝叶斯分类器： <span class="math display">\[
\begin{align}
y=f(x)=\arg \max _{c_{k}} \frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right) \\ }{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}
\end{align}
\]</span> (5)中的分母本质上为<span class="math inline">\(P(X=x)\)</span>，与<span class="math inline">\(c_k\)</span>的取值无关，故(5)可表示成 <span class="math display">\[
y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
\]</span></p>
<h4 id="含义">1.3.1 含义</h4>
<ul>
<li>后验概率最大化等价于期望风险最小化</li>
</ul>
<h3 id="朴素贝叶斯法中的参数估计">1.4 朴素贝叶斯法中的参数估计</h3>
<p>为了学习朴素贝叶斯法中的<span class="math inline">\(P\left(Y=c_{k}\right)\)</span>与<span class="math inline">\(P\left(X=x \mid Y=c_{k}\right)\)</span></p>
<h4 id="极大似然估计">1.4.1 极大似然估计</h4>
<p>核心思想为根据以有的测试数据集，得到每一类的频率，即为概率</p>
<blockquote>
<p>算法 4.1 (朴素贝叶斯算法 (Naive Bayes algorithm) )</p>
<p>输入: 训练数据 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中 <span class="math inline">\(x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots,\right.\)</span> <span class="math inline">\(\left.x_{i}^{(n)}\right)^{\mathrm{T}}, x_{i}^{(j)}\)</span> 是第 <span class="math inline">\(i\)</span> 个样本的第 <span class="math inline">\(j\)</span> 个特征, <span class="math inline">\(x_{i}^{(j)} \in\left\{a_{j 1}, a_{j 2}, \cdots, a_{j S_{j}}\right\}, a_{j l}\)</span> 是第 <span class="math inline">\(j\)</span> 个特 征可能取的第 <span class="math inline">\(l\)</span> 个值, <span class="math inline">\(j=1,2, \cdots, n, l=1,2, \cdots, S_{j}, y_{i} \in\left\{c_{1}, c_{2}, \cdots, c_{K}\right\} ;\)</span> 实例 <span class="math inline">\(x\)</span>; 输出：实例 <span class="math inline">\(x\)</span> 的分类。</p>
<p>（1）计算先验概率及条件概率 <span class="math display">\[
\begin{align}
P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
P\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right) }{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)} \\
j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K
\end{align}
\]</span> （2）对于给定的实例 <span class="math inline">\(x=\left(x^{(1)}, x^{(2)}, \cdots, x^{(n)}\right)^{\mathrm{T}},\)</span> 计算 <span class="math display">\[
\begin{align}
P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right), \quad k=1,2, \cdots, K
\end{align}
\]</span> （3）确定实例 <span class="math inline">\(x\)</span> 的类 <span class="math display">\[
\begin{align}
y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
\end{align}
\]</span></p>
</blockquote>
<h4 id="贝叶斯估计">1.4.2 贝叶斯估计</h4>
<p><strong>问题</strong>：通过极大似然估计，可能会出现(10)中的几个<span class="math inline">\(P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)\)</span>为0（我们把“发票”当成了一个特征，但句子中没有出现“发票”这一词），这个情况很常见</p>
<p><strong>原因</strong>：训练集太少，没有满足大数定律。</p>
<p><strong>解决</strong>：</p>
<p>给定先验概率，赋值为<span class="math inline">\(\lambda \ge0\)</span></p>
<ul>
<li>在一个训练集中，对一个本应该为0 的概率赋予一个较小值，那么就会降低其他词语的概率</li>
<li><span class="math inline">\(\lambda=0\)</span>：极大似然估计</li>
<li><span class="math inline">\(\lambda=1\)</span>：Laplace Smoothing</li>
<li>”加上“基础的概率<span class="math inline">\(1/K,\;1/S_j\)</span></li>
</ul>
<p><strong>算法</strong>：</p>
<ul>
<li><p>计算先验概率的等式(6)变成 <span class="math display">\[
P_{\lambda}\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+\lambda}{N+K \lambda}
\]</span></p></li>
<li><p>计算条件概率的等式(7)变成 <span class="math display">\[
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits_{i=1}^NI(x_i^{j}=a_{jl},y_j=c_k)+\lambda}{\sum\limits_{i=1}^NI(y_j=c_k)+S_j\lambda}
\]</span></p></li>
</ul>
<h2 id="trick">3. Trick</h2>
<h3 id="log-function">3.1 Log function</h3>
<p>在等式(10)中，我们可以取对数，把乘法运算转化为加法运算，提高计算速度。</p>
<h2 id="coding">4. Coding</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayes</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.model=<span class="string">&#x27;Naive Bayes&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, Py, Pxi_y, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param Py: 先验概率</span></span><br><span class="line"><span class="string">        :param Pxi_y: 条件概率</span></span><br><span class="line"><span class="string">        :param x: test align</span></span><br><span class="line"><span class="string">        :return: 通过极大似然法得出最大概率的标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        (labelNumber,featureNumber,features)=Pxi_y.shape</span><br><span class="line">        <span class="comment"># p存放所有的似然</span></span><br><span class="line">        p=Py</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labelNumber):</span><br><span class="line">            <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(featureNumber):</span><br><span class="line">                <span class="built_in">sum</span> += Pxi_y[i][j][x[j]] <span class="comment"># 因为转化成了log，所以累乘变累加</span></span><br><span class="line">            p[i] += <span class="built_in">sum</span></span><br><span class="line">        <span class="keyword">return</span> np.argmax(p)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getAllProbability</span>(<span class="params">self, labelalign, labelNumber, trainalign, featureNumber, features, para=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param labelalign: [int,int,...,int] 1*n</span></span><br><span class="line"><span class="string">        :param labelNumber: 一共有N个标签</span></span><br><span class="line"><span class="string">        :param trainalign: train data, ~*m</span></span><br><span class="line"><span class="string">        :param featureNumber: 一共有m个特征标签</span></span><br><span class="line"><span class="string">        :param features: int k, 对第m个特征标签，有k_m个特征。</span></span><br><span class="line"><span class="string">        :param para: lambda</span></span><br><span class="line"><span class="string">        :return: 先验概率Py, 条件概率Pxi_y</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        目前（没有完善load function）对于输入的形式非常的苛刻，也出现了重复的参数，日后完善。</span></span><br><span class="line"><span class="string">        example:</span></span><br><span class="line"><span class="string">        trainalign=[[0,0],[0,1],[0,1],[0,0],[0,0],[1,0],[1,1],[1,1],[1,2],[1,2],[2,2],[2,1],[2,1],[2,2],[2,2]]</span></span><br><span class="line"><span class="string">        trainalign=np.asalign(trainalign)</span></span><br><span class="line"><span class="string">        featureNumber=2</span></span><br><span class="line"><span class="string">        features=3</span></span><br><span class="line"><span class="string">        labelalign=[0,0,1,1,0,0,0,1,1,1,1,1,1,1,0]</span></span><br><span class="line"><span class="string">        labelalign=np.asalign(labelalign)</span></span><br><span class="line"><span class="string">        labelNumber=2</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 一共有N个标签</span></span><br><span class="line">        Py = np.zeros((labelNumber, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 对标签为y，特征为m，第？个特征取值</span></span><br><span class="line">        Pxi_y = np.zeros((labelNumber, featureNumber, features))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算先验概率Py</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labelNumber):</span><br><span class="line">            numerator = np.<span class="built_in">sum</span>(labelalign==i) + para</span><br><span class="line">            denominator = <span class="built_in">len</span>(labelalign) + para * labelNumber</span><br><span class="line">            Py[i] = numerator/denominator</span><br><span class="line">        Py = np.log(Py) <span class="comment"># 取对数加快运算</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算条件概率Pxi_y</span></span><br><span class="line">        (a,b) = trainalign.shape <span class="comment"># a:几条数据  b:几个特征</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(a): <span class="comment"># 第几条数据，选取他们的label和train data align</span></span><br><span class="line">            label = labelalign[i]</span><br><span class="line">            x=trainalign[i]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(featureNumber): <span class="comment"># 对x的每一个标签上的值，出现一次加一次</span></span><br><span class="line">                Pxi_y[label][j][x[j]] +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> <span class="built_in">range</span>(labelNumber): <span class="comment"># 得到次数后计算概念</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(featureNumber):</span><br><span class="line">                denominator = np.<span class="built_in">sum</span>(Pxi_y[label][i]) + features * para</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(features):</span><br><span class="line">                    numerator = Pxi_y[label][i][j] + para</span><br><span class="line">                    Pxi_y[label][i][j]=np.log(numerator/denominator)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Py, Pxi_y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loadData</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">modelTest</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<h2 id="proof">5. Proof</h2>
<h3 id="用极大似然法推出等式6">5.1 用极大似然法推出等式(6)</h3>
<figure>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-4%20Proof%201.png" alt="253e0884011c7a303fda71c8962cfa6" /><figcaption aria-hidden="true">253e0884011c7a303fda71c8962cfa6</figcaption>
</figure>
<h2 id="reference">6. Reference</h2>
<p><a href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a><a href="https://github.com/Dod-o/Statistical-Learning-Method_Code">Dod-o/Statistical-Learning-Method_Code</a></p>
<p><a href="https://github.com/Dod-o/Statistical-Learning-Method_Code">Dod-o/Statistical-Learning-Method_Code</a></p>
<p><a href="https://blog.csdn.net/han_xiaoyang/article/details/50629608">NLP系列(4)_朴素贝叶斯实战与进阶</a></p>
<p><a href="https://github.com/fengdu78/lihang-code">fengdu78/lihang-code</a></p>
<p><a href="https://github.com/SmirkCao/Lihang">SmirkCao, Lihang, (2018), GitHub repository</a></p>
]]></content>
      <categories>
        <category>study</category>
        <category>《统计学习方法》</category>
      </categories>
      <tags>
        <tag>Baye Theorem</tag>
        <tag>MLE</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title>[统计学习方法] 3. K近邻法 KNN</title>
    <url>/slm003/</url>
    <content><![CDATA[<p>k-nearest neighbor (KNN)，一种基本的分类与回归方法。这里只介绍分类问题。<strong>k值的选择、距离度量、分类决策规则</strong>是KNN的3个基本要素。 <a id="more"></a></p>
<h2 id="algorithm">1. Algorithm</h2>
<blockquote>
<p><strong>算法 <span class="math inline">\(3.1(k\)</span> 近邻法 <span class="math inline">\()\)</span></strong> 输入: 训练数据集 <span class="math display">\[
\begin{aligned}
T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}
\end{aligned}
\]</span> 其中， <span class="math inline">\(x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}\)</span> 为实例的特征向量, <span class="math inline">\(y_{i} \in \mathcal{Y}=\left\{c_{1}, c_{2}, \cdots, c_{K}\right\}\)</span> 为实例的类别， <span class="math inline">\(i=1,2, \cdots, N ;\)</span> 实例特征向量 <span class="math inline">\(x\)</span>; 输出：实例 <span class="math inline">\(x\)</span> 所属的类 <span class="math inline">\(y\)</span>。 (1) 根据给定的距离度量，在训练集 <span class="math inline">\(T\)</span> 中找出与 <span class="math inline">\(x\)</span> 最邻近的 <span class="math inline">\(k\)</span> 个点，涵盖这 <span class="math inline">\(k\)</span> 个点的 <span class="math inline">\(x\)</span> 的邻域记作 <span class="math inline">\(N_{k}(x)\)</span>; (2) 在 <span class="math inline">\(N_{k}(x)\)</span> 中根据分类决策规则 (如多数表决) 决定 <span class="math inline">\(x\)</span> 的类别 <span class="math inline">\(y\)</span> : <span class="math display">\[
y=\arg \max _{c_{j}} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right), \quad i=1,2, \cdots, N ; j=1,2, \cdots, K
\]</span> 式 (1) 中， <span class="math inline">\(I\)</span> 为指示函数，即当 <span class="math inline">\(y_{i}=c_{j}\)</span> 时 <span class="math inline">\(I\)</span> 为 <span class="math inline">\(1,\)</span> 否则 <span class="math inline">\(I\)</span> 为 0 。</p>
</blockquote>
<ul>
<li>算法没有显式的学习过程。</li>
<li>例子：
<ol type="1">
<li>S市划分成住宅区、商业区、工业区（K=3），每个区域都有各自地标建筑，共N座。你的坐标为x，选取离你最近的k个地标建筑，k中包括哪类地标建筑最多，你就位于哪个区。</li>
</ol></li>
</ul>
<h2 id="model">2. Model</h2>
<ul>
<li><p>KNN的模型对应于对特征空间的划分。</p></li>
<li><p>在训练集、距离度量、k值、分类决策鬼册确定后，对任意新输入实例，所属类别唯一。</p></li>
<li><p>每一个训练实例点<span class="math inline">\(x_i\)</span>，存在一个单元cell，在单元内到此<span class="math inline">\(x_i\)</span>的距离最小。所有实例点的单元划分了特征空间。</p></li>
</ul>
<h3 id="距离度量">2.1 距离度量</h3>
<p>计算特征空间中两点的距离。</p>
<p><span class="math inline">\(L_{p}\)</span> (Minkowski) distance <span class="math display">\[
L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}
\]</span></p>
<ul>
<li><ul>
<li>设特征空间 <span class="math inline">\(\mathcal{X}\)</span> 是 <span class="math inline">\(n\)</span> 维实数向量空间 <span class="math inline">\(x_{i}, x_{j} \in \mathbf{R}^{n}, x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}},\)</span> <span class="math inline">\(x_{j}=\left(x_{j}^{(1)}, x_{j}^{(2)}, \cdots, x_{j}^{(n)}\right)^{\mathrm{T}}\)</span></li>
<li><span class="math inline">\(p\geq1\)</span></li>
</ul></li>
<li><span class="math inline">\(p=1\)</span>，Manhattan distance</li>
<li><span class="math inline">\(p=2\)</span>， Euclidean distance</li>
<li><span class="math inline">\(p=\infty\)</span>，'Max' distance</li>
</ul>
<h3 id="k值选择">2.2 k值选择</h3>
<p>选取一些较小k值，通过<strong>交叉验证法</strong>选取最优k值。</p>
<ul>
<li>小k：
<ul>
<li>近似误差减小，估计误差增大。对近邻实例点非常敏感</li>
<li>模型更复杂，容易过拟合。</li>
</ul></li>
<li>大k：
<ul>
<li>近似误差增大，估计误差减小。较远错误标签实例点也能造成影响。</li>
<li>模型更简单。</li>
<li><span class="math inline">\(k=N\)</span>时，则为测试集最多类。</li>
</ul></li>
</ul>
<h3 id="分类决策规则">2.3 分类决策规则</h3>
<p>一般为<strong>多数表决(majority voting rule)</strong></p>
<p>对给定的实例<span class="math inline">\(x\)</span>，最近邻的<span class="math inline">\(k\)</span>个训练实例点存在<span class="math inline">\(c\)</span>个类别。为了使得分类错误的概率最小，所以选取类别中最多的那一类。</p>
<h2 id="implement-kd-tree">3. Implement: kd tree</h2>
<p>线性扫描 (linear scan) 要计算实例与每一个训练实例的距离，耗时。</p>
<h3 id="kd树的构造">3.1 kd树的构造</h3>
<p>选取一个训练实例点，构造一个垂直于某一轴的超平面，使得特征空间被分成左右两个子空间（左小右大）。在2个特征子空间中重复，直到子空间中没有训练实例点。</p>
<h4 id="平衡kd树">3.1.1平衡kd树</h4>
<ul>
<li>训练实例点的选取为选定坐标轴上的中位数。</li>
<li>效率不一定最优。</li>
</ul>
<blockquote>
<p>算法 3.2 (构造平衡 <span class="math inline">\(k d\)</span> 树）</p>
<p>输入: <span class="math inline">\(k\)</span> 维空间数据集 <span class="math inline">\(T=\left\{x_{1}, x_{2}, \cdots, x_{N}\right\},\)</span> 其中 <span class="math inline">\(x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(k)}\right)^{\mathrm{T}},\)</span> <span class="math inline">\(i=1,2, \cdots, N\)</span> 输出: <span class="math inline">\(k d\)</span> 树。</p>
<p>（1）开始：构造根结点，根结点对应于包含 <span class="math inline">\(T\)</span> 的 <span class="math inline">\(k\)</span> 维空间的超矩形区域。 选择 <span class="math inline">\(x^{(1)}\)</span> 为坐标轴，以 <span class="math inline">\(T\)</span> 中所有实例的 <span class="math inline">\(x^{(1)}\)</span> 坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 <span class="math inline">\(x^{(1)}\)</span> 垂直的超平面实现。&gt; 由根结点生成深度为 1 的左、右子结点: <strong>左子结点对应坐标 <span class="math inline">\(x^{(1)}\)</span> 小于切分点的子 区域，右子结点对应于坐标 <span class="math inline">\(x^{(1)}\)</span> 大于切分点的子区域</strong>。 将落在切分超平面上的实例点保存在根结点。</p>
<p>（2）重复: 对深度为 <span class="math inline">\(j\)</span> 的结点，选择 <span class="math inline">\(x^{(l)}\)</span> 为切分的坐标轴， <span class="math inline">\(l=j(\bmod k)+1,\)</span> 以该结点的区域中所有实例的 <span class="math inline">\(x^{(l)}\)</span> 坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 <span class="math inline">\(x^{(l)}\)</span> 垂直的超平面实现。 由该结点生成深度为 <span class="math inline">\(j+1\)</span> 的左、右子结点: 左子结点对应坐标 <span class="math inline">\(x^{(l)}\)</span> 小于切分点 的子区域，右子结点对应坐标 <span class="math inline">\(x^{(l)}\)</span> 大于切分点的子区域。 将落在切分超平面上的实例点保存在该结点。</p>
<p>（3）直到两个子区域没有实例存在时停止。从而形成 <span class="math inline">\(k d\)</span> 树的区域划分.</p>
</blockquote>
<p><strong>构造过程图示</strong>：</p>
<figure>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-3%20%E5%B9%B3%E8%A1%A1kd%E6%A0%91%E5%9B%BE%E7%A4%BA.png" alt="SLM-3 平衡kd树图示" /><figcaption aria-hidden="true">SLM-3 平衡kd树图示</figcaption>
</figure>
<ul>
<li>按照黄、绿、蓝、红，垂直x，y，x，y的顺序切分。</li>
<li>左右节点确定：根据在第<span class="math inline">\(l\)</span>维度上2个子节点对应坐标大小确定，小左大右。</li>
<li><span class="math inline">\(l=j(\bmod k)+1\)</span>：在k维方向都按顺序切上之后，回过头在循环切。
<ul>
<li>公式细节因初始根节点的深度是0或1而有不同</li>
</ul></li>
<li>停止条件：所有实例点都位于一个超平面上。</li>
<li>偶数个数的中位数：自己确定，代码上别忘了。</li>
</ul>
<h3 id="kd树的搜索">3.2 kd树的搜索</h3>
<h4 id="k1">3.2.1 <span class="math inline">\(k=1\)</span></h4>
<blockquote>
<p>算法 3.3 (用 <span class="math inline">\(k d\)</span> 树的最近邻搜索) 输入: 已构造的 <span class="math inline">\(k d\)</span> 树，目标点 <span class="math inline">\(x\)</span>; 输出: <span class="math inline">\(x\)</span> 的最近邻，<span class="math inline">\(k=1\)</span>。</p>
<ol type="1">
<li><p>在 <span class="math inline">\(k d\)</span> 树中找出包含目标点 <span class="math inline">\(x\)</span> 的叶结点: 从根结点出发，递归地向下访问 <span class="math inline">\(k d\)</span> 树。若目标点 <span class="math inline">\(x\)</span> 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。</p></li>
<li><p>以此叶结点为“当前最近点”。</p></li>
<li><p>递归地向上回退，在每个结点进行以下操作:</p>
<p>(a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点 为“当前最近点”。</p>
<p>(b）当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”问的距离为半径的超球体相交。 (c）如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索; 如果不相交，向上回退。</p></li>
</ol>
<p>(4）当回退到根结点时，搜索结束。最后的“当前最近点"即为 <span class="math inline">\(x\)</span> 的最近邻点。</p>
</blockquote>
<h4 id="k1-1">3.2.2 <span class="math inline">\(k&gt;1\)</span></h4>
<blockquote>
<p>算法 3.04 (用 <span class="math inline">\(k d\)</span> 树的最近邻搜索) 输入: 已构造的 <span class="math inline">\(k d\)</span> 树，目标点 <span class="math inline">\(x\)</span>; 输出: <span class="math inline">\(x\)</span> 的k最近邻。</p>
<ol type="1">
<li><p>根据p的坐标和kd树的结点向下进行搜索 (如果树的结点是以 <span class="math inline">\(x^{(l)}=c\)</span> 来切分的, 那么如果p的 <span class="math inline">\(x^{(l)}\)</span> 坐标小于c, 则走左子结点, 否则走右子结点)</p></li>
<li><p>到达叶子结点时，将其标记为已访问。如果S中不足k个点, 则将该结点加入到S中; 如果S不空且当前结点与p点的距离小于S中最长的距离，则用当前结点替换S中离p最远的点</p></li>
<li><p>如果当前结点不是根节点, 执行（a）; 否则，结束算法</p></li>
</ol>
<p>(a). 回退到当前结点的父结点, 此时的结点为当前结点 (回退之后的结点) ，将当前结点标 记为已访问, 执行 (b) 和（c) ; 如果当前结点已经被访过, 再次执行（a）。</p>
<p>(b). 如果此时S中不足k个点, 则将当前结点加入到S中; 如果S中已有k个点, 且当前结点与p 点的距离小于S中最长距离，则用当前结点替换S中距离最远的点。</p>
<p>(c). 计算p点和当前结点切分线的距离。如果该距离大于等于S中距离p最远的距离并且S中已 有k个点, 执行3; 如果该距离小于S中最远的距离或S中没有k个点, 从当前结点的另一子节点开始执行1; 如果当前结点没有另一子结点, 执行3。</p>
</blockquote>
<p><strong>算法3.04 (用 <span class="math inline">\(k d\)</span> 树的最近邻搜索)搜索过程</strong>：目标点P（-1，-5），k=3，S：存储k个近邻点。初始所有点=[0, 0] ，即 [未访问, 不在S中]</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-3%20%E5%B9%B3%E8%A1%A1kd%E6%A0%91%E5%9B%BE%E7%A4%BA.png" alt="image-20201216151045483" style="zoom:80%;" /></p>
<blockquote>
<ol type="1">
<li>执行算法1：
<ul>
<li>(-1,-5)的-1与A点(6,5)的6小，往左到B</li>
<li>-5&lt;-3，往左到D</li>
<li>D子节点唯一，到H</li>
</ul></li>
<li>执行算法2：
<ul>
<li>H=[1, 0]，H=[1, 1]</li>
</ul></li>
<li>执行算法3：
<ul>
<li>执行(a)，到D。D=[1, 0]</li>
<li>执行(b)，D=[1, 1]</li>
<li>执行(c)，P到D的切分线<span class="math inline">\(x^1=-6\)</span>距离为5。S未满。D没有另一子节点。</li>
</ul></li>
<li>执行算法3：
<ul>
<li>执行(a)，到B。B=[1, 0]</li>
<li>执行(b)，B=[1,1]，S={H, D, B}</li>
<li>执行(c)，P到B的切分线<span class="math inline">\(x^2=-3\)</span>距离为2，小于S的最大距离。从B另一子节点E执行算法1.</li>
</ul></li>
<li>执行算法1：
<ul>
<li>-1&gt;-2，往右到J</li>
</ul></li>
<li>执行算法2：
<ul>
<li>J=[1,0]，d(J, P)大于S的最大距离。不加入</li>
</ul></li>
<li>执行算法3：
<ul>
<li>执行(a)，到E，E=[1, 0]</li>
<li>执行(b)，d(E,P)小于S的最大距离。E=[1, 1]，H=[1, 0]，S={E, D, B}</li>
<li>执行(c)，P到E的切分线<span class="math inline">\(x^1=-2\)</span>距离为1，小于S的最大距离。从E另一子节点I执行算法1.</li>
</ul></li>
<li>执行算法1：
<ul>
<li>I是子节点，到I</li>
</ul></li>
<li>执行算法2：
<ul>
<li>I=[1, 0]，d(I, P)大于S的最大距离。不加入。</li>
</ul></li>
<li>执行算法3：
<ul>
<li>执行若干(a)，到A，A=[1, 0]</li>
<li>执行(b)，d(A, P)大于S的最大距离。</li>
<li>执行(c)，P到E的切分线<span class="math inline">\(x^1=6\)</span>距离为7，大于最长距离。不加入。</li>
</ul></li>
<li>执行算法3：
<ul>
<li>A是根节点。算法结束。已按顺序访问{H, D, B, J, E, I, A}，最终结果S={E, D, B}</li>
</ul></li>
</ol>
</blockquote>
<h3 id="summary">3.3 Summary</h3>
<ol type="1">
<li><p>kd Tree的平均复杂度<span class="math inline">\(O(logN)\)</span>, <span class="math inline">\(N\)</span>为训练实例数。适合训练实例数远大于空间维数的KNN。</p></li>
<li><p>把k维大空间多次对半分成k维小空间。</p></li>
<li><p>目标点与实例点的距离<span class="math inline">\(\geq\)</span>目标点与该实例点对应切分超平面的距离。因此：</p>
<ol type="1">
<li>当此实例点当前应该替换S中某点从而加入到S集合中时，另一半空间存在子空间，子空间的点到目标点的距离小于实例点到目标点的距离。
<ol type="1">
<li>此时如果存在实例点位于子空间，那么应该继续循环（例子中没有体现）。</li>
<li>没有实例点在此子空间，那么另一半的空间的实例点都不会加入到集合S中。</li>
</ol></li>
<li>这个实例点不应该加入到S中，情况如2.1.2</li>
</ol></li>
<li><p>动态规划DP的思想？</p></li>
<li><p>别人笔记的原话：</p>
<p>1、找到叶子结点，看能不能加入到S中</p>
<p>2、回退到父结点，看父结点能不能加入到S中</p>
<p>3、看目标点和回退到的父结点切分线的距离，判断另一子结点能不能加入到S中</p></li>
</ol>
<h2 id="question">4. Question</h2>
<ol type="1">
<li>kd方差的代码！
<ul>
<li>有点点难搞-_-</li>
</ul></li>
<li>非平衡/方差kd树
<ul>
<li>同上，难搞</li>
</ul></li>
</ol>
<h2 id="code">5. Code</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, train_set, label_set</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray, m:dims, n:train points</span></span><br><span class="line"><span class="string">        :param label_set: 1*n int ndarray and from 0 to n-1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.train_set = train_set</span><br><span class="line">        self.label_set = label_set</span><br><span class="line">        self.m, self.n = self.train_set.shape</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">knn_naive</span>(<span class="params">self, test_set, k=<span class="number">1</span>,p=<span class="number">2</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param test_set: m*n ndarray, m:dims, n:test points</span></span><br><span class="line"><span class="string">        :param k: k nearest neighbor</span></span><br><span class="line"><span class="string">        :param p: order of norm</span></span><br><span class="line"><span class="string">        :return: 1*n list for n points</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        (m,n)=test_set.shape</span><br><span class="line">        <span class="comment"># 有几个点，输出对于shape的list</span></span><br><span class="line">        final=[-<span class="number">1</span>]*n</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="comment"># 转置后取点再转置</span></span><br><span class="line">            test_point=test_set.T[i].T</span><br><span class="line">            <span class="comment"># 有些可能是(m,)的ndarray</span></span><br><span class="line">            test_point=test_point.reshape([m,<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 计算距离</span></span><br><span class="line">            distance=np.linalg.norm(self.train_set-test_point,<span class="built_in">ord</span>=p,axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 最近k个点的index</span></span><br><span class="line">            nearestK=np.argsort(distance)[:k]</span><br><span class="line">            <span class="comment"># label是0，1，2，...的顺序，存储对应label出现的总次数</span></span><br><span class="line">            labelList=[<span class="number">0</span>]*(<span class="built_in">max</span>(self.label_set) + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> nearestK:</span><br><span class="line">                labelList[<span class="built_in">int</span>(self.label_set[index])] +=<span class="number">1</span></span><br><span class="line">                point_label=labelList.index(<span class="built_in">max</span>(labelList))</span><br><span class="line">            final[i]=point_label</span><br><span class="line">        <span class="keyword">return</span> final</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">knn_kdtree</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span>(<span class="params">self, points=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        画不同k从而分割成不同子%tb域的图</span></span><br><span class="line"><span class="string">        画结果图</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> self.m != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;Unable to draw a picture&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
<li>Github求start！！！^_^</li>
</ul>
<h2 id="reference">6. Reference</h2>
<p><a href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>
<p><a href="https://blog.csdn.net/zzpzm/article/details/88565645">KNN算法和kd树详解（例子+图示）</a></p>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict">sklearn</a></p>
<p><a href="https://github.com/fengdu78/lihang-code">fengdu78/lihang-code</a></p>
<p><a href="https://github.com/Dod-o/Statistical-Learning-Method_Code">Dod-o/Statistical-Learning-Method_Code</a></p>
]]></content>
      <categories>
        <category>study</category>
        <category>《统计学习方法》</category>
      </categories>
      <tags>
        <tag>KNN</tag>
        <tag>kd tree</tag>
      </tags>
  </entry>
  <entry>
    <title>[统计数学方法] 2. 感知机 Perception</title>
    <url>/slm002/</url>
    <content><![CDATA[<p>感知机（perceptron）是<strong>二类线性分类</strong>模型。</p>
<a id="more"></a>
<h2 id="model">1. Model</h2>
<blockquote>
<p>定义 2.1 (感知机) <span class="math inline">\(\quad\)</span>假设输入空间（特征空间 ) 是 <span class="math inline">\(\mathcal{X} \subseteq \mathbf{R}^{n},\)</span> 输出空间是 <span class="math inline">\(\mathcal{Y}=\{+1,-1\}_{\circ}\)</span> 输入 <span class="math inline">\(x \in \mathcal{X}\)</span> 表示实例的特征向量，对应于输入空间 ( 特征空间 <span class="math inline">\()\)</span> 的点; 输出 <span class="math inline">\(y \in \mathcal{Y}\)</span> 表示实例的类别。由输入空间到输出空间的如下函数: <span class="math display">\[
f(x)=\operatorname{sign}(w \cdot x+b)
\]</span> 称为感知机。其中， <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span> 为感知机模型参数, <span class="math inline">\(w \in \mathbf{R}^{n}\)</span> 叫作权值（weight ) 或权值向量（weight vector) <span class="math inline">\(, b \in \mathbf{R}\)</span> 叫作偏置 ( bias <span class="math inline">\(), w \cdot x\)</span> 表示 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(x\)</span> 的内积。sign 是符号 函数，即 <span class="math display">\[
\operatorname{sign}(x)=\left\{\begin{array}{ll}
+1, &amp; x \geqslant 0 \\
-1, &amp; x&lt;0
\end{array}\right.
\]</span></p>
</blockquote>
<ul>
<li><p>假设空间：特征空间中的所有线性分类模型，即函数集合<span class="math inline">\(\{f|f(x)=w \cdot x+b\}\)</span></p></li>
<li><p>几何解释：线性方程<span class="math inline">\(w \cdot x+b=0\)</span>是特征空间<span class="math inline">\(\mathbf{R}^{n}\)</span>的一个超平面<span class="math inline">\(\mathbf{S}\)</span>，把特征空间分成2个部分，使得特征向量分别划入正负两类。<span class="math inline">\(\mathbf{S}\)</span>也叫分离超平面（separating hyperplane)</p>
<ul>
<li>超平面<span class="math inline">\(\mathbf{S}\subseteq \mathbf{R}^{n-1}\)</span>并且截距为0。在这需要把b当成特征而不是截距才说的通。</li>
</ul></li>
<li><p>例子：</p>
<ul>
<li>通过<span class="math inline">\(\{房屋面积，房龄，...\}\)</span>来判断房子总价是否高于价格<span class="math inline">\(a\)</span>
<ul>
<li>假设数据集线性可分</li>
</ul></li>
</ul></li>
</ul>
<p><br><br></p>
<h2 id="strategy">2. Strategy</h2>
<h3 id="数据可分性">2.1 数据可分性</h3>
<blockquote>
<p>定义 2.2 (数据集的线性可分性) <span class="math inline">\(\quad\)</span> 给定一个数据集 <span class="math display">\[
T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}
\]</span> 其中， <span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N,\)</span> 如果存在某个超平面 <span class="math inline">\(S\)</span> <span class="math display">\[
w \cdot x+b=0
\]</span> 能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有 <span class="math inline">\(y_{i}=+1\)</span> 的实例 <span class="math inline">\(i,\)</span> 有 <span class="math inline">\(w \cdot x_{i}+b&gt;0,\)</span> 对所有 <span class="math inline">\(y_{i}=-1\)</span> 的实例 <span class="math inline">\(i,\)</span> 有 <span class="math inline">\(w \cdot x_{i}+b&lt;0,\)</span> 则 称数据集 <span class="math inline">\(T\)</span> 为线性可分数据集（linearly separable data set ) ; 否则，称数据集 <span class="math inline">\(T\)</span> 线性 不可分。</p>
</blockquote>
<ul>
<li>存在超平面<span class="math inline">\(\mathbf{S}\Longrightarrow\)</span>数据集线性可分</li>
</ul>
<h3 id="学习策略">2.2 学习策略</h3>
<h4 id="误分类点总数">2.2.1 误分类点总数</h4>
<p>把误分类点总数当成损失函数，不是参数<span class="math inline">\(w,b\)</span>的连续可导函数，不易优化</p>
<h4 id="误分类点到超平面的总距离">2.2.2 误分类点到超平面的总距离</h4>
<blockquote>
<p>定理（2.01) <span class="math inline">\(\quad\)</span>空间 <span class="math inline">\(\mathbf{R}^n\)</span>中任意一点<span class="math inline">\(x_0\)</span>到超平面 <span class="math inline">\(\mathbf{S}=\{x|w\cdot x+b=0\}\)</span> 的距离为 <span class="math display">\[
\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|
\]</span></p>
</blockquote>
<p>对于误分类数据<span class="math inline">\((x_i,y_i)\)</span>，M为误分类点集合，误分类点到超平面的总距离为 <span class="math display">\[
\begin{align}
D&amp;=\frac{1}{\|w\|} \sum_{x_{i} \in M} |y_{i}\left(w \cdot x_{i}+b\right)|\nonumber\\
&amp;=-\frac{1}{\|w\|} \sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)\\
L(w,b)&amp;=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
\end{align}
\]</span></p>
<ul>
<li>对误分类点<span class="math inline">\((x_i,y_i)\)</span>，<span class="math inline">\(-y_{i}\left(w \cdot x_{i}+b\right) \geq0\)</span></li>
<li>(1)是给定数据集<span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}\)</span>，其中<span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N\)</span></li>
<li>(2)非负函数，且对于<span class="math inline">\(w,b\)</span>连续可导</li>
<li>我们的策略是<strong>极小化损失函数</strong>，即<span class="math inline">\(\min _{w, b} L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)\)</span></li>
</ul>
<p><br><br></p>
<h2 id="algorithm">3. Algorithm</h2>
<p>求解损失函数(2)的最优化问题，通过随机梯度下降(stochastic gradient descent)</p>
<h3 id="算法原始形式">3.1 算法原始形式</h3>
<blockquote>
<p>算法 2.1（感知机学习算法的原始形式） 输入: 训练数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中 <span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in\)</span><span class="math inline">\(\mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N ;\)</span> 学习率 <span class="math inline">\(\eta(0&lt;\eta \leqslant 1)\)</span>; 输出 <span class="math inline">\(: w, b ;\)</span> 感知机模型 <span class="math inline">\(f(x)=\operatorname{sign}(w \cdot x+b)\)</span> 。</p>
<p>(1)选取初值 <span class="math inline">\(w_{0}, b_{0}\)</span>;</p>
<p>(2)在训练集中随机选取数据<span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span>;</p>
<p>(3)如果 <span class="math inline">\(y_{i}\left(w \cdot x_{i}+b\right) \leqslant 0\)</span>, <span class="math display">\[
\begin{align}
w&amp;\leftarrow w+\eta y_{i} x_{i} \\
b&amp;\leftarrow b+\eta y_{i}
\end{align}
\]</span> 步骤（4）转至步骤（2），直至训练集中没有误分类点。</p>
</blockquote>
<ul>
<li><p>一次随机选取一个误分类点使其梯度下降</p></li>
<li><p><strong>随机</strong>选取一个误分类点 <span class="math inline">\(\left(x_{i}, y_{i}\right),\)</span> 对 <span class="math inline">\(w, b\)</span> 进行更新 : <span class="math display">\[
\begin{aligned}
w &amp;\leftarrow w+\eta y_{i} x_{i} \\
b &amp;\leftarrow b+\eta y_{i}
\end{aligned}
\]</span></p></li>
<li><p>不同初值，不同分类点选取会影响最后的解</p></li>
</ul>
<h3 id="原始算法的收敛性">3.2 原始算法的收敛性</h3>
<p>证明对一个线性可分的数据集，感知机学习算法的原始形式在有限次迭代后能得到正确的分离超平面与感知机模型</p>
<blockquote>
<p><strong>定理 2.1 (Novikoff)</strong> 设训练数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}\)</span> 是线 性可分的，其中 <span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N,\)</span> 则</p>
<p>(1)存在满足条件 <span class="math inline">\(\left\|\hat{w}_{\mathrm{opt}}\right\|=1\)</span> 的超平面 <span class="math inline">\(\hat{w}_{\mathrm{opt}} \cdot \hat{x}=w_{\mathrm{opt}} \cdot x+b_{\mathrm{opt}}=0\)</span> 将训练数据集完全正确分开; 且存&gt; 在 <span class="math inline">\(\gamma&gt;0,\)</span> 对所有 <span class="math inline">\(i=1,2, \cdots, N\)</span> <span class="math display">\[
y_{i}\left(\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right) \geqslant \gamma
\]</span> (2)令 <span class="math inline">\(R=\max _{1 \leqslant i \leqslant N}\left\|\hat{x}_{i}\right\|,\)</span> 则感知机算法 2.1 在训练数据集上的误分类次数 <span class="math inline">\(k\)</span> 满足不等式 <span class="math display">\[
k \leqslant\left(\frac{R}{\gamma}\right)^{2}
\]</span></p>
</blockquote>
<ul>
<li><span class="math inline">\(\hat{w}=\left(w^{\mathrm{T}}, b\right)^{\mathrm{T}}\)</span>，<span class="math inline">\(\hat{x}=\left(x^{\mathrm{T}}, 1\right)^{\mathrm{T}}\)</span>，<span class="math inline">\(\hat{x} \in \mathbf{R}^{n+1}, \hat{w} \in \mathbf{R}^{n+1}\)</span>，<span class="math inline">\(\hat{w} \cdot \hat{x}=w \cdot x+b\)</span></li>
<li>对于线性可分数据集，感知机的解存在但不唯一。</li>
<li>在线性支持向量机中，添加超平面约束条件，从而得到唯一超平面。</li>
</ul>
<h3 id="算法对偶形式">3.3 算法对偶形式</h3>
<p>我们假设<span class="math inline">\(w,b=0\)</span>，在原始算法中，对于一个误分类点<span class="math inline">\((x_i,y_i)\)</span>，我们进行了<span class="math inline">\(n_i\)</span>次更新，对于所有<span class="math inline">\(N\)</span>个点，我们有 <span class="math display">\[
\begin{aligned}
w &amp;=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} =\sum_{i=1}^{N} n_i(\eta  y_{i} x_{i})\\
b &amp;=\sum_{i=1}^{N} \alpha_{i} y_{i}=\sum_{i=1}^{N} n_i(\eta y_i)
\end{aligned}
\]</span> 我们从解w，b转化为<span class="math inline">\(n_i\)</span>即第<span class="math inline">\(i\)</span>个是实例点由于误分而进行更新的次数。</p>
<p>实例点更新越多，说明距离分离超平面越近，越难分类，对结果影响大，很可能就是支持向量。</p>
<blockquote>
<p>算法 2.2 (感知机学习算法的对偶形式) 输入: 线性可分的数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中 <span class="math inline">\(x_{i} \in \mathbf{R}^{n}, y_{i} \in\)</span> <span class="math inline">\(\{-1,+1\}, i=1,2, \cdots, N ;\)</span> 学习率 <span class="math inline">\(\eta(0&lt;\eta \leqslant 1) ;\)</span> 输 出: <span class="math inline">\(\alpha, b ;\)</span> 感 知机 模 型 <span class="math inline">\(f(x)=\operatorname{sign}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x+b\right),\)</span> 其中 <span class="math inline">\(\alpha=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}\)</span></p>
<p>(1)<span class="math inline">\(\alpha \leftarrow 0, b \leftarrow 0 ;\)</span></p>
<p>(2)在训练集中选取数据 <span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span>;</p>
<p>(3)如果 <span class="math inline">\(y_{i}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i}+b\right) \leqslant 0\)</span> <span class="math display">\[
\begin{array}{l}
\alpha_{i} \leftarrow \alpha_{i}+\eta \\
b \leftarrow b+\eta y_{i}
\end{array}
\]</span> (4)转至 步骤(2)直到没有误分类数据。</p>
</blockquote>
<ul>
<li><p>当<span class="math inline">\(\eta=1\)</span>，即<span class="math inline">\(n_i=n_i+1\)</span>，对点更新次数加一</p></li>
<li><p>对偶算法只涉及到矩阵的内积<span class="math inline">\(x_i\cdot x_j\)</span>计算，我们可以将内积预先计算存储，即我们计算Gram矩阵<span class="math inline">\(\mathbf{G}=[x_i \cdot x_j]_{\mathbf{N}\times\mathbf{N}}\)</span></p></li>
<li><p>和原始形式一样，感知机学习算法的对偶形式迭代是收敛的，且存在多个解。</p></li>
</ul>
<p><br><br></p>
<h2 id="question">4. Question</h2>
<ol type="1">
<li><p>超平面的定义是什么？</p></li>
<li><p>这么判断数据集可不可分？</p>
<ul>
<li><p>证明以下定理： 样本集线性可分的充分必要条件是正实例点集所构成的凸壳CD 与负实例点集所构成的凸壳互不相交。</p></li>
<li><p>设集合 <span class="math inline">\(S \subset \mathbf{R}^{n}\)</span> 是由 <span class="math inline">\(\mathbf{R}^{n}\)</span> 中的 <span class="math inline">\(k\)</span> 个点所组成的集合, 即 <span class="math inline">\(S=\left\{x_{1}, x_{2}, \cdots, x_{k}\right\} .\)</span> 定义 <span class="math inline">\(S\)</span> 的凸亮<span class="math inline">\(\operatorname{conv}(S)\)</span> 为 <span class="math display">\[
\begin{aligned}
\operatorname{conv}(S)=\left\{x=\sum_{i=1}^{k} \lambda_{i} x_{i} \mid \sum_{i=1}^{k} \lambda_{i}=1, \lambda_{i} \geqslant 0, i=1,2, \cdots, k\right\}
\end{aligned}
\]</span></p></li>
</ul></li>
</ol>
<p><br><br></p>
<h2 id="code">5. Code</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">perceptron</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, X_train, Y_train, learning_rate=<span class="number">0.0001</span>, tol=<span class="number">0</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        X_train:Array, default=None</span></span><br><span class="line"><span class="string">            dataset for training</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Y_train:Array, default=None</span></span><br><span class="line"><span class="string">            labelset for training</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        learning_rate:float,default=0.0001</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        tol:int, default=0</span></span><br><span class="line"><span class="string">            the stopping criterion. When the number of misclassification </span></span><br><span class="line"><span class="string">            points small or equal to tol, stop training.</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.X_train=np.mat(X_train)</span><br><span class="line">        self.Y_train=np.mat(Y_train).T</span><br><span class="line">        self.m,self.n=np.shape(self.X_train)</span><br><span class="line">        self.w=np.zeros((<span class="number">1</span>,np.shape(self.X_train)[<span class="number">1</span>]))</span><br><span class="line">        self.b=<span class="number">0</span></span><br><span class="line">        self.learning_rate=learning_rate</span><br><span class="line">        self.tol=tol</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x, w, b</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(x,w.T)+b</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        w: np.mat</span></span><br><span class="line"><span class="string">            weights</span></span><br><span class="line"><span class="string">        b: np.mat</span></span><br><span class="line"><span class="string">            bias        </span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        is_wrong=<span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> is_wrong:</span><br><span class="line">            wrong_count=<span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.m):</span><br><span class="line">                xi=self.X_train[s]</span><br><span class="line">                yi=self.Y_train[s]</span><br><span class="line">                <span class="keyword">if</span> self.calculate(xi,self.w,self.b)*yi &lt;= <span class="number">0</span>:</span><br><span class="line">                    self.w=self.w+self.learning_rate*yi*xi</span><br><span class="line">                    self.b=self.b+self.learning_rate*yi</span><br><span class="line">                    wrong_count+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> wrong_count &lt;=self.tol:</span><br><span class="line">                is_wrong=<span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> self.w,self.b</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<p><br><br></p>
<h2 id="proof">6. Proof</h2>
<h3 id="theorem-2.01">6.1 Theorem 2.01</h3>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-2%20thm2.01%20distance" alt="image-20201123163513240" style="zoom:50%;" /></p>
<h3 id="theorem-2.1">6.2 Theorem 2.1</h3>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-2%20thm2.1%20Novikoff.png" alt="SLM-2 thm2.1 Novikoff" style="zoom:50%;" /></p>
<p><br><br></p>
<h2 id="reference">7. Reference</h2>
<p><a href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>
<p><a href="https://www.pkudodo.com/2018/11/18/1-4/">统计学习方法|感知机原理剖析及实现</a></p>
<p><a href="https://www.zhihu.com/question/26526858/answer/136577337">如何理解感知机学习算法的对偶形式？ - Zongrong Zheng的回答 - 知乎</a></p>
<p><a href="https://github.com/fengdu78/lihang-code">fengdu78/lihang-code</a></p>
]]></content>
      <categories>
        <category>study</category>
        <category>《统计学习方法》</category>
      </categories>
      <tags>
        <tag>Dual</tag>
        <tag>感知机 Perception</tag>
        <tag>线性可分性</tag>
      </tags>
  </entry>
  <entry>
    <title>[Time Series] 2. Continuous Time Dynamic Models</title>
    <url>/timeseries1/</url>
    <content><![CDATA[<h2 id="动态模型">动态模型</h2>
<ol type="1">
<li><p>HMM. Discrete.</p></li>
<li><p>LDS. Continuous. linear. Filtering problem.</p></li>
<li><p>PF. Continuous. Nonlinear. Filtering problem. <a id="more"></a> #### Assumption</p></li>
<li><p>齐次 Markov 假设（未来只依赖于当前）：</p>
<ul>
<li><span class="math inline">\(i\)</span>: true states</li>
<li><span class="math inline">\(o\)</span>: observations</li>
</ul>
<p><span class="math display">\[
p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)=p(i_{t+1}|i_t)
\]</span></p></li>
<li><p>观测独立假设： <span class="math display">\[
p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)=p(o_t|i_t)
\]</span></p></li>
</ol>
<h4 id="two-equations">Two equations</h4>
<ol type="1">
<li>状态方程 system equation
<ul>
<li>g已知。我们操作机器人行走，过程轨迹收到地面摩擦、风力的影响。</li>
</ul></li>
</ol>
<p><span class="math display">\[
\begin{align}
Z_t=g(Z_{t-1},u,\delta_1)
\end{align}
\]</span></p>
<ol type="1">
<li>观测方程 measurement equation</li>
</ol>
<p><span class="math display">\[
X_t=f(Z_t,u,\delta_2)
\]</span></p>
<h4 id="filtering-problem">Filtering problem</h4>
<ul>
<li><p>Find <span class="math display">\[
p(z_t|x_1,x_2,\cdots,x_t)
\]</span></p></li>
<li><p>online problem</p></li>
</ul>
<h4 id="process">Process</h4>
<figure>
<img src="C:%5Ctypora%5Cfigures%5C%E5%8A%A8%E6%80%81%E6%A8%A1%E5%9E%8B.jpg" alt="动态模型" /><figcaption aria-hidden="true">动态模型</figcaption>
</figure>
<h2 id="linear-dynamical-system">Linear Dynamical System</h2>
<h4 id="linearly">Linearly</h4>
<ul>
<li>two functions are linearly.</li>
</ul>
<p><span class="math display">\[
\begin{align}
z_t&amp;=A\cdot z_{t-1}+B+\varepsilon\\
x_t&amp;=C\cdot z_t+D+\delta\\
\varepsilon&amp;\sim\mathcal{N}(0,Q)\\
\delta&amp;\sim\mathcal{N}(0,R)
\end{align}
\]</span></p>
<h4 id="idea">Idea</h4>
<ol type="1">
<li>relationship between <span class="math inline">\(p(z_t|x_{1:t})\;\&amp;\;p(z_{t-1}|x_{1:t-1})\)</span></li>
<li>we know <span class="math inline">\(p(z_t|z_{t-1}),\;p(x_t|z_t)\)</span></li>
</ol>
<h4 id="derivation">Derivation</h4>
<p>For equation (5), we have <span class="math display">\[
p(z_t|x_{1:t})=p(x_{1:t},z_t)/p(x_{1:t})=Cp(x_{1:t},z_t)
\]</span> For <span class="math inline">\(p(x_{1:t},z_t)\)</span>： <span class="math display">\[
\begin{align}p(x_{1:t},z_t)&amp;=p(x_t|x_{1:t-1},z_t)p(x_{1:t-1},z_t)\\&amp;=p(x_t|z_t)p(x_{1:t-1},z_t)\nonumber\\
&amp;=p(x_t|z_t)p(z_t|x_{1:t-1})p(x_{1:t-1})\\&amp;=Cp(x_t|z_t)p(z_t|x_{1:t-1})\\
\end{align}
\]</span> For <span class="math inline">\(p(z_{t}|x_{1:t-1})\)</span>, period probability: <span class="math display">\[
\begin{align}
p(z_t|x_{1:t-1})&amp;=\int_{z_{t-1}}p(z_t,z_{t-1}|x_{1:t-1})dz_{t-1}\nonumber\\
&amp;=\int_{z_{t-1}}p(z_t|z_{t-1},x_{1:t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}\nonumber\\
&amp;=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}
\end{align}
\]</span> In (13), it needs <span class="math inline">\(p(z_{t-1}|x_{1:t-1})\)</span>, a circuit.</p>
<h4 id="process-1">Process</h4>
<p>根据计算过程，我们可以分成2个过程。并由于线性高斯假设，我们可以得到解析解。</p>
<ol type="1">
<li><p>Prediction:</p>
<ul>
<li>calculate <span class="math inline">\(p(z_{t}|x_{1:t-1})\)</span>, period probability。 <span class="math display">\[
\begin{align}
&amp; p(z_t|x_{1:t-1})=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}=\int_{z_{t-1}}\mathcal{N}(Az_{t-1}+B,Q)\mathcal{N}(\mu_{t-1},\Sigma_{t-1})dz_{t-1}\\
&amp; p(z_t|x_{1:t-1})=\mathcal{N}(A\mu_{t-1}+B,Q+A\Sigma_{t-1}A^T)
\end{align}
\]</span></li>
</ul></li>
<li><p>Update：</p>
<ul>
<li><span class="math display">\[
p(z_t|x_{1:t})\propto p(x_t|z_t)p(z_t|x_{1:t-1})
\]</span></li>
</ul></li>
</ol>
<h2 id="particle-filter">Particle Filter</h2>
<p>非线性、非高斯，我们不能得到解析解。为了确定分布(5)需要MC采样。</p>
<h4 id="is-importance-sampling">IS (Importance Sampling)</h4>
<p>对分布<span class="math inline">\(p(z)\)</span>求关于某个函数<span class="math inline">\(f(z)\)</span>的期望 <span class="math display">\[
\mathbb{E}[f(z)]=\int_zf(z)p(z)dz=\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)
\]</span> 直接在分布<span class="math inline">\(p(z)\)</span>中采样比较困难，我们引入简单分布<span class="math inline">\(q(z)\)</span>，并根据<span class="math inline">\(\frac{p(z)}{q(z)}\)</span>的比值确定采样点的权重 <span class="math display">\[
\mathbb{E}[f(z)]=\int_zf(z)p(z)dz=\int_zf(z)\frac{p(z)}{q(z)}q(z)dz=\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}
\]</span> <img src="C:%5Ctypora%5Cfigures%5Cf%20q%20p.png" alt="f q p" style="zoom:50%;" /></p>
<p>在滤波问题中，需要求解 <span class="math inline">\(p(z_t|x_{1:t})\)</span>，其权重为： <span class="math display">\[
w_t^i=\frac{p(z_t^i|x_{1:t})}{q(z_t^i|x_{1:t})},i=1,2,\cdots,N
\]</span></p>
<p>It is difficult to sample N points and calculate <span class="math inline">\(p(z_t^i|x_{1:t})\)</span>.</p>
<h4 id="sis-sequential-importance-sampling">SIS (Sequential Importance Sampling)</h4>
<p>In SIS, we talk about probability <span class="math inline">\(p(z_{1:t}|x_{1:t})\)</span>: <span class="math display">\[
w_t^i\propto\frac{p(z_{1:t}|x_{1:t})}{q(z_{1:t}|x_{1:t})}
\]</span> We want to find the relationship between <span class="math inline">\(w_t^i\)</span> and <span class="math inline">\(w_{t-1}^i\)</span>.</p>
<p>By (17), for <span class="math inline">\(p(z_{1:t}|x_{1:t})\)</span>: <span class="math display">\[
\begin{align}p(z_{1:t}|x_{1:t})\propto p(x_{1:t},z_{1:t})&amp;=p(x_t|z_{1:t},x_{1:t-1})p(z_{1:t},x_{1:t-1})\nonumber\\
&amp;=p(x_t|z_t)p(z_t|x_{1:t-1},z_{1:t-1})p(x_{1:t-1},z_{1:t-1})\nonumber\\
&amp;=p(x_t|z_t)p(z_t|z_{t-1})p(x_{1:t-1},z_{1:t-1})\nonumber\\
&amp;\propto p(x_t|z_t)p(z_t|z_{t-1})p(z_{1:t-1}|x_{1:t-1})
\end{align}
\]</span> For <span class="math inline">\(q(z_{1:t}|x_{1:t})\)</span>, we let <span class="math display">\[
q(z_{1:t}|x_{1:t})=q(z_t|z_{1:t-1},x_{1:t})q(z_{1:t-1}|x_{1:t-1})
\]</span> Then we have <span class="math display">\[
w_t^i\propto\frac{p(z_{1:t}|x_{1:t})}{q(z_{1:t}|x_{1:t})}\propto \frac{p(x_t|z_t)p(z_t|z_{t-1})p(z_{1:t-1}|x_{1:t-1})}{q(z_t|z_{1:t-1},x_{1:t})q(z_{1:t-1}|x_{1:t-1})}=\frac{p(x_t|z_t)p(z_t|z_{t-1})}{q(z_t|z_{1:t-1},x_{1:t})}w_{t-1}^i
\]</span> We have distribution for <span class="math inline">\(p(x_t|z_t)\)</span> and <span class="math inline">\(p(z_t|z_{t-1})\)</span>.</p>
<h4 id="problem">Problem</h4>
<p>权值退化 (the degeneracy phenomenon)，权重的方差变大</p>
<p>通过<span class="math inline">\(N_{eff}=\frac{1}{\sum_{i=1}^{N_s}(w_k^i)^2}\)</span>判断是否存在权值退化。越小越严重。</p>
<p>sol:</p>
<ol type="1">
<li><p>resampling</p>
<ol start="2" type="1">
<li>find a proper distribution <span class="math inline">\(q(z)\)</span>, let <span class="math inline">\(q(z_t|z_{1:t-1},x_{1:t})=p(z_t|z_{t-1})\)</span>, then <span class="math inline">\(w_t^i=p(x_t|z_t)w_{t-1}^i\)</span></li>
</ol></li>
</ol>
<h4 id="sir-filter-algorithm-sampling-importance-resampling-sis-resampling-qzqz">SIR Filter Algorithm (Sampling Importance Resampling, SIS + Resampling + <span class="math inline">\(q(z)=q&#39;(z)\)</span>)</h4>
<h6 id="resampling">resampling:</h6>
<ol type="1">
<li>t - 1 时刻，采样完成并计算得到权重</li>
<li>t 时刻，根据 <span class="math inline">\(q(z_t|z_{1:t-1},x_{1:t})\)</span> 进行采样得到 <span class="math inline">\(z_t^i\)</span>。然后计算得到 <span class="math inline">\(N\)</span> 个权重。</li>
<li>最后对权重归一化。</li>
</ol>
<h6 id="sir">SIR:</h6>
<p>For loop:</p>
<ol type="1">
<li>predict
<ul>
<li>by system equation</li>
</ul></li>
<li>update
<ul>
<li>get <span class="math inline">\(w_t^i\)</span> by <span class="math inline">\(w_{t-1}^i\)</span></li>
</ul></li>
<li>resampling
<ul>
<li>necessary when <span class="math inline">\(N_{eff}&lt;N\)</span></li>
</ul></li>
<li>estimate
<ul>
<li><span class="math inline">\(x_t=average( w_t^i*x_t^i)\)</span></li>
</ul></li>
</ol>
<h4 id="coding">Coding</h4>
<p><a href="https://github.com/johnhw/pfilter">time series</a></p>
<p><a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python">robot</a></p>
<h2 id="reference">Reference</h2>
<p><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=93">【机器学习】【白板推导系列】【合集 1～23】</a></p>
<p><a href="https://github.com/tsyw/MachineLearningNotes">Machine Learning Notes</a></p>
<p><a href="https://www.jianshu.com/p/3d30070932a8">随机模拟-Monte Carlo积分及采样（详述直接采样、接受-拒绝采样、重要性采样）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/41217212">重要性采样（Importance Sampling）</a></p>
<p>M. S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, “A tutorial on particle filters for online nonlinear/non-gaussian Bayesian tracking,” IEEE Transactions on signal processing, vol. 50, no. 2, pp. 174–188, 2002.</p>
]]></content>
      <categories>
        <category>study</category>
        <category>Time Series</category>
      </categories>
      <tags>
        <tag>Linear Dynamical System</tag>
        <tag>Particle Filter</tag>
        <tag>Sampling</tag>
      </tags>
  </entry>
  <entry>
    <title>[Time Series] 1. Introduction</title>
    <url>/timeseries2/</url>
    <content><![CDATA[<blockquote>
<p><em>A <strong>time series</strong> is a series of data points indexed (or listed or graphed) in <strong>time order</strong>. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of <strong>discrete-time</strong> data.</em> ————Wikipedia</p>
</blockquote>
<a id="more"></a>
<h2 id="conditions">Conditions</h2>
<p>There are two sets of conditions under which much of the theory is built:</p>
<h3 id="stationary-process">1. Stationary process</h3>
<h4 id="a.-strict-sense-stationarity">a. Strict-sense stationarity</h4>
<h4 id="b.-weak-or-wide-sense-stationarity">b. Weak or wide-sense stationarity</h4>
<h5 id="definition">Definition</h5>
<ul>
<li>WSS random processes only require that 1st moment (i.e. the mean) and auto covariance do not vary with respect to time and that the 2nd moment is finite for all times.</li>
</ul>
<h4 id="c.-check">c. Check</h4>
<p>ADF检验</p>
<h4 id="d.-exchange-from-non-stationary-to-stationary">d. Exchange from non-stationary to stationary</h4>
<ul>
<li>一阶差分 <span class="math inline">\(\nabla X_T=X_T-X_{T-1}\)</span> to exclude non-stationary</li>
<li>二阶差分 <span class="math inline">\(\nabla X_T-\nabla X_{T-1}=(X_T-X_{T-1})-(X_{T-1}-X_{T-2})\)</span></li>
<li>k步差分 <span class="math inline">\(X_T-X_{T-k}\)</span> to exclude seasonality</li>
</ul>
<h3 id="ergodic-process">2. Ergodic process</h3>
<h4 id="definition-1">Definition</h4>
<ul>
<li>Its statistical properties can be deduced from a single, sufficiently long, random sample of the process.</li>
</ul>
<h2 id="model">Model</h2>
<h3 id="a.-ar-自回归模型">a. AR 自回归模型</h3>
<p><span class="math display">\[
AR(p): X_t=\alpha_1X_{t-1}+\alpha_2X_{t-2}+...+\alpha_p X_{t-p}+e_t,\,\,e_t\sim N(o,\sigma^2)
\]</span></p>
<ul>
<li><p>过去<span class="math inline">\(p\)</span>期对当前值的影响</p></li>
<li><p>没有趋势性，有相关性、随机性</p></li>
</ul>
<h3 id="b.-ma">b. MA</h3>
<p><span class="math display">\[
MA(q):X_t=e_t+\beta_1e_{t-1}+\beta_2e_{t-2}+...+\beta_qe_{t-q}，\,\,e_t\sim N(o,\sigma^2)
\]</span></p>
<ul>
<li>过去<span class="math inline">\(q\)</span>阶白噪声对当前值的影响</li>
<li>没有趋势性，有相关性、随机性</li>
</ul>
<h3 id="c.-arma">c. ARMA</h3>
<p><span class="math display">\[
X_t=\alpha_1X_{t-1}+\alpha_2X_{t-2}+...+\alpha_p X_{t-p}+\beta_1e_{t-1}+\beta_2e_{t-2}+...+\beta_qe_{t-q}+e_t，\,\,e_t\sim N(o,\sigma^2)
\]</span></p>
<ul>
<li><span class="math inline">\(ARMA(p,q)=AR(p)+MA(q)-e_t\)</span></li>
<li>没有趋势性，有相关性、随机性</li>
</ul>
<h3 id="d.-arima-ar-i-ma">d. ARIMA (AR &amp; I &amp; MA)</h3>
<h4 id="derivation">Derivation</h4>
<p>by <span class="math inline">\(ARMA(p&#39;,q)\)</span>, <span class="math display">\[
X_t-\alpha_1X_{t-1}-\alpha_2X_{t-2}-...-\alpha_p&#39; X_{t-p&#39;}=\beta_1e_{t-1}+\beta_2e_{t-2}+...+\beta_qe_{t-q}+e_t，(1)\;\;e_t\sim N(o,\sigma^2)
\]</span> let <span class="math inline">\(L^iz_t=z_{t-i}\)</span> be the lag operator. <span class="math display">\[
(1-\sum_{i=1}^{p&#39;}{\alpha_iL^i})X_t=(1+\sum_{i=1}^{q}{\beta_iL^i})e_t
\]</span> <span class="math inline">\(assume\; (1-L)^d\;is\;the\;root\;of\;(1-\sum_{i=1}^{p&#39;}{\alpha_iL^i})\)</span>, left part of (5) becomes: <span class="math display">\[
(1-\sum_{i=1}^{p&#39;}{\alpha_iL^i})X_t=(1-\sum_{i=1}^{p&#39;-d}{\alpha_iL^i})(1-L)^dX_t
\]</span> (5) becomes <span class="math inline">\(ARIMA(p,d,q)\)</span>: <span class="math display">\[
(1-\sum_{i=1}^{p}{\alpha_iL^i})(1-L)^dX_t=(1+\sum_{i=1}^{q}{\beta_iL^i})e_t
\]</span></p>
<ul>
<li><span class="math inline">\(\alpha\)</span>: coefficients of the autoregressive models</li>
<li><span class="math inline">\(\epsilon:\)</span> white noise</li>
<li><span class="math inline">\(d:\)</span> order of integrated, controls the level of differencing</li>
<li>趋势性、相关性、随机性</li>
</ul>
<h2 id="ar-in-pf">AR in PF</h2>
<p><span class="math inline">\(\begin{cases} x_k=x_{k-1}+v_{k-1}&amp;(A)\;\; system\;equation\\z_k=H_kx_k+w_k&amp;(B)\;\;measurement\;equation \end{cases}\)</span></p>
<p><span class="math inline">\(H_k=[z_{k-1}\;z_{k-2}\;...\;z_{k-p}]\)</span></p>
<p><span class="math inline">\(x_k=[\varphi_{1,k}\;\varphi_{2,k}\;...\;\varphi_{p,k}]^T\)</span>: time-varying autoregression coefficients</p>
<h2 id="appendix">Appendix</h2>
<p>Chapman-Kolmogorov equation</p>
<p><span class="math inline">\(p(y \mid x)=\int p(y \mid x, z) p(z \mid x) d z\)</span> Proof: <span class="math inline">\(p(y \mid x)=\frac{p(x, y)}{p(x)}=\frac{\int p\left(x, y_{1} z\right) d z}{p(x)}=\frac{\int p(x) p(z \mid x) p(y \mid x, z) d z}{p(x)}=\int p(y \mid x, z) p(z \mid x) d z\)</span></p>
<h2 id="reference">Reference</h2>
]]></content>
      <categories>
        <category>study</category>
        <category>Time Series</category>
      </categories>
      <tags>
        <tag>Particle Filter</tag>
        <tag>ARIMA</tag>
      </tags>
  </entry>
  <entry>
    <title>【锋言疯语】4，Do not go gentle into that good night</title>
    <url>/fyfy4/</url>
    <content><![CDATA[<blockquote>
<p>Do not go gentle into that good night,</p>
<p>Rage rage against the dying of the light!</p>
</blockquote>
<a id="more"></a>
<p>第一次看《星际穿越》是在电脑屏幕上，感觉前期蛮乏味的，可能一半都没看到就没看了。也听过网上讨论“不要温柔的走入那个良夜”咋的咋的，但自己没有听到后半句，就没多想。只是把他看成了例如石碑至于《2001太空漫游》，没兴趣搞懂，就不想了吧。得益于疫情？暑假在银幕上又看了一遍，虽说因为是连看了二场的缘故（第一场看的1917，真是坑啊），中途恍惚了一下，但好歹也看完了。也听到了后一句。再加上冰川星球上那种挣扎与抗争，似乎懂了点。</p>
<p>其实脑子里想gap的想法很早就有了，大概是起源在大二寒假，因为自己专业课有点划水，便萌生出gap一年去扎实掌握的想法，和爸妈说了一下被一顿骂。第二次有强烈欲望便是疫情下学校说不开线下课，结果也是遭到了激烈的反对。最后就是自己在拖，拖语言考试，拖找中介。在暑假调整状态，假装自己在gap中，调整自己时间安排。虽然说还有很多可以提升的，但也算是自己gap的一个底气吧。虽然但是，提交申请的时候也还是迷糊，还是gentle的走进了gap的无底黑夜。感谢爸妈的理解。</p>
<p>说到底还是缺少规划吧。很多人问我为啥gap，我的回答也五花八门。是gre，托福没考出来并且自己的水平很难短时间取得满意的成绩？是自己背景太单薄不容易伸到理想中的学校？是想补补与拓展浪费的两年知识？是实习还没弄好？是在一个学期内完成以上的种种精神和时间压力过大？自己真的很想给上面的种种理由排个序，但无从排起，所以不同的回答也就只能是以上几种答案的排列组合了。</p>
<p>是某天晚上cb快要闭馆了，自己脑子里还在想着休学的种种手续，脑子里突然想起“Do not go gentle into that good night”。回去的路上，我坐在车上，车在桥上，桥在河上，七月十五的大圆月在河里。真是good night啊。然后就好像突然明白这上下两句在说啥了（好吧我觉得当时的过程没有这么浪漫，这大概是臆想的情景）。</p>
<p>希望自己能在这一年rage下去，当然，也要保护嗓子，合理安排体力啊。一开始好像给自己找了太多事，也把脑子崩的太紧，什么一星期完成一节网课，什么8点坐到cb就要着手学习。很多的目标定的太高了，有时还是得合理一些。</p>
<p>不管怎么说，休学的第一个星期过去了，要有目标，要有调整，比如这篇想到啥就打啥的文章，得尽量控制篇数哈哈哈。</p>
]]></content>
      <categories>
        <category>锋言疯语</category>
      </categories>
  </entry>
  <entry>
    <title>【锋言疯语】3，雨天离苏客</title>
    <url>/fyfy3/</url>
    <content><![CDATA[<p>躺坐在长途客车的卧铺上。若是把驾驶员的位置记为（0，0，0），自己则位于（1，-5，1）。两边悬空，为了防止客车突然改变轨迹把电脑甩出去，自己硬挺起了上身，发灰发黑的被子垫在背下也不太够。</p>
<p>驾驶员来后面逛了一圈，挨个问到哪儿。不是汽车站买票的放到了高速服务区，有汽车站票的才会放到临沂服务区。车上顿时热闹了起来。客运站外头的兜售票的人该整改一下，客车司机是，放人下车恰饭的服务区是，客车也是。</p>
<p>从奥运年开始，自己12年中，来临沂一共11次。受制于小时候的年纪与铁路情况，来的11回中，蹭托运站的车2次，私家车1一次，其余8回都是乘客车北上。客车从苏州到临沂要9？个小时，从台州到临沂快要12个小时，托运站的车记不大请，大概是白天中午出发，次日下午到的水平？</p>
<p>前2年，自己还是小学高年级，因为担心坐长途客车遇到危险（但其实上了小学自己就能背家庭住址和电话号码了，当然，然并卵），便坐爸妈认识的托运部的车北上。车是大货车，驾驶舱是2个驾驶位，背后还有一个可以躺着休息的地方。很久远的事情了，现在只记得在后排，一边吃着零食，一边看着小汽车又超过我们了，导航牌上距离目的地的数字又小了。以及真实的高速尿急解决之道。</p>
<p>后来上了初中，便随着同村一位开客车的人来临沂。见识到了也不是一定要从车站买票才能上车。一起工作驾驶一辆车的同事也不是那么齐心协力。跟着司机在服务器内吃饭，原来服务区的人也能这么殷勤。用夸张手法，算是自己的社会启蒙了。</p>
<p>每坐一次车，便会见识到周围铺上的人，交流是很少的，更多的是端详，再加上自己主观的臆断。（2，5，2）铺上应该是个来苏州务工返乡的伯伯，手里攥着麻袋。一上来便问了我的目的地，得知是临沂后好似放了下心，后来询问，目的地差不多。他很警觉，不知道是因为第一次坐车，抑或是种种，后方一有动静便会回头看，有时我也会回头看看有没有人踩到了自己的显示器，等我回过头来，他还在看，头附着脖子，随着车颠簸着。得知我们目的地，出发地差不多后，他也问了问我的票价，只是自己带着耳机没有听清，只能点点头附和着。好几次我挺着身子拿东西的时候，他有着继续和我交谈的意思。可能是我也有，便幻觉对方也是。我不明确知道自己想了解啥，便作罢。（3，6，1）躺着的貌似是司机，我刚上车的时候想找到自己的铺子，怀疑他的位子是我的，便问了一下他，他回到“没有座位号的。”得嘞，我便躺在了现在的位置上。推测他是司机，是因为他赤着上身，躺在被子里，我觉得一般乘客会嫌脏，不过现在看起来好像推测不是很准确的样子。（1，5，2）是一个和我年纪相仿的男青年。他上车的时候我在寻思着这篇文章写啥没太注意，现在（晚上8点多）他在和女朋友视频，真好啊，刚刚拿手机的时候碰到了他的靠垫，他回头看了看呆了一下的我，便继续了。打断了一下他，真不好意思。车的最后是一个3人位，我来的时候是一个女的，可能是听司机说往后排躺吧。和我也这么说，我觉得我位置不是，环境太差，便不去了。除了车上的，还有服务区里的。刚下车是一个推着行李箱的女孩，便想到了明天就是今年高考了，我狼狈的系好鞋带后，她已经离开了。去玩餐厅路上，看到一个约莫155cm的人熟悉的点着烟，他拿着打火机的手往右下45°移了移，用食指和中指接管了香烟，我才看清这是一副初中一二年级的脸。又去了厕所，途中的摊子上有卖神奇的药材，广告说着免费试吃。厕所符合自己的预期，心里嘀咕着以后口罩得常备。</p>
<p>这篇文章写完了，我也挺了挺背，舒展一下。这车咋和我一个德行，下午慢悠悠，晚上才发狠的开起来，问了下驾驶员，到临沂要半夜1点了，又和自己的作息撞上了。哎。</p>
<p>祝好</p>
<p>2020.7.6</p>
]]></content>
      <categories>
        <category>锋言疯语</category>
      </categories>
  </entry>
  <entry>
    <title>【锋言疯语】0，Readme</title>
    <url>/fyfy0/</url>
    <content><![CDATA[<center>
<b>文笔拙劣，只是流水帐。<br> 思想空洞，全是鸡屁股。<br> 真假未知，只当听笑话。<br> 止增笑耳，全靠您海量。<b>
</center>
]]></content>
      <categories>
        <category>锋言疯语</category>
        <category>ReadMe</category>
      </categories>
  </entry>
  <entry>
    <title>[How to] 1. Wordpress+阿里云</title>
    <url>/howto1/</url>
    <content><![CDATA[<p><strong>此文基于经验而非知识，如有错误请指出，见谅!</strong> <a id="more"></a> ### 1，文章</p>
<ul>
<li>在本地电脑上事先编辑好文章。推荐用Markdown配上<a href="https://www.typora.io/">Typora</a>这款编辑器。</li>
</ul>
<h3 id="服务器与域名购买与备案">2，<a href="https://www.zhihu.com/question/36495153">服务器与域名购买与备案</a></h3>
<ul>
<li>面向国内可以考虑阿里云的服务器与域名。
<ul>
<li>大学生优惠(<a href="https://developer.aliyun.com/adc/student/">因疫情推出的免费服务器</a>)</li>
<li>服务器加购Wordpress镜像
<ul>
<li>购买好服务器后可以更改服务器系统以及镜像</li>
</ul></li>
</ul></li>
<li>面向国外推荐用设立在国外或是香港的服务器（待补充）。</li>
</ul>
<h3 id="本地wordpress搭建">3，本地Wordpress搭建</h3>
<ul>
<li>观看b站上的教程<a href="https://www.bilibili.com/video/BV1St411s7VL">WordPress大学-WordPress快速入门篇</a>。
<ul>
<li>非常基础，建议少看视频，多点鼠标尝试</li>
<li>phpstudy不推荐pro版</li>
<li>关注主题、文章、、小工具、标签与分类目录</li>
</ul></li>
<li>Wordpress文章编辑（夹带数学公式）。
<ul>
<li>推荐本地用Typora编辑好后转成html格式，再复制到Wordpress上对应的格式方块中。</li>
</ul></li>
<li>在本地中建好自己的博客。</li>
</ul>
<h3 id="服务器上的wordpress搭建教程链接同2">4，服务器上的Wordpress搭建(教程链接同2)</h3>
<ul>
<li>服务器上的Wordpress与MySQL账号及内容与本地无关。</li>
</ul>
<h3 id="本地内容上传覆盖更新到服务器">5，本地内容上传（覆盖、更新）到服务器</h3>
<ul>
<li>使用ftp上传。
<ul>
<li><a href="https://www.filezilla.cn/">官网</a>下载使用，不推荐phpstudy自带的ftp</li>
<li>注意连接是的IP地址（全数字）、密码（IP注册密码）、端口（21、22）选择</li>
<li>对端口无了解，建议试出可行的即可</li>
</ul></li>
<li><a href="https://cloud.tencent.com/developer/article/1134911">可能需要上传的内容</a></li>
<li>覆盖上传Wordpress文件夹以及数据库（做好备份）。</li>
</ul>
<h3 id="域名与ip地址绑定">6，域名与IP地址绑定</h3>
<h3 id="网站安全事宜待补充欢迎指教">7，网站安全事宜（待补充，欢迎指教）</h3>
<h3 id="其他注意事项">8，其他注意事项</h3>
<ul>
<li>做好备份。
<ul>
<li>上传内容前备份好数据库与Wordpress内容</li>
<li>调试好本地上的Wordpress后再上传到服务器端，不在服务器上直接编辑</li>
<li>只要本地端不出错，服务器上可以推倒重来（包括但不限于：更改服务覅系统、更换服务器镜像）</li>
</ul></li>
<li>Wordpress更新、下载缓慢问题。
<ul>
<li>本地
<ul>
<li>官网下载比仪表盘Dashboard中下载更快</li>
<li>VPN</li>
<li>镜像</li>
</ul></li>
<li>服务器端
<ul>
<li>本地下载、更像好后上传到服务器中</li>
</ul></li>
</ul></li>
<li>Wordpress Dashboard页无法打开可能是url错误，在数据库中更改。</li>
<li>服务器上Wordpress路径：var/www/wordpress（因服务器会有不同）。</li>
</ul>
]]></content>
      <categories>
        <category>How to</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Wordpress</tag>
        <tag>FileZilla</tag>
        <tag>Typora</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title>【锋言疯语】2，考后忆梦者</title>
    <url>/fyfy2/</url>
    <content><![CDATA[<p>‘正常人谁写日记啊？’ 嗯我是正常人，所以是这‘日记’不正常</p>
<p>虽然大三上下的状态比较割裂。但总的来说，回到了很久之前的状态。</p>
<p>就像初一刚入学的自己连I要大写都不知道，刚进入大三，知识真是一塌糊涂。ode只会照着公式做计算，建立在荒漠上的分析摇摇欲坠，统计概念停留在平均数中位数众数，代数只是个入门级别。不过还好，心思摆正了过来，虽然不似中学时代那种出自本能的听课写作业，也有点被现实所迫不得不的感觉。</p>
<p>但是有精神力也不能做出结果。大三上起码在肉体上，呆在图书馆修着9106.5的福报，当然，这块时间海绵里稍微挤挤，就能挤出一大滩的水分。一学期下来，真的能体会到那种在图书馆学习<del>摸鱼</del>的快乐。特别是闭馆铃声中的人流，雪堂街两侧被路灯照着灿黄的树叶，崩溃后的缓缓的回过神......很难说这些种种经历能激励自己走多远，但只要我想，我该是会记起这些的。如果说大三上是事在人为的话，那么大三下真的是被时代的尘埃压的喘不过气。寒假到家的每一天都想明天就回学校，很难想想从高一开始就没在家写过作业的自己能好好的学习。终于是在4月初逃离了家里的游戏桌<del>书桌</del>，进入到了想象中自己会好好学习的环境。</p>
<p>上了心，做了努力，但很难说自己取得了不错的成绩。第一学期均分没上90，第二学期甚至都没有一个大概值。越学越感觉基础的欠缺，应该熟悉的定义定理意义都不熟悉。以至于提到时不知道去学习哪一方面。分析基础的薄弱也导致自己复变函数面对众多的习题抓不住重点，没有跟上节奏，最后成绩一拖三。对ode的不了解使得即使odes有较高的成绩，却连这门课的重点思想都是这学期和人交流的时候才发现。统计的不熟悉让自己前几周的统计分布云里雾里，啃下整本书才入了道。还有是越努力越感到无力的向量场。甚至于不那么上心，不那么努力的下学期。运筹，度量空间只能算堪堪入门。pde算是从一些定理中管中窥豹。数值分析学到的内容甚至一本书的1/5都不到。</p>
<p>但成绩不等于成果，学数学也不完全是证明计算。第一个简单quiz满分的‘全都回来了的感觉’，212考前几乎能把证明推导出来的自信，考后忘得七七八八的从容，219拉跨后的‘要跟上节奏’的教训，224让自己清楚自己在分析上的差距不足，203过了变运筹、算法后对以后方向的转变。以及210，208与自己的初心，与时事结合后对自己方向的明确。网课期间对另一种学习生活、习惯的尝试与适应。前辈同行者的思维方式，困顿与收获。甚至于在21世界10年代的末尾（嗯充满了年代感）那种所有负面情绪一股脑涌上脑子的感觉，以及平复的过程，缓过来后的后劲。受益匪浅。</p>
<p>好的，又上桥成功！</p>
<p>祝好</p>
<p>2020.6.19</p>
]]></content>
      <categories>
        <category>锋言疯语</category>
      </categories>
  </entry>
  <entry>
    <title>【锋言疯语】1，考前做梦家</title>
    <url>/fyfy1/</url>
    <content><![CDATA[<p>老毛病了，总是在有紧急的眼前任务时给未来做一些很美好但是最后往往一地鸡毛的规划。毫无疑问的，这是在逃避眼前的事情。但至于为什么此刻的计划往往会一地稀碎为结尾，我想大概是一下几点吧，</p>
<p>1，是规划往往太过于理想。这时的自己往往有着负罪感，对于眼前事情越逃避，负罪感越强烈，想要弥补的欲望就越大。欲望越大，计划里就想全都要了。过于丰满往往会产生赘肉，破坏美感；抑或是过于光彩夺目，以至于不敢面对，怕自己的拙能在这美好面前显得滑稽可笑。</p>
<p>2，是关于成就感。自己是个非常需要成就感来激励自己，满足自己的粗人。这块儿有2个成就达成点，一个是完成了对于计划的计划，一个是完成了计划。这是两个可以合并也可以分离的事件。而自己往往是完成了前者，享受了微不足道缺更具迷惑感的那点点满足后，安然入睡。一觉起来，便进入了贤者时间，对把后者完成的欲望大幅下降，对预期能得到的成就感的期望与渴望也大幅降低。物质上的高支出，精神上的低收入。</p>
<p>怎么解决呢？一是在布置计划的时候，收回那踢门一脚，保持对前一个事件的成就感的渴望，把渴望做为接下来的动力。二是收回脚的同时，动起手，开始对二部分的投入，好开头是成功的一半嘛。这样，同时拥有结果的反馈与预期的成就去激励，站在前一个时间的末尾的同时也在后一个时间的开头。在2个独立事件之间找桥梁，在一个单独事件中找分割点。永远在桥上。</p>
<p>当然，方法论有了，实际操作会是怎么样呢？这篇文章是刻在桥上了，抑或只是一个终点？实践出真知。</p>
<p>祝好</p>
<p>2020.6.11</p>
]]></content>
      <categories>
        <category>锋言疯语</category>
      </categories>
  </entry>
</search>
