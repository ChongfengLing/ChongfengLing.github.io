<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/Ling.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/Ling.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/Ling.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1.0.2/themes/blue/pace-theme-minimal.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"8.0.0","exturl":false,"sidebar":{"position":"left","Pisces | Gemini":220,"display":"alwas","padding":18,"offset":20},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"search.xml","localsearch":{"enable":true,"trigger":"manual","top_n_per_article":3,"unescape":false,"preload":true}};
  </script>

  <meta name="description" content="支持向量机(Support Vector Machines)是一种二元分类模型。基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。基本模型是特征空间上的间隔最大的线性分类器，区别于感知机。学习策略为间隔最大化，可化为求解凸二次规划convex quadratic programming。学习算法为求解凸二次规划的最优算法。">
<meta property="og:type" content="article">
<meta property="og:title" content="[统计数学方法] 7. 支持向量机 SVM">
<meta property="og:url" content="http://example.com/slm007/index.html">
<meta property="og:site_name" content="LIng&#39;s Blog">
<meta property="og:description" content="支持向量机(Support Vector Machines)是一种二元分类模型。基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。基本模型是特征空间上的间隔最大的线性分类器，区别于感知机。学习策略为间隔最大化，可化为求解凸二次规划convex quadratic programming。学习算法为求解凸二次规划的最优算法。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E8%BD%AF%E9%97%B4%E9%9A%94%E5%88%86%E7%A6%BB%E5%90%91%E9%87%8F.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png">
<meta property="article:published_time" content="2020-12-22T05:33:10.385Z">
<meta property="article:modified_time" content="2021-01-02T13:40:28.948Z">
<meta property="article:author" content="Chongfeng Ling 凌崇锋">
<meta property="article:tag" content="Dual Problem">
<meta property="article:tag" content="SVM">
<meta property="article:tag" content="Kernel Method">
<meta property="article:tag" content="Maximum Margin">
<meta property="article:tag" content="Hinge Loss Function">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png">


<link rel="canonical" href="http://example.com/slm007/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>[统计数学方法] 7. 支持向量机 SVM | LIng's Blog</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LIng's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Math&Algo, ML\DL</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">39</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">19</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E7%A1%AC%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="nav-text">1. 线性可分支持向量机与硬间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Model"><span class="nav-text">1.1 Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E9%97%B4%E9%9A%94"><span class="nav-text">1.2 间隔</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-%E5%87%BD%E6%95%B0%E9%97%B4%E9%9A%94-Functional-Margin"><span class="nav-text">1.2.1 函数间隔 Functional Margin</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94-Geometric-Margin"><span class="nav-text">1.2.2 几何间隔 Geometric Margin</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="nav-text">1.3 间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-Algorithm"><span class="nav-text">1.3.1 Algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-%E8%A7%A3%E7%9A%84%E5%AD%98%E5%9C%A8%E6%80%A7%E4%B8%8E%E5%94%AF%E4%B8%80%E6%80%A7"><span class="nav-text">1.3.2 解的存在性与唯一性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-3-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F-Support-Vector-%E9%97%B4%E9%9A%94%E8%BE%B9%E7%95%8C"><span class="nav-text">1.3.3 支持向量 Support Vector &amp; 间隔边界</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E5%AF%B9%E5%81%B6%E7%AE%97%E6%B3%95-Dual-Problem"><span class="nav-text">1.4 对偶算法 Dual Problem</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-1-%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E7%9A%84%E5%AF%BC%E5%87%BA"><span class="nav-text">1.4.1 对偶问题的导出</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-2-%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">1.4.2 对偶问题的计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-3-%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3"><span class="nav-text">1.4.3 对偶问题的解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-4-Algorithm"><span class="nav-text">1.4.4 Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E8%BD%AF%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="nav-text">2. 线性支持向量机与软间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E5%8E%9F%E5%A7%8B%E9%97%AE%E9%A2%98"><span class="nav-text">2.1 原始问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0"><span class="nav-text">2.2 拉格朗日函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="nav-text">2.3 对偶问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E8%A7%A3%E7%9A%84%E7%AD%89%E4%BB%B7%E6%80%A7"><span class="nav-text">2.4 解的等价性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="nav-text">2.4 支持向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Hinge-Loss-Function"><span class="nav-text">2.5 合页损失函数 Hinge Loss Function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-1-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94"><span class="nav-text">2.5.1 损失函数对比</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-text">3. 非线性支持向量机与核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E6%A0%B8%E6%8A%80%E5%B7%A7-Kernel-Trick"><span class="nav-text">3.1 核技巧 Kernel Trick</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E6%AD%A3%E5%AE%9A%E6%A0%B8"><span class="nav-text">3.2 正定核</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-text">3.3 常用核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A0%B8%E5%87%BD%E6%95%B0-Polynomial-Kernel-Function"><span class="nav-text">3.3.1 多项式核函数 Polynomial Kernel Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-%E9%AB%98%E6%96%AF%E6%A0%B8%E5%87%BD%E6%95%B0-Gaussian-Kernel-Function"><span class="nav-text">3.3.2 高斯核函数 Gaussian Kernel Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%B8%E5%87%BD%E6%95%B0-String-Kernel-Function"><span class="nav-text">3.3.3 字符串核函数 String Kernel Function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-text">3.4 非线性支持向量机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-SMO-Algorithm-sequential-minimal-optimization"><span class="nav-text">4. SMO Algorithm (sequential minimal optimization)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Reference"><span class="nav-text">5. Reference</span></a></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chongfeng Ling 凌崇锋"
      src="/uploads/avatar.png">
  <p class="site-author-name" itemprop="name">Chongfeng Ling 凌崇锋</p>
  <div class="site-description" itemprop="description">Always on the bridge.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button animated"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ChongfengLing" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ChongfengLing" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chongfeng.zero@gmail.com" title="E-Mail → mailto:chongfeng.zero@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/www.example.com" title="Linkedin → www.example.com"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/Lingcf" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;Lingcf" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
  </div>



<script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
<div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div id="myCanvasContainer" class="widget tagcloud">
        <canvas width="250" height="250" id="resCanvas" style="width=100%">
            <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ARIMA/" rel="tag">ARIMA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AdaBoost/" rel="tag">AdaBoost</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Baye-Theorem/" rel="tag">Baye Theorem</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blog/" rel="tag">Blog</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C4-5/" rel="tag">C4.5</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CART/" rel="tag">CART</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Confidence-Interval/" rel="tag">Confidence Interval</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decision-Tree/" rel="tag">Decision Tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dual/" rel="tag">Dual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dual-Problem/" rel="tag">Dual Problem</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/" rel="tag">Entropy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FileZilla/" rel="tag">FileZilla</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gini-Index/" rel="tag">Gini Index</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo-NexT/" rel="tag">Hexo-NexT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hinge-Loss-Function/" rel="tag">Hinge Loss Function</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ID3/" rel="tag">ID3</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Info-Gain/" rel="tag">Info Gain</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/" rel="tag">KNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kernel-Method/" rel="tag">Kernel Method</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Dynamical-System/" rel="tag">Linear Dynamical System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logistic-model/" rel="tag">Logistic model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MLE/" rel="tag">MLE</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maximum-Entropy-Model/" rel="tag">Maximum Entropy Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maximum-Margin/" rel="tag">Maximum Margin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Moments/" rel="tag">Moments</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/" rel="tag">Naive Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Particle-Filter/" rel="tag">Particle Filter</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Rao-Blackwell-Theorem/" rel="tag">Rao-Blackwell Theorem</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sampling/" rel="tag">Sampling</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Typora/" rel="tag">Typora</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Wordpress/" rel="tag">Wordpress</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bias/" rel="tag">bias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/confidence-Coefficient/" rel="tag">confidence Coefficient</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kd-tree/" rel="tag">kd tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sufficient-statistics/" rel="tag">sufficient statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA-Perception/" rel="tag">感知机 Perception</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%80%A7/" rel="tag">线性可分性</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%98%BF%E9%87%8C%E4%BA%91/" rel="tag">阿里云</a><span class="tag-list-count">1</span></li></ul>
        </canvas>
    </div>
</div>

      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/slm007/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.png">
      <meta itemprop="name" content="Chongfeng Ling 凌崇锋">
      <meta itemprop="description" content="Always on the bridge.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LIng's Blog">
    </span>

    
    
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [统计数学方法] 7. 支持向量机 SVM
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-12-22 13:33:10" itemprop="dateCreated datePublished" datetime="2020-12-22T13:33:10+08:00">2020-12-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-01-02 21:40:28" itemprop="dateModified" datetime="2021-01-02T21:40:28+08:00">2021-01-02</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/study/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B/" itemprop="url" rel="index"><span itemprop="name">《统计学习方法》</span></a>
        </span>
    </span>

  
    <span id="/slm007/" class="post-meta-item leancloud_visitors" data-flag-title="[统计数学方法] 7. 支持向量机 SVM" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/slm007/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/slm007/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="Symbols count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols count in article: </span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>13 mins.</span>
    </span>
</div>


          
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>支持向量机(Support Vector Machines)是一种<strong>二元分类</strong>模型。<strong>基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。<strong>基本模型是</strong>特征空间上的间隔最大</strong>的线性分类器，区别于感知机。学习策略为间隔最大化，可化为求解凸二次规划convex quadratic programming。学习算法为求解凸二次规划的最优算法。</p>
<a id="more"></a>
<h2 id="1-线性可分支持向量机与硬间隔最大化">1. 线性可分支持向量机与硬间隔最大化</h2>
<h3 id="1-1-Model">1.1 Model</h3>
<blockquote>
<p>定义 7.1 (线性可分支持向量机 ) $\quad$ 给定<strong>线性可分</strong>训练数据集<br>
$$<br>
\begin{align}<br>
T=&amp;{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}), \cdots,\left(x_{N}, y_{N}\right)\right}\<br>
x_{i} \in \mathcal{X}=&amp;\mathbf{R}^{n}, y_{i} \in \mathcal{Y}={+1,-1}, i=1,2, \cdots, N<br>
\end{align}<br>
$$<br>
通过<strong>间隔最大化或等价地求解相应的凸二次规划问题</strong>学习得到的分离超平面为<br>
$$<br>
w^{<em>} \cdot x+b^{</em>}=0<br>
$$<br>
以及相应的分类决策函数<br>
$$<br>
f(x)=\operatorname{sign}\left(w^{<em>} \cdot x+b^{</em>}\right)<br>
$$<br>
称为线性可分支持向量机。</p>
</blockquote>
<ul>
<li>对线性可分数据集，存在无穷超平面分离数据。感知机用误分类最小策略，得到的解无穷多个。SVM用间隔最大化策略，得到的解唯一。</li>
</ul>
<h3 id="1-2-间隔">1.2 间隔</h3>
<ol>
<li>
<p>点到分离超平面的远近表示对分类预测的确信程度。</p>
</li>
<li>
<p>$y_{pre}$与$\hat{y}$的符号一致与否表示分类预测的准确性。</p>
</li>
</ol>
<h4 id="1-2-1-函数间隔-Functional-Margin">1.2.1 函数间隔 Functional Margin</h4>
<blockquote>
<p>定义 7.2 (函数间隔) $\quad$ 对于给定的训练数据集 $T$ 和超平面 $(w, b),$ 定义超平面$(w, b)$ <strong>关于样本点 $\left(x_{i}, y_{i}\right)$ 的函数间隔</strong>为<br>
$$<br>
\hat{\gamma}<em>{i}=y</em>{i}\left(w \cdot x_{i}+b\right)<br>
$$<br>
定义超平面 $(w, b)$ <strong>关于训练数据集 $T$ 的函数间隔</strong>为超平面 $(w, b)$ 对于 $T$ 中所有样本点 $\left(x_{i}, y_{i}\right)$ 的函数间隔之最小值，即<br>
$$<br>
\begin{align}<br>
\hat{\gamma}=\min <em>{i=1, \cdots, N} \hat{\gamma}</em>{i}<br>
\end{align}<br>
$$</p>
</blockquote>
<ul>
<li>
<p>(3)表示分类的准确性与准确程度</p>
</li>
<li>
<p>对超平面$w \cdot x+b=0$，成倍的改变$w,;b$不会改变该平面，但是会成倍的改变函数间隔，且<strong>倍数相等</strong></p>
</li>
</ul>
<h4 id="1-2-2-几何间隔-Geometric-Margin">1.2.2 几何间隔 Geometric Margin</h4>
<p>对分离超平面$w \cdot x+b=0$的法向量$w$进行正规化，使得$||w’||=\frac{w}{|w|}$（同时对$b$也是）。</p>
<blockquote>
<p>定义 7.3 (几何间隔) $\quad$ 对于给定的训练数据集 $T$ 和超平面 $(w, b),$ 定义超平面$(w, b)$ 关于样本,点 $\left(x_{i}, y_{i}\right)$ 的几何间隔为<br>
$$<br>
\gamma_{i}=y_{i}\left(\frac{w}{|w|} \cdot x_{i}+\frac{b}{|w|}\right)<br>
$$<br>
定义超平面 $(w, b)$ 对于训练数据集 $T$ 的几何间隔为超平面 $(w, b)$ 关千 $T$ 中所有样本点 $\left(x_{i}, y_{i}\right)$ 的几何间隔之最小值，即<br>
$$<br>
\gamma=\min <em>{i=1, \cdots, N} \gamma</em>{i}<br>
$$</p>
</blockquote>
<ul>
<li>几何间隔不随参数的改变而改变</li>
</ul>
<h3 id="1-3-间隔最大化">1.3 间隔最大化</h3>
<ul>
<li>对线性可分数据集，线性可分分离超平面有无穷多个（即感知机），但几何间隔最大的分离超平面唯一。</li>
<li>对线性可分训练集，间隔最大化又称硬间隔最大化。</li>
<li>间隔最大化，即以充分大的确信度，对训练数据进行分类；也就是说，在正负实例分开的同时，对离超平面最近的点也能有足够大的确信度。</li>
</ul>
<h4 id="1-3-1-Algorithm">1.3.1 Algorithm</h4>
<p>由几何间隔的定义可知，硬间隔最大化可以表示为<br>
$$<br>
\begin{align}<br>
\max <em>{w, b} &amp; \gamma \<br>
\text { s.t. } &amp; y</em>{i}\left(\frac{w}{|w|} \cdot x_{i}+\frac{b}{|w|}\right) \geqslant \gamma&gt;0, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$<br>
由几何间隔和函数间隔的定义，(7)转化成<br>
$$<br>
\begin{align}<br>
\max <em>{u \cdot b}&amp; \frac{\hat{\gamma}}{|w|} \<br>
\text { s.t. } &amp; y</em>{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$<br>
我们要求$w,;b$，使得$\frac{\hat{\gamma}}{|w|}$最大化，在这个目标函数与约束条件中，$\hat{y}$的取值对最后的超平面没有影响。于是我们设$\hat{y}=1$，当成单位1。同时最大化$\frac{1}{|w|}$ 和最小化 $\frac{1}{2}|w|^{2}$等价。(8)转化成<br>
$$<br>
\begin{align}<br>
\min <em>{w, b} &amp; \frac{1}{2}|w|^{2} \<br>
\text { s.t. } &amp; y</em>{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$</p>
<ul>
<li>(8-9)为凸二次规划问题convex quadratic programming</li>
</ul>
<p>求出(8-9)的解$w^<em>,;b^</em>$，我们可以得出最大间隔分离超平面$w^{<em>} \cdot x+b^{</em>}=0$ 及分类决策函数$f(x)=\operatorname{sign}\left(w^{<em>} \cdot x+b^{</em>}\right)$，即线性可分向量机模型。</p>
<blockquote>
<p>算法 7.1 (线性可分支持向量机学习算法——最大间隔法)</p>
<p>输入: 线性可分训练数据集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right}$，$x_{i}\in \mathcal{X}=\mathbf{R}^{n}$,$y_{i} \in \mathcal{Y}={-1,+1}, i=1,2, \cdots, N$</p>
<p>输出：最大间隔分离超平面和分类决策函数。</p>
<p>（1）构造并求解约束最优化问题:<br>
$$<br>
\begin{align}<br>
\min <em>{w, b} &amp; \frac{1}{2}|w|^{2} \<br>
\text { s.t. } &amp; y</em>{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$<br>
求得最优解 $w^{<em>}, b^{</em>}$ 。</p>
<p>（2）由此得到分离超平面:<br>
$$<br>
w^{<em>} \cdot x+b^{</em>}=0<br>
$$<br>
分类决策函数<br>
$$<br>
f(x)=\operatorname{sign}\left(w^{<em>} \cdot x+b^{</em>}\right)<br>
$$</p>
</blockquote>
<h4 id="1-3-2-解的存在性与唯一性">1.3.2 解的存在性与唯一性</h4>
<blockquote>
<p>定理 7.1 (最大间隔分离超平面的存在唯一性) $\quad$ 若训练数据集 $T$ 线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</p>
</blockquote>
<h4 id="1-3-3-支持向量-Support-Vector-间隔边界">1.3.3 支持向量 Support Vector &amp; 间隔边界</h4>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png" alt="image-20201216151045483" style="zoom:80%;" />
<p>对线性可分数据集：</p>
<ul>
<li>支持向量：训练样本点中与分离超平面距离最近的<strong>实例们$(x_i,;b_i)$</strong>，使得约束调节(10)取等号。即位于$H_{1,2}: w \cdot x+b=\pm1$上的两（或更多）点。</li>
<li>间隔边界：$H_1$和$H_2$</li>
</ul>
<p><strong>此模型的结果只由这些少数个支持向量决定</strong>，故此得名。</p>
<h3 id="1-4-对偶算法-Dual-Problem">1.4 对偶算法 Dual Problem</h3>
<p>求解最优化问题(10)，我们可以把其当成原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。</p>
<ol>
<li>对偶问题更容易求解。高维甚至无限维的最优化问题难求解。</li>
<li>引入核函数</li>
</ol>
<h4 id="1-4-1-对偶问题的导出">1.4.1 对偶问题的导出</h4>
<p>对于拘束最优化问题<br>
$$<br>
\begin{align}<br>
\min <em>{w, b} &amp; \frac{1}{2}|w|^{2} \<br>
\text { s.t. } &amp; y</em>{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$<br>
的每一个约束条件，我们引入一个拉格朗日乘子 Lagrange multiplier $\alpha_{i} \geqslant 0$, $i=1,2, \cdots, N$，定义拉格朗日函数 Generalized Lagrange Function，$\alpha=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}$ 为拉格朗日乘子向量。<br>
$$<br>
\begin{align}<br>
L(w, b, \alpha)&amp;=\frac{1}{2}|w|^{2}-\sum_{i=1}^{N} \alpha_{i}(y_{i}\left(w \cdot x_{i}+b\right)-1)\nonumber<br>
\&amp;=\frac{1}{2}|w|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}<br>
\end{align}<br>
$$<br>
带拘束最优化问题(12-13)可以变成无拘束优化问题(15-16)<br>
$$<br>
\begin{align}<br>
\min _{w, b} \max <em>{\lambda} \mathcal{L}(w, b, \alpha) \<br>
\text { s.t. } \lambda</em>{i} \geqslant 0<br>
\end{align}<br>
$$</p>
<p>由强对偶关系，拘束问题(15-16)可以化为无拘束优化问题<br>
$$<br>
\begin{align}<br>
\max _{\lambda} \min <em>{w, b} \mathcal{L}(w, b, \alpha) \<br>
\text { s.t. } \lambda</em>{i} \geqslant 0<br>
\end{align}<br>
$$</p>
<p>由上面的拉格朗日（强）对偶性可得，原始问题(12-13)的对偶问题是极大极小问题(17-18)</p>
<ul>
<li>详细推导见：【拉格朗日对偶，等价对偶以及KKT条件】（文件没保存，有缘再续）</li>
</ul>
<h4 id="1-4-2-对偶问题的计算">1.4.2 对偶问题的计算</h4>
<p><strong>1.4.2.1: 求$\min <em>{w, b} L(w, b, \alpha)=\frac{1}{2}|w|^{2}-\sum</em>{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}$</strong></p>
<p>计算$\min <em>{w, b} L(w, b, \alpha)$对变量$w,;b$的偏导为0<br>
$$<br>
\begin{align}<br>
\nabla</em>{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0 \<br>
\nabla_{b} L(w, b, \alpha)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0\nonumber\<br>
\end{align}<br>
$$<br>
得<br>
$$<br>
\begin{align}<br>
w&amp;=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}\<br>
\sum_{i=1}^{N} \alpha_{i} y_{i}&amp;=0<br>
\end{align}<br>
$$<br>
把(20-21)带入到(14)，得<br>
$$<br>
\begin{aligned}<br>
L(w, b, \alpha) &amp;=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \<br>
&amp;=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}<br>
\end{aligned}<br>
$$<br>
得到结果<br>
$$<br>
\min <em>{w, b} L(w, b, \alpha)=-\frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}<br>
$$<br>
<strong>求$\min _{w, b} L(w, b, \alpha) \text { 对 } \alpha \text { 的极大 }$</strong><br>
$$<br>
\begin{aligned}<br>
\max <em>{\alpha} &amp; -\frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \<br>
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>
\end{aligned}<br>
$$<br>
调整符号，得与原始问题等价的对偶最优化问题<br>
$$<br>
\begin{align}<br>
\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$</p>
<ul>
<li>$\sum_{i=1}^{N} \alpha_{i} y_{i}=0$意味着很大部分的$\alpha=0$，即决定超平面的只是很小一部分的支持向量。</li>
</ul>
<h4 id="1-4-3-对偶问题的解">1.4.3 对偶问题的解</h4>
<blockquote>
<p>定理 7.2 设 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{l}^{</em>}\right)^{\mathrm{T}}$ 是对偶最优化问题的解，则存在下标 $j,$ 使得 $\alpha_{j}^{<em>}&gt;0,$ 并可按下式求得原始最优化问题的解 $w^{</em>}, b^{<em>}$ :<br>
$$<br>
\begin{align}<br>
w^{</em>}=\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i} x_{i} \<br>
b^{</em>}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)<br>
\end{align}<br>
$$</p>
</blockquote>
<p>分离超平面为$\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i}\left(x \cdot x_{i}\right)+b^{</em>}=0$</p>
<p>分类决策函数为$f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i}\left(x \cdot x_{i}\right)+b^{</em>}\right)$</p>
<h4 id="1-4-4-Algorithm">1.4.4 Algorithm</h4>
<blockquote>
<p>算法 7.2 (线性可分支持向量机学习算法)</p>
<p>输入: 线性可分训练集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},$ 其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}$, $y_{i} \in \mathcal{Y}={-1,+1}, i=1,2, \cdots, N$</p>
<p>输出：分离超平面和分类决策函数。</p>
<p>（1）构造并求解约束最优化问题<br>
$$<br>
\begin{align}<br>
\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$<br>
求得最优解 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$ 。<br>
(2) 计算<br>
$$<br>
w^{<em>}=\sum_{i=1}^{N} \alpha_{i}^{</em>} y_{i} x_{i}<br>
$$<br>
并选择 $\alpha^{<em>}$ 的一个正分量 $\alpha_{j}^{</em>}&gt;0,$ 计算<br>
$$<br>
b^{<em>}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{</em>} y_{i}\left(x_{i} \cdot x_{j}\right)<br>
$$<br>
（3）求得分离超平面<br>
$$<br>
w^{<em>} \cdot x+b^{</em>}=0<br>
$$<br>
分类决策函数:<br>
$$<br>
f(x)=\operatorname{sign}\left(w^{<em>} \cdot x+b^{</em>}\right)<br>
$$</p>
</blockquote>
<p><br><br></p>
<h2 id="2-线性支持向量机与软间隔最大化">2. 线性支持向量机与软间隔最大化</h2>
<p>对线性不可分数据集，存在一些特异点（outlier）使得不等式不能全部满足，即函数间隔大于1。</p>
<p>因此我们引入松弛变量$\xi_i \geq0$，使得函数间隔大于等于$1-\xi_i$。同时对松弛变量付出一个惩罚参数。</p>
<ul>
<li>
<p>约束条件<br>
$$<br>
y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}<br>
$$</p>
</li>
<li>
<p>目标函数<br>
$$<br>
\frac{1}{2}|w|^{2}+C \sum_{i=1}^{N} \xi_{i}<br>
$$</p>
<ul>
<li>$C$为惩罚参数。$C$越大，对误分类的惩罚越大</li>
<li>最小化目标函数，即使$\frac{1}{2}|w|^{2}$尽可能小，间隔尽可能大</li>
</ul>
</li>
</ul>
<h3 id="2-1-原始问题">2.1 原始问题</h3>
<p>$$<br>
\begin{align}<br>
\min <em>{w, b, \xi} &amp; \frac{1}{2}|w|^{2}+C \sum</em>{i=1}^{N} \xi_{i} \<br>
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \<br>
&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$</p>
<ul>
<li>凸二次规划问题</li>
<li>$w$的解唯一</li>
<li>$b$的解可能不唯一，而是一个区间</li>
</ul>
<h3 id="2-2-拉格朗日函数">2.2 拉格朗日函数</h3>
<p>$$<br>
L(w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}|w|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}\<br>
\alpha_i\geq 0,;\mu_i\geq0<br>
$$</p>
<h3 id="2-3-对偶问题">2.3 对偶问题</h3>
<p>原始问题(30-32)的对偶问题使拉格朗日函数的极大极小问题。<br>
$$<br>
\begin{align}<br>
\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$</p>
<h3 id="2-4-解的等价性">2.4 解的等价性</h3>
<blockquote>
<p>定理 7.3 设 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$ 是对偶问题的一个解，若存在 $\alpha^{<em>}$ 的一个分量 $\alpha_{j}^{</em>}, 0&lt;\alpha_{j}^{<em>}&lt;C,$ 则原始问题的解 $w^{</em>}, b^{<em>}$ 可按下式 求得:<br>
$$<br>
\begin{align}<br>
w^{</em>}=\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i} x_{i} \<br>
b^{</em>}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)<br>
\end{align}<br>
$$</p>
</blockquote>
<blockquote>
<p>算法 7.3 (线性支持向量机学习算法)</p>
<p>输入: 训练数据集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},$ 其中， $x_{i} \in \mathcal{X}=\mathbf{R}^{n}$, $y_{i} \in \mathcal{Y}={-1,+1}, i=1,2, \cdots, N$</p>
<p>输出：分离超平面和分类决策函数。</p>
<p>（1）选择惩罚参数 $C&gt;0,$ 构造并求解凸二次规划问题<br>
$$<br>
\begin{align}<br>
\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$<br>
求得最优解 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$</p>
<p>（2） 计算 $w^{<em>}=\sum_{i=1}^{N} \alpha_{i}^{</em>} y_{i} x_{i}$<br>
选择 $\alpha^{<em>}$ 的一个分量 $\alpha_{j}^{</em>}$ 适合条件 $0&lt;\alpha_{j}^{<em>}&lt;C$ ，计算<br>
$$<br>
b^{</em>}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{<em>}\left(x_{i} \cdot x_{j}\right)<br>
$$<br>
（3）求得分离超平面<br>
$$<br>
w^{</em>} \cdot x+b^{<em>}=0<br>
$$<br>
分类决策函数:<br>
$$<br>
f(x)=\operatorname{sign}\left(w^{</em>} \cdot x+b^{*}\right)<br>
$$</p>
</blockquote>
<ul>
<li>第二部中，每一个合适的$\alpha$都会得出一个$b$，所以$b$不唯一</li>
</ul>
<h3 id="2-4-支持向量">2.4 支持向量</h3>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E8%BD%AF%E9%97%B4%E9%9A%94%E5%88%86%E7%A6%BB%E5%90%91%E9%87%8F.png" alt="image-20201216151045483" style="zoom:50%;" />
<p>虚线是间隔边界，实线是分离超平面。对每一个实例点$(x_i,y_i)$和他们的$\alpha _i^*,;\xi_i$，我们有</p>
<ul>
<li>$\alpha_{i}^{*}&lt;C$，则 $\xi_{i}=0$</li>
<li>$\alpha_{i}^{*}=C, 0&lt;\xi_{i}&lt;1$，则</li>
<li>$\alpha_{i}^{*}=C, \xi_{i}=1$，则</li>
<li>$\alpha_{i}^{*}=C, \xi_{i}&gt;1$，则</li>
<li>没有$\alpha_{i}^{*}=C, 0&lt;\xi_{i}&lt;1$</li>
</ul>
<h3 id="2-5-合页损失函数-Hinge-Loss-Function">2.5 合页损失函数 Hinge Loss Function</h3>
<blockquote>
<p>定理 7.4 线性支持向量机原始最优化问题:<br>
$$<br>
\begin{align}<br>
\min <em>{w, b, \xi} &amp; \frac{1}{2}|w|^{2}+C \sum</em>{i=1}^{N} \xi_{i} \<br>
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \<br>
&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$<br>
等价于最优化问题<br>
$$<br>
\min <em>{w, b} \sum</em>{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda|w|^{2}<br>
$$</p>
</blockquote>
<ul>
<li>$[z]_{+}=\left{\begin{array}{ll}z, &amp; z&gt;0 \ 0, &amp; z \leqslant 0\end{array}\right.$表示取正值的函数。</li>
<li>$L(y(w \cdot x+b))=[1-y(w \cdot x+b)]_{+}$ 经验损失。但实例点被正确分类且函数间隔大于1，损失才是0。位于间隔边界上的实例点损失不为0。</li>
<li>$\lambda|w|^{2}$ 正则化项</li>
</ul>
<h4 id="2-5-1-损失函数对比">2.5.1 损失函数对比</h4>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt="image-20201216155233841" style="zoom:50%;" />
<p>虚线为感知机的损失函数$\left[-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}$</p>
<ul>
<li>0-1损失函数为二元分类真正的损失函数。<strong>不过因其不连续可导，无法直接优化它所构成的函数</strong>。</li>
<li>合页损失函数是0-1损失函数的上界。<strong>线性支持向量机是优化由0-1损失函数的上界构成的目标函数</strong>。</li>
<li>合页损失函数对学习有更高要求。不但需要分类正确，还要确信度（距离）足够高才为0损失。</li>
</ul>
<p><br><br></p>
<h2 id="3-非线性支持向量机与核函数">3. 非线性支持向量机与核函数</h2>
<p>对于一个非线性数据集，首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。</p>
<p>由于特征空间是高维甚至无限维，加上对偶问题中多组高维向量计算内积。因此引入核函数减少计算量。</p>
<h3 id="3-1-核技巧-Kernel-Trick">3.1 核技巧 Kernel Trick</h3>
<p>核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间 (欧氏空间 $\mathbf{R}^{n}$ 或离散集合) 对应于一个特征空间 (希尔伯特空间 $\mathcal{H}$ )，使得在输入空间 $\mathbf{R}^{n}$ 中的超曲面模型对应于特征空间 $\mathcal{H}$ 中的超平面模型支持向量机）。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。</p>
<blockquote>
<p>定义 7.6 (核函数) $\quad$ 设 $\mathcal{X}$ 是输入空间 ( 欧氏空间 $\mathbf{R}^{n}$ 的子集或离散集合 $),$ 又设 $\mathcal{H}$ 为特征空间 $($ 希 尔伯特空间 $),$ 如果存在一个从 $\mathcal{X}$ 到 $\mathcal{H}$ 的映射<br>
$$<br>
\phi(x): \mathcal{X} \rightarrow \mathcal{H}<br>
$$<br>
使得对所有 $x, z \in \mathcal{X},$ 函数 $K(x, z)$ 满足条件<br>
$$<br>
K(x, z)=\phi(x) \cdot \phi(z)<br>
$$<br>
则称 <strong>$K(x, z)$ 为核函数， $\phi(x)$ 为映射函数</strong>，式中 $\phi(x) \cdot \phi(z)$ 为 $\phi(x)$ 和 $\phi(z)$ 的内积。</p>
</blockquote>
<ul>
<li><strong>在使用过程中只定义核函数$K(x,z)$，不显式定义映射函数$\phi$</strong></li>
<li>特征空间$\mathcal{H}$一般是高维甚至无限维</li>
<li>给定核$K(x,z)$，特征空间和映射函数不唯一</li>
</ul>
<p>在支持向量机中，通过映射函数$\phi$把输入空间变换到新的特征空间，在新的特征空间中训练线性支持向量机。此时对偶问题的目标函数成为$W(\alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}$，分类决策函数成为$f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{<em>} y_{i} \phi\left(x_{i}\right) \cdot \phi(x)+b^{</em>}\right)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{<em>} y_{i} K\left(x_{i}, x\right)+b^{</em>}\right)$</p>
<h3 id="3-2-正定核">3.2 正定核</h3>
<blockquote>
<p>定理 7.5 (正定核的充要条件) $\quad$ 设 $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbf{R}$ 是对称函数，则 $K(x, z)$ 为正 定核函数的充要条件是对任意 $x_{i} \in \mathcal{X}, i=1,2, \cdots, m, K(x, z)$ 对应的 Gram 矩阵:<br>
$$<br>
K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}<br>
$$<br>
是半正定矩阵。</p>
</blockquote>
<blockquote>
<p>定义 7.7 (正定核的等价定义) $\quad$ 设 $\mathcal{X} \subset \mathbf{R}^{n}, K(x, z)$ 是定义在 $\mathcal{X} \times \mathcal{X}$ 上的对称<br>
函数, 如果对任意 $x_{i} \in \mathcal{X}, i=1,2, \cdots, m, K(x, z)$ 对应的 Gram 矩阵<br>
$$<br>
K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}<br>
$$</p>
</blockquote>
<h3 id="3-3-常用核函数">3.3 常用核函数</h3>
<h4 id="3-3-1-多项式核函数-Polynomial-Kernel-Function">3.3.1 多项式核函数 Polynomial Kernel Function</h4>
<p>$$<br>
K(x, z)=(x \cdot z+1)^{p}<br>
$$</p>
<p>对应的支持向量机是一个 $p$ 次多项式分类器。在此情形下，分类决策函数成为<br>
$$<br>
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{<em>} y_{i}\left(x_{i} \cdot x+1\right)^{p}+b^{</em>}\right)<br>
$$</p>
<h4 id="3-3-2-高斯核函数-Gaussian-Kernel-Function">3.3.2 高斯核函数 Gaussian Kernel Function</h4>
<p>$$<br>
K(x, z)=\exp \left(-\frac{|x-z|^{2}}{2 \sigma^{2}}\right)<br>
$$</p>
<p>对应的支持向量机是高斯径向基函数（radial basis function）分类器。在此情形下，分类决策函数成为<br>
$$<br>
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{<em>} y_{i} \exp \left(-\frac{\left|x-x_{i}\right|^{2}}{2 \sigma^{2}}\right)+b^{</em>}\right)<br>
$$</p>
<h4 id="3-3-3-字符串核函数-String-Kernel-Function">3.3.3 字符串核函数 String Kernel Function</h4>
<p>定义在离散数据集合上。</p>
<h3 id="3-4-非线性支持向量机">3.4 非线性支持向量机</h3>
<blockquote>
<p>定义 7.8 (非线性支持向量机 $) \quad$ 从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划(46)学习得到的分类决策函数<br>
$$<br>
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i} K\left(x, x_{i}\right)+b^{</em>}\right)<br>
$$<br>
称为非线性支持向量机, $K(x, z)$ 是正定核涵数。</p>
</blockquote>
<blockquote>
<p>算法 7.4 (非线性支持向量机学习算法)</p>
<p>输入: 训练数据集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},$ 其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in$$\mathcal{Y}={-1,+1}, i=1,2, \cdots, N$</p>
<p>输出：分类决策函数。</p>
<p>（1）选取适当的核函数 $K(x, z)$ 和适当的参数 $C,$ 构造并求解最优化问题<br>
$$<br>
\begin{align}<br>
\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$<br>
求得最优解 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$ 。<br>
（2）选择 $\alpha^{<em>}$ 的一个正分量 $0&lt;\alpha_{j}^{</em>}&lt;C,$ 计算<br>
$$<br>
b^{<em>}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{</em>} y_{i} K\left(x_{i}, x_{j}\right)<br>
$$<br>
（3）构造决策函数:<br>
$$<br>
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i} K\left(x, x_{i}\right)+b^{</em>}\right)<br>
$$</p>
</blockquote>
<ul>
<li>$K(x,z)$为正定核函数是，问题(46)是二次规划问题，解存在。</li>
</ul>
<p><br><br></p>
<h2 id="4-SMO-Algorithm-sequential-minimal-optimization">4. SMO Algorithm (sequential minimal optimization)</h2>
<p>序列最小算法用来求解凸二次规划的对偶问题<br>
$$<br>
\begin{align}<br>
\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N<br>
\end{align}<br>
$$</p>
<p><br><br></p>
<h2 id="5-Reference">5. Reference</h2>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>

    </div>


    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>
    

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Dual-Problem/" rel="tag"><i class="fa fa-tag"></i> Dual Problem</a>
              <a href="/tags/SVM/" rel="tag"><i class="fa fa-tag"></i> SVM</a>
              <a href="/tags/Kernel-Method/" rel="tag"><i class="fa fa-tag"></i> Kernel Method</a>
              <a href="/tags/Maximum-Margin/" rel="tag"><i class="fa fa-tag"></i> Maximum Margin</a>
              <a href="/tags/Hinge-Loss-Function/" rel="tag"><i class="fa fa-tag"></i> Hinge Loss Function</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/howto2/" rel="prev" title="[How to] 2. Hexo+Github+NexT">
                  <i class="fa fa-chevron-left"></i> [How to] 2. Hexo+Github+NexT
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/slm008/" rel="next" title="[统计数学方法] 8. 提升方法 Boosting">
                  [统计数学方法] 8. 提升方法 Boosting <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
  
  
  



      
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chongfeng Ling 凌崇锋</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Symbols count total">88k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">1:20</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  <script src="/js/local-search.js"></script>












  








  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.0/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
<script>
NexT.utils.loadComments('#valine-comments', () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', () => {
    new Valine(Object.assign({
      el  : '#valine-comments',
      path: "/slm007/",
    }, {"enable":true,"appId":"2jasoi9GfAqCxft7Pa84eFgg-MdYXbMMI","appKey":"zoneD6dBAq92qKtR85UaOHz5","placeholder":"Just go go","avatar":"mm","meta":["nick","mail"],"pageSize":10,"lang":null,"visitor":true,"comment_count":true,"recordIP":true,"serverURLs":null,"enableQQ":false,"requiredFields":["nick"]}
    ));
  }, window.Valine);
});
</script>

</body>
</html>
