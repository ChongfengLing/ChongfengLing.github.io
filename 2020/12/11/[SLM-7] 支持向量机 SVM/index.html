<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","version":"8.1.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="支持向量机 Support Vector Machines 支持向量机(Support Vector Machines)是一种二元分类模型。基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。基本模型是特征空间上的间隔最大的线性分类器，区别于感知机。学习策略为间隔最大化，可化为求解凸二次规划convex quadratic programming。学习算法为求解凸二次规划的最优算法">
<meta property="og:type" content="article">
<meta property="og:title" content="Chongfeng Ling">
<meta property="og:url" content="http://example.com/2020/12/11/[SLM-7]%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%20SVM/index.html">
<meta property="og:site_name" content="Chongfeng Ling">
<meta property="og:description" content="支持向量机 Support Vector Machines 支持向量机(Support Vector Machines)是一种二元分类模型。基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。基本模型是特征空间上的间隔最大的线性分类器，区别于感知机。学习策略为间隔最大化，可化为求解凸二次规划convex quadratic programming。学习算法为求解凸二次规划的最优算法">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E8%BD%AF%E9%97%B4%E9%9A%94%E5%88%86%E7%A6%BB%E5%90%91%E9%87%8F.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png">
<meta property="article:published_time" content="2020-12-11T09:06:20.416Z">
<meta property="article:modified_time" content="2020-12-20T03:12:51.365Z">
<meta property="article:author" content="CHongfeng Ling 凌崇锋">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png">


<link rel="canonical" href="http://example.com/2020/12/11/[SLM-7]%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%20SVM/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title> | Chongfeng Ling</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chongfeng Ling</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">subtitle</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-support-vector-machines"><span class="nav-number">1.</span> <span class="nav-text">支持向量机 Support Vector Machines</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E7%A1%AC%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="nav-number">1.1.</span> <span class="nav-text">1. 线性可分支持向量机与硬间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%B4%E9%9A%94"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 间隔</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E9%97%B4%E9%9A%94-functional-margin"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">1.2.1 函数间隔 Functional Margin</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94-geometric-margin"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">1.2.2 几何间隔 Geometric Margin</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#algorithm"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">1.3.1 Algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E7%9A%84%E5%AD%98%E5%9C%A8%E6%80%A7%E4%B8%8E%E5%94%AF%E4%B8%80%E6%80%A7"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">1.3.2 解的存在性与唯一性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F-support-vector-%E9%97%B4%E9%9A%94%E8%BE%B9%E7%95%8C"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">1.3.3 支持向量 Support Vector &amp; 间隔边界</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E7%AE%97%E6%B3%95-dual-problem"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 对偶算法 Dual Problem</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E7%9A%84%E5%AF%BC%E5%87%BA"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">1.4.1 对偶问题的导出</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">1.4.2 对偶问题的计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3"><span class="nav-number">1.1.4.3.</span> <span class="nav-text">1.4.3 对偶问题的解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#algorithm-1"><span class="nav-number">1.1.4.4.</span> <span class="nav-text">1.4.4 Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E8%BD%AF%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="nav-number">1.2.</span> <span class="nav-text">2. 线性支持向量机与软间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E5%A7%8B%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 原始问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 拉格朗日函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 对偶问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E7%9A%84%E7%AD%89%E4%BB%B7%E6%80%A7"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 解的等价性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="nav-number">1.2.5.</span> <span class="nav-text">2.4 支持向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-hinge-loss-function"><span class="nav-number">1.2.6.</span> <span class="nav-text">2.5 合页损失函数 Hinge Loss Function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94"><span class="nav-number">1.2.6.1.</span> <span class="nav-text">2.5.1 损失函数对比</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.</span> <span class="nav-text">3. 非线性支持向量机与核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E6%8A%80%E5%B7%A7-kernel-trick"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 核技巧 Kernel Trick</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%AE%9A%E6%A0%B8"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 正定核</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3 常用核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A0%B8%E5%87%BD%E6%95%B0-polynomial-kernel-function"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">3.3.1 多项式核函数 Polynomial Kernel Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E6%A0%B8%E5%87%BD%E6%95%B0-gaussian-kernel-function"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">3.3.2 高斯核函数 Gaussian Kernel Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%B8%E5%87%BD%E6%95%B0-string-kernel-function"><span class="nav-number">1.3.3.3.</span> <span class="nav-text">3.3.3 字符串核函数 String Kernel Function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">1.3.4.</span> <span class="nav-text">3.4 非线性支持向量机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#smo-algorithm-sequential-minimal-optimization"><span class="nav-number">1.4.</span> <span class="nav-text">4. SMO Algorithm (sequential minimal optimization)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reference"><span class="nav-number">1.5.</span> <span class="nav-text">5. Reference</span></a></li></ol></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">CHongfeng Ling 凌崇锋</p>
  <div class="site-description" itemprop="description">description</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/11/[SLM-7]%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%20SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-12-11 17:06:20" itemprop="dateCreated datePublished" datetime="2020-12-11T17:06:20+08:00">2020-12-11</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-20 11:12:51" itemprop="dateModified" datetime="2020-12-20T11:12:51+08:00">2020-12-20</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="支持向量机-support-vector-machines">支持向量机 Support Vector Machines</h1>
<p>支持向量机(Support Vector Machines)是一种<strong>二元分类</strong>模型。<strong>基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。</strong>基本模型是<strong>特征空间上的间隔最大</strong>的线性分类器，区别于感知机。学习策略为间隔最大化，可化为求解凸二次规划convex quadratic programming。学习算法为求解凸二次规划的最优算法。</p>
<h2 id="线性可分支持向量机与硬间隔最大化">1. 线性可分支持向量机与硬间隔最大化</h2>
<h3 id="model">1.1 Model</h3>
<blockquote>
<p>定义 7.1 (线性可分支持向量机 ) <span class="math inline">\(\quad\)</span> 给定<strong>线性可分</strong>训练数据集 <span class="math display">\[
\begin{array}{1}
T=\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}), \cdots,\left(x_{N}, y_{N}\right)\right\},\nonumber\\
x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N
\end{array}
\]</span> 通过<strong>间隔最大化或等价地求解相应的凸二次规划问题</strong>学习得到的分离超平面为 <span class="math display">\[
w^{*} \cdot x+b^{*}=0
\]</span> 以及相应的分类决策函数 <span class="math display">\[
f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\]</span> 称为线性可分支持向量机。</p>
</blockquote>
<ul>
<li>对线性可分数据集，存在无穷超平面分离数据。感知机用误分类最小策略，得到的解无穷多个。SVM用间隔最大化策略，得到的解唯一。</li>
</ul>
<h3 id="间隔">1.2 间隔</h3>
<ol type="1">
<li><p>点到分离超平面的远近表示对分类预测的确信程度。</p></li>
<li><p><span class="math inline">\(y_{pre}\)</span>与<span class="math inline">\(\hat{y}\)</span>的符号一致与否表示分类预测的准确性。</p></li>
</ol>
<h4 id="函数间隔-functional-margin">1.2.1 函数间隔 Functional Margin</h4>
<blockquote>
<p>定义 7.2 (函数间隔) <span class="math inline">\(\quad\)</span> 对于给定的训练数据集 <span class="math inline">\(T\)</span> 和超平面 <span class="math inline">\((w, b),\)</span> 定义超平面<span class="math inline">\((w, b)\)</span> <strong>关于样本点 <span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span> 的函数间隔</strong>为 <span class="math display">\[
\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)
\]</span> 定义超平面 <span class="math inline">\((w, b)\)</span> <strong>关于训练数据集 <span class="math inline">\(T\)</span> 的函数间隔</strong>为超平面 <span class="math inline">\((w, b)\)</span> 对于 <span class="math inline">\(T\)</span> 中所有样本点 <span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span> 的函数间隔之最小值，即 <span class="math display">\[
\hat{\gamma}=\min _{i=1, \cdots, N} \hat{\gamma}_{i}
\]</span></p>
</blockquote>
<ul>
<li><p>(3)表示分类的准确性与准确程度</p></li>
<li><p>对超平面<span class="math inline">\(w \cdot x+b=0\)</span>，成倍的改变<span class="math inline">\(w,\;b\)</span>不会改变该平面，但是会成倍的改变函数间隔，且<strong>倍数相等</strong></p></li>
</ul>
<h4 id="几何间隔-geometric-margin">1.2.2 几何间隔 Geometric Margin</h4>
<p>对分离超平面<span class="math inline">\(w \cdot x+b=0\)</span>的法向量<span class="math inline">\(w\)</span>进行正规化，使得<span class="math inline">\(||w&#39;||=\frac{w}{\|w\|}\)</span>（同时对<span class="math inline">\(b\)</span>也是）。</p>
<blockquote>
<p>定义 7.3 (几何间隔) <span class="math inline">\(\quad\)</span> 对于给定的训练数据集 <span class="math inline">\(T\)</span> 和超平面 <span class="math inline">\((w, b),\)</span> 定义超平面<span class="math inline">\((w, b)\)</span> 关于样本,点 <span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span> 的几何间隔为 <span class="math display">\[
\gamma_{i}=y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)
\]</span> 定义超平面 <span class="math inline">\((w, b)\)</span> 对于训练数据集 <span class="math inline">\(T\)</span> 的几何间隔为超平面 <span class="math inline">\((w, b)\)</span> 关千 <span class="math inline">\(T\)</span> 中所有样本点 <span class="math inline">\(\left(x_{i}, y_{i}\right)\)</span> 的几何间隔之最小值，即 <span class="math display">\[
\gamma=\min _{i=1, \cdots, N} \gamma_{i}
\]</span></p>
</blockquote>
<ul>
<li>几何间隔不随参数的改变而改变</li>
</ul>
<h3 id="间隔最大化">1.3 间隔最大化</h3>
<ul>
<li>对线性可分数据集，线性可分分离超平面有无穷多个（即感知机），但几何间隔最大的分离超平面唯一。</li>
<li>对线性可分训练集，间隔最大化又称硬间隔最大化。</li>
<li>间隔最大化，即以充分大的确信度，对训练数据进行分类；也就是说，在正负实例分开的同时，对离超平面最近的点也能有足够大的确信度。</li>
</ul>
<h4 id="algorithm">1.3.1 Algorithm</h4>
<p>由几何间隔的定义可知，硬间隔最大化可以表示为 <span class="math display">\[
\begin{array}{ll}
\max _{w, b} &amp; \gamma \\
\text { s.t. } &amp; y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma&gt;0, \quad i=1,2, \cdots, N
\end{array}
\]</span> 由几何间隔和函数间隔的定义，(7)转化成 <span class="math display">\[
\begin{array}{ll}
\max _{u \cdot b}&amp; \frac{\hat{\gamma}}{\|w\|} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N
\end{array}
\]</span> 我们要求<span class="math inline">\(w,\;b\)</span>，使得<span class="math inline">\(\frac{\hat{\gamma}}{\|w\|}\)</span>最大化，在这个目标函数与约束条件中，<span class="math inline">\(\hat{y}\)</span>的取值对最后的超平面没有影响。于是我们设<span class="math inline">\(\hat{y}=1\)</span>，当成单位1。同时最大化<span class="math inline">\(\frac{1}{\|w\|}\)</span> 和最小化 <span class="math inline">\(\frac{1}{2}\|w\|^{2}\)</span>等价。(8)转化成 <span class="math display">\[
\begin{array}{ll}
\min _{w, b} &amp; \frac{1}{2}\|w\|^{2} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]</span></p>
<ul>
<li>(9)为凸二次规划问题convex quadratic programming</li>
</ul>
<p>求出(9)的解<span class="math inline">\(w^*,\;b^*\)</span>，我们可以得出最大间隔分离超平面<span class="math inline">\(w^{*} \cdot x+b^{*}=0\)</span> 及分类决策函数<span class="math inline">\(f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)\)</span>，即线性可分向量机模型。</p>
<blockquote>
<p>算法 7.1 (线性可分支持向量机学习算法————最大间隔法) 输入: 线性可分训练数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}\)</span>，<span class="math inline">\(x_{i}\in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N\)</span> 输出：最大间隔分离超平面和分类决策函数。</p>
<p>（1）构造并求解约束最优化问题: <span class="math display">\[
\begin{array}{ll}
\min _{w, b} &amp; \frac{1}{2}\|w\|^{2} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]</span> 求得最优解 <span class="math inline">\(w^{*}, b^{*}\)</span> 。 （2）由此得到分离超平面: <span class="math display">\[
w^{*} \cdot x+b^{*}=0
\]</span> 分类决策函数 <span class="math display">\[
f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\]</span></p>
</blockquote>
<h4 id="解的存在性与唯一性">1.3.2 解的存在性与唯一性</h4>
<blockquote>
<p>定理 7.1 (最大间隔分离超平面的存在唯一性) <span class="math inline">\(\quad\)</span> 若训练数据集 <span class="math inline">\(T\)</span> 线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</p>
</blockquote>
<h4 id="支持向量-support-vector-间隔边界">1.3.3 支持向量 Support Vector &amp; 间隔边界</h4>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png" alt="image-20201216151045483" style="zoom:80%;" /></p>
<p>对线性可分数据集：</p>
<ul>
<li>支持向量：训练样本点中与分离超平面距离最近的<strong>实例们<span class="math inline">\((x_i,\;b_i)\)</span></strong>，使得约束调节(10)取等号。即位于<span class="math inline">\(H_{1,2}: w \cdot x+b=\pm1\)</span>上的两（或更多）点。</li>
<li>间隔边界：<span class="math inline">\(H_1\)</span>和<span class="math inline">\(H_2\)</span></li>
</ul>
<p><strong>此模型的结果只由这些少数个支持向量决定</strong>，故此得名。</p>
<h3 id="对偶算法-dual-problem">1.4 对偶算法 Dual Problem</h3>
<p>求解最优化问题(10)，我们可以把其当成原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。</p>
<ol type="1">
<li>对偶问题更容易求解。高维甚至无限维的最优化问题难求解。</li>
<li>引入核函数</li>
</ol>
<h4 id="对偶问题的导出">1.4.1 对偶问题的导出</h4>
<p>对于拘束最优化问题(10) <span class="math display">\[
\begin{array}{ll}
\min _{w, b} &amp; \frac{1}{2}\|w\|^{2} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N\nonumber \tag{10}
\end{array}
\]</span> 的每一个约束条件，我们引入一个拉格朗日乘子 Lagrange multiplier <span class="math inline">\(\alpha_{i} \geqslant 0\)</span>, <span class="math inline">\(i=1,2, \cdots, N\)</span>，定义拉格朗日函数 Generalized Lagrange Function，<span class="math inline">\(\alpha=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}\)</span> 为拉格朗日乘子向量。 <span class="math display">\[
\begin{align}
L(w, b, \alpha)&amp;=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i}(y_{i}\left(w \cdot x_{i}+b\right)-1)\nonumber
\\&amp;=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}
\end{align}
\]</span> 带拘束最优化问题(10)可以变成无拘束优化问题(14) <span class="math display">\[
\begin{array}{l}
\min _{w, b} \max _{\lambda} \mathcal{L}(w, b, \alpha) \\
\text { s.t. } \lambda_{i} \geqslant 0
\end{array}
\]</span></p>
<p>由强对偶关系，拘束问题(14)可以化为无拘束优化问题 <span class="math display">\[
\begin{array}{l}
\max _{\lambda} \min _{w, b} \mathcal{L}(w, b, \alpha) \\
\text { s.t. } \lambda_{i} \geqslant 0
\end{array}
\]</span></p>
<p>由上面的拉格朗日（强）对偶性可得，原始问题(10)的对偶问题是极大极小问题(15)</p>
<ul>
<li>详细推导见：【拉格朗日对偶，等价对偶以及KKT条件】（文件没保存，有缘再续）</li>
</ul>
<h4 id="对偶问题的计算">1.4.2 对偶问题的计算</h4>
<p><strong>1.4.2.1: 求<span class="math inline">\(\min _{w, b} L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}\)</span></strong></p>
<p>计算<span class="math inline">\(\min _{w, b} L(w, b, \alpha)\)</span>对变量<span class="math inline">\(w,\;b\)</span>的偏导为0 <span class="math display">\[
\begin{array}{l}
\nabla_{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0 \\
\nabla_{b} L(w, b, \alpha)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0\nonumber\\
\end{array}
\]</span> 得 <span class="math display">\[
\begin{array}{l}
w&amp;=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}\\
\sum_{i=1}^{N} \alpha_{i} y_{i}&amp;=0
\end{array}
\]</span> 把(16)带入到(13)，得 <span class="math display">\[
\begin{aligned}
L(w, b, \alpha) &amp;=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\
&amp;=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{aligned}
\]</span> 得到结果 <span class="math display">\[
\min _{w, b} L(w, b, \alpha)=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\]</span> <strong>求<span class="math inline">\(\min _{w, b} L(w, b, \alpha) \text { 对 } \alpha \text { 的极大 }\)</span></strong> <span class="math display">\[
\begin{aligned}
\max _{\alpha} &amp; -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{aligned}
\]</span> 调整符号，得与原始问题等价的对偶最优化问题 <span class="math display">\[
\begin{array}{1}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]</span></p>
<ul>
<li>$ <em>{i=1}^{N} </em>{i} y_{i}=0<span class="math inline">\(意味着很大部分的\)</span>$，即决定超平面的只是很小一部分的支持向量。</li>
</ul>
<h4 id="对偶问题的解">1.4.3 对偶问题的解</h4>
<blockquote>
<p>定理 7.2 设 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{l}^{*}\right)^{\mathrm{T}}\)</span> 是对偶最优化问题 (7.22)<span class="math inline">\(\sim(7.24)\)</span> 的解，则存在下标 <span class="math inline">\(j,\)</span> 使得 <span class="math inline">\(\alpha_{j}^{*}&gt;0,\)</span> 并可按下式求得原始最优化问题 (7.13)<span class="math inline">\(\sim(7.14)\)</span> 的解 <span class="math inline">\(w^{*}, b^{*}\)</span> : <span class="math display">\[
\begin{array}{c}
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)
\end{array}
\]</span></p>
</blockquote>
<p>分离超平面为<span class="math inline">\(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x \cdot x_{i}\right)+b^{*}=0\)</span></p>
<p>分类决策函数为<span class="math inline">\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x \cdot x_{i}\right)+b^{*}\right)\)</span></p>
<h4 id="algorithm-1">1.4.4 Algorithm</h4>
<blockquote>
<p>算法 7.2 (线性可分支持向量机学习算法) 输入: 线性可分训练集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中 <span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}\)</span>, <span class="math inline">\(y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N\)</span> 输出：分离超平面和分类决策函数。 （1）构造并求解约束最优化问题 <span class="math display">\[
\begin{array}{ll}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]</span> 求得最优解 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}\)</span> 。 (2) 计算 <span class="math display">\[
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}
\]</span> 并选择 <span class="math inline">\(\alpha^{*}\)</span> 的一个正分量 <span class="math inline">\(\alpha_{j}^{*}&gt;0,\)</span> 计算 <span class="math display">\[
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)
\]</span> （3）求得分离超平面 <span class="math display">\[
w^{*} \cdot x+b^{*}=0
\]</span> 分类决策函数: <span class="math display">\[
f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\]</span></p>
</blockquote>
<h2 id="线性支持向量机与软间隔最大化">2. 线性支持向量机与软间隔最大化</h2>
<p>对线性不可分数据集，存在一些特异点（outlier）使得不等式不能全部满足，即函数间隔大于1。</p>
<p>因此我们引入松弛变量<span class="math inline">\(\xi_i \geq0\)</span>，使得函数间隔大于等于<span class="math inline">\(1-\xi_i\)</span>。同时对松弛变量付出一个惩罚参数。</p>
<ul>
<li><p>约束条件 <span class="math display">\[
y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}
\]</span></p></li>
<li><p>目标函数 <span class="math display">\[
\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}
\]</span></p>
<ul>
<li><span class="math inline">\(C\)</span>为惩罚参数。<span class="math inline">\(C\)</span>越大，对误分类的惩罚越大</li>
<li>最小化目标函数，即使<span class="math inline">\(\frac{1}{2}\|w\|^{2}\)</span>尽可能小，间隔尽可能大</li>
</ul></li>
</ul>
<h3 id="原始问题">2.1 原始问题</h3>
<p><span class="math display">\[
\begin{array}{ll}
\min _{w, b, \xi} &amp; \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]</span></p>
<ul>
<li>凸二次规划问题</li>
<li><span class="math inline">\(w\)</span>的解唯一</li>
<li><span class="math inline">\(b\)</span>的解可能不唯一，而是一个区间</li>
</ul>
<h3 id="拉格朗日函数">2.2 拉格朗日函数</h3>
<p><span class="math display">\[
L(w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}\\
\alpha_i\geq 0,\;\mu_i\geq0
\]</span></p>
<h3 id="对偶问题">2.3 对偶问题</h3>
<p>原始问题(27)的对偶问题使拉格朗日函数的极大极小问题。 <span class="math display">\[
\begin{array}{ll}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
\]</span></p>
<h3 id="解的等价性">2.4 解的等价性</h3>
<blockquote>
<p>定理 7.3 设 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}\)</span> 是对偶问题的一个解，若存在 <span class="math inline">\(\alpha^{*}\)</span> 的一个分量 <span class="math inline">\(\alpha_{j}^{*}, 0&lt;\alpha_{j}^{*}&lt;C,\)</span> 则原始问题的解 <span class="math inline">\(w^{*}, b^{*}\)</span> 可按下式 求得: <span class="math display">\[
\begin{array}{c}
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)
\end{array}
\]</span></p>
</blockquote>
<blockquote>
<p>算法 7.3 (线性支持向量机学习算法) 输入: 训练数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中， <span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}\)</span>, <span class="math inline">\(y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N\)</span> 输出：分离超平面和分类决策函数。 （1）选择惩罚参数 <span class="math inline">\(C&gt;0,\)</span> 构造并求解凸二次规划问题 <span class="math display">\[
\begin{array}{ll}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
\]</span> 求得最优解 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}\)</span> （2） 计算 <span class="math inline">\(w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}\)</span> 选择 <span class="math inline">\(\alpha^{*}\)</span> 的一个分量 <span class="math inline">\(\alpha_{j}^{*}\)</span> 适合条件 <span class="math inline">\(0&lt;\alpha_{j}^{*}&lt;C\)</span> ，计算 <span class="math display">\[
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)
\]</span> （3）求得分离超平面 <span class="math display">\[
w^{*} \cdot x+b^{*}=0
\]</span> 分类决策函数: <span class="math display">\[
f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\]</span></p>
</blockquote>
<ul>
<li>(32)中，每一个合适的<span class="math inline">\(\alpha\)</span>都会得出一个<span class="math inline">\(b\)</span>，所以<span class="math inline">\(b\)</span>不唯一</li>
</ul>
<h3 id="支持向量">2.4 支持向量</h3>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E8%BD%AF%E9%97%B4%E9%9A%94%E5%88%86%E7%A6%BB%E5%90%91%E9%87%8F.png" alt="image-20201216151045483" style="zoom:50%;" /></p>
<p>虚线是间隔边界，实线是分离超平面。对每一个实例点<span class="math inline">\((x_i,y_i)\)</span>和他们的<span class="math inline">\(\alpha _i^*,\;\xi_i\)</span>，我们有</p>
<ul>
<li><span class="math inline">\(\alpha_{i}^{*}&lt;C\)</span>，则 <span class="math inline">\(\xi_{i}=0\)</span></li>
<li><span class="math inline">\(\alpha_{i}^{*}=C, 0&lt;\xi_{i}&lt;1\)</span>，则</li>
<li><span class="math inline">\(\alpha_{i}^{*}=C, \xi_{i}=1\)</span>，则</li>
<li><span class="math inline">\(\alpha_{i}^{*}=C, \xi_{i}&gt;1\)</span>，则</li>
<li>没有<span class="math inline">\(\alpha_{i}^{*}=C, 0&lt;\xi_{i}&lt;1\)</span></li>
</ul>
<h3 id="合页损失函数-hinge-loss-function">2.5 合页损失函数 Hinge Loss Function</h3>
<blockquote>
<p>定理 7.4 线性支持向量机原始最优化问题: <span class="math display">\[
\begin{array}{ll}
\min _{w, b, \xi} &amp; \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
\]</span> 等价于最优化问题 <span class="math display">\[
\min _{w, b} \sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
\]</span></p>
</blockquote>
<ul>
<li><span class="math inline">\([z]_{+}=\left\{\begin{array}{ll}z, &amp; z&gt;0 \\ 0, &amp; z \leqslant 0\end{array}\right.\)</span>表示取正值的函数。</li>
<li><span class="math inline">\(L(y(w \cdot x+b))=[1-y(w \cdot x+b)]_{+}\)</span> 经验损失。但实例点被正确分类且函数间隔大于1，损失才是0。位于间隔边界上的实例点损失不为0。</li>
<li><span class="math inline">\(\lambda\|w\|^{2}\)</span> 正则化项</li>
</ul>
<h4 id="损失函数对比">2.5.1 损失函数对比</h4>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt="image-20201216155233841" style="zoom:50%;" /></p>
<p>虚线为感知机的损失函数<span class="math inline">\(\left[-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}\)</span></p>
<ul>
<li>0-1损失函数为二元分类真正的损失函数。<strong>不过因其不连续可导，无法直接优化它所构成的函数</strong>。</li>
<li>合页损失函数是0-1损失函数的上界。<strong>线性支持向量机是优化由0-1损失函数的上界构成的目标函数</strong>。</li>
<li>合页损失函数对学习有更高要求。不但需要分类正确，还要确信度（距离）足够高才为0损失。</li>
</ul>
<h2 id="非线性支持向量机与核函数">3. 非线性支持向量机与核函数</h2>
<p>对于一个非线性数据集，首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。</p>
<p>由于特征空间是高维甚至无限维，加上对偶问题中多组高维向量计算内积。因此引入核函数减少计算量。</p>
<h3 id="核技巧-kernel-trick">3.1 核技巧 Kernel Trick</h3>
<p>核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间 (欧氏空间 <span class="math inline">\(\mathbf{R}^{n}\)</span> 或离散集合) 对应于一个特征空间 (希尔伯特空间 <span class="math inline">\(\mathcal{H}\)</span> )，使得在输入空间 <span class="math inline">\(\mathbf{R}^{n}\)</span> 中的超曲面模型对应于特征空间 <span class="math inline">\(\mathcal{H}\)</span> 中的超平面模型支持向量机）。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。</p>
<blockquote>
<p>定义 7.6 (核函数) <span class="math inline">\(\quad\)</span> 设 <span class="math inline">\(\mathcal{X}\)</span> 是输入空间 ( 欧氏空间 <span class="math inline">\(\mathbf{R}^{n}\)</span> 的子集或离散集合 <span class="math inline">\(),\)</span> 又设 <span class="math inline">\(\mathcal{H}\)</span> 为特征空间 <span class="math inline">\((\)</span> 希 尔伯特空间 <span class="math inline">\(),\)</span> 如果存在一个从 <span class="math inline">\(\mathcal{X}\)</span> 到 <span class="math inline">\(\mathcal{H}\)</span> 的映射 <span class="math display">\[
\phi(x): \mathcal{X} \rightarrow \mathcal{H}
\]</span> 使得对所有 <span class="math inline">\(x, z \in \mathcal{X},\)</span> 函数 <span class="math inline">\(K(x, z)\)</span> 满足条件 <span class="math display">\[
K(x, z)=\phi(x) \cdot \phi(z)
\]</span> 则称 <strong><span class="math inline">\(K(x, z)\)</span> 为核函数， <span class="math inline">\(\phi(x)\)</span> 为映射函数</strong>，式中 <span class="math inline">\(\phi(x) \cdot \phi(z)\)</span> 为 <span class="math inline">\(\phi(x)\)</span> 和 <span class="math inline">\(\phi(z)\)</span> 的内积。</p>
</blockquote>
<ul>
<li><strong>在使用过程中只定义核函数<span class="math inline">\(K(x,z)\)</span>，不显式定义映射函数<span class="math inline">\(\phi\)</span></strong></li>
<li>特征空间<span class="math inline">\(\mathcal{H}\)</span>一般是高维甚至无限维</li>
<li>给定核<span class="math inline">\(K(x,z)\)</span>，特征空间和映射函数不唯一</li>
</ul>
<p>在支持向量机中，通过映射函数<span class="math inline">\(\phi\)</span>把输入空间变换到新的特征空间，在新的特征空间中训练线性支持向量机。此时对偶问题的目标函数成为<span class="math inline">\(W(\alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}\)</span>，分类决策函数成为<span class="math inline">\(f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \phi\left(x_{i}\right) \cdot \phi(x)+b^{*}\right)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} K\left(x_{i}, x\right)+b^{*}\right)\)</span></p>
<h3 id="正定核">3.2 正定核</h3>
<blockquote>
<p>定理 7.5 (正定核的充要条件) <span class="math inline">\(\quad\)</span> 设 <span class="math inline">\(K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbf{R}\)</span> 是对称函数，则 <span class="math inline">\(K(x, z)\)</span> 为正 定核函数的充要条件是对任意 <span class="math inline">\(x_{i} \in \mathcal{X}, i=1,2, \cdots, m, K(x, z)\)</span> 对应的 Gram 矩阵: <span class="math display">\[
K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}
\]</span> 是半正定矩阵。</p>
</blockquote>
<blockquote>
<p>定义 7.7 (正定核的等价定义) <span class="math inline">\(\quad\)</span> 设 <span class="math inline">\(\mathcal{X} \subset \mathbf{R}^{n}, K(x, z)\)</span> 是定义在 <span class="math inline">\(\mathcal{X} \times \mathcal{X}\)</span> 上的对称 函数, 如果对任意 <span class="math inline">\(x_{i} \in \mathcal{X}, i=1,2, \cdots, m, K(x, z)\)</span> 对应的 Gram 矩阵 <span class="math display">\[
K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}
\]</span></p>
</blockquote>
<h3 id="常用核函数">3.3 常用核函数</h3>
<h4 id="多项式核函数-polynomial-kernel-function">3.3.1 多项式核函数 Polynomial Kernel Function</h4>
<p><span class="math display">\[
K(x, z)=(x \cdot z+1)^{p}
\]</span></p>
<p>对应的支持向量机是一个 <span class="math inline">\(p\)</span> 次多项式分类器。在此情形下，分类决策函数成为 <span class="math display">\[
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i}\left(x_{i} \cdot x+1\right)^{p}+b^{*}\right)
\]</span></p>
<h4 id="高斯核函数-gaussian-kernel-function">3.3.2 高斯核函数 Gaussian Kernel Function</h4>
<p><span class="math display">\[
K(x, z)=\exp \left(-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)
\]</span></p>
<p>对应的支持向量机是高斯径向基函数（radial basis function）分类器。在此情形下，分类决策函数成为 <span class="math display">\[
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \exp \left(-\frac{\left\|x-x_{i}\right\|^{2}}{2 \sigma^{2}}\right)+b^{*}\right)
\]</span></p>
<h4 id="字符串核函数-string-kernel-function">3.3.3 字符串核函数 String Kernel Function</h4>
<p>定义在离散数据集合上。</p>
<h3 id="非线性支持向量机">3.4 非线性支持向量机</h3>
<blockquote>
<p>定义 7.8 (非线性支持向量机 <span class="math inline">\() \quad\)</span> 从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划(46)学习得到的分类决策函数 <span class="math display">\[
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x, x_{i}\right)+b^{*}\right)
\]</span> 称为非线性支持向量机, <span class="math inline">\(K(x, z)\)</span> 是正定核涵数。</p>
</blockquote>
<blockquote>
<p>算法 7.4 (非线性支持向量机学习算法) 输入: 训练数据集 <span class="math inline">\(T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\},\)</span> 其中 <span class="math inline">\(x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in\)</span><span class="math inline">\(\mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N\)</span> 输出：分类决策函数。 （1）选取适当的核函数 <span class="math inline">\(K(x, z)\)</span> 和适当的参数 <span class="math inline">\(C,\)</span> 构造并求解最优化问题 <span class="math display">\[
\begin{array}{1}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
\]</span> 求得最优解 <span class="math inline">\(\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}\)</span> 。 （2）选择 <span class="math inline">\(\alpha^{*}\)</span> 的一个正分量 <span class="math inline">\(0&lt;\alpha_{j}^{*}&lt;C,\)</span> 计算 <span class="math display">\[
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x_{i}, x_{j}\right)
\]</span> （3）构造决策函数: <span class="math display">\[
f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x, x_{i}\right)+b^{*}\right)
\]</span></p>
</blockquote>
<ul>
<li><span class="math inline">\(K(x,z)\)</span>为正定核函数是，问题(46)是二次规划问题，解存在。</li>
</ul>
<h2 id="smo-algorithm-sequential-minimal-optimization">4. SMO Algorithm (sequential minimal optimization)</h2>
<p>序列最小算法用来求解凸二次规划的对偶问题 <span class="math display">\[
\begin{array}{ll}
\min _{\alpha} &amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
\]</span></p>
<h2 id="reference">5. Reference</h2>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/12/02/%5BSLM-5%5D%20%E5%86%B3%E7%AD%96%E6%A0%91%20Decision%20Tree/" rel="prev" title="">
                  <i class="fa fa-chevron-left"></i> 
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/12/20/%5BSLM-6%5D%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%20LS&ME/" rel="next" title="">
                   <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CHongfeng Ling 凌崇锋</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  







  





  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
