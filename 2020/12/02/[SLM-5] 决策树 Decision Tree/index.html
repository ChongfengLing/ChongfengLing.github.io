<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","version":"8.1.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="决策树 Decision Tree 决策树是一种基本的分类与回归方法，这里主要讨论分类问题。**他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。**学习过程主要包括3个步骤：特征选择、决策树的生成、决策树的修建，并根据损失函数最小化的原则建立决策树模型。 1. Model  定义 5.1 (决策树)  分类决策树模型是一种描述对实例进行分类的树形结构。决策">
<meta property="og:type" content="article">
<meta property="og:title" content="Chongfeng Ling">
<meta property="og:url" content="http://example.com/2020/12/02/[SLM-5]%20%E5%86%B3%E7%AD%96%E6%A0%91%20Decision%20Tree/index.html">
<meta property="og:site_name" content="Chongfeng Ling">
<meta property="og:description" content="决策树 Decision Tree 决策树是一种基本的分类与回归方法，这里主要讨论分类问题。**他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。**学习过程主要包括3个步骤：特征选择、决策树的生成、决策树的修建，并根据损失函数最小化的原则建立决策树模型。 1. Model  定义 5.1 (决策树)  分类决策树模型是一种描述对实例进行分类的树形结构。决策">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://math.now.sh?inline=%5Cquad">
<meta property="og:image" content="https://math.now.sh?inline=P%28Y%7CX%29%2C%5C%3BX%3A%E7%89%B9%E5%BE%81%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%5C%3B%5C%3B%5C%3B%5C%3BY%3A%E7%B1%BB%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F">
<meta property="og:image" content="https://math.now.sh?inline=P%28Y%7CX%29">
<meta property="og:image" content="https://math.now.sh?inline=y">
<meta property="og:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLm-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87.png">
<meta property="og:image" content="https://math.now.sh?inline=X">
<meta property="og:image" content="https://math.now.sh?from=P%5Cleft%28X%3Dx_%7Bi%7D%5Cright%29%3Dp_%7Bi%7D%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%0A">
<meta property="og:image" content="https://math.now.sh?inline=X">
<meta property="og:image" content="https://math.now.sh?from=H%28X%29%3DH(p)%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p_%7Bi%7D%20%5Clog%20p_%7Bi%7D%0A">
<meta property="og:image" content="https://math.now.sh?from=%E5%AF%B9p_i%3D0%EF%BC%8C%E5%AE%9A%E4%B9%890log0%3D0%5Cnonumber%0A">
<meta property="og:image" content="https://math.now.sh?inline=e">
<meta property="og:image" content="https://math.now.sh?inline=X">
<meta property="og:image" content="https://math.now.sh?inline=x_i">
<meta property="og:image" content="https://math.now.sh?inline=0%20%5Cleqslant%20H%28p%29%20%5Cleqslant%20%5Clog%20n">
<meta property="og:image" content="https://math.now.sh?from=P%5Cleft%28X%3Dx_%7Bi%7D%2C%20Y%3Dy_%7Bj%7D%5Cright%29%3Dp_%7Bi%20j%7D%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%20%3B%20%5Cquad%20j%3D1%2C2%2C%20%5Ccdots%2C%20m%5Cnonumber%0A">
<meta property="og:image" content="https://math.now.sh?inline=H%28Y%7CX%29">
<meta property="og:image" content="https://math.now.sh?inline=X">
<meta property="og:image" content="https://math.now.sh?inline=Y">
<meta property="og:image" content="https://math.now.sh?inline=X">
<meta property="og:image" content="https://math.now.sh?inline=Y">
<meta property="og:image" content="https://math.now.sh?inline=X">
<meta property="og:image" content="https://math.now.sh?from=H%28Y%20%5Cmid%20X%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p_%7Bi%7D%20H%5Cleft(Y%20%5Cmid%20X%3Dx_%7Bi%7D%5Cright)%5C%5C%0Ap_%7Bi%7D%3DP%5Cleft(X%3Dx_%7Bi%7D%5Cright)%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%0A">
<meta property="og:image" content="https://math.now.sh?inline=X">
<meta property="og:image" content="https://math.now.sh?inline=Y">
<meta property="og:image" content="https://math.now.sh?inline=%5Cquad">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=g%28D%2C%20A%29%2C">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=H%28D%29">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=H%28D%20%5Cmid%20A%29">
<meta property="og:image" content="https://math.now.sh?from=g%28D%2C%20A%29%3DH(D)-H(D%20%5Cmid%20A)%5Cnonumber%0A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=g_%7BR%7D%28D%2C%20A%29">
<meta property="og:image" content="https://math.now.sh?inline=g%28D%2C%20A%29">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=H_%7BA%7D%28D%29">
<meta property="og:image" content="https://math.now.sh?from=g_%7BR%7D%28D%2C%20A%29%3D%5Cfrac%7Bg(D%2C%20A)%7D%7BH_%7BA%7D(D)%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=%2C%20H_%7BA%7D%28D%29%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%2C%20n">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=k">
<meta property="og:image" content="https://math.now.sh?inline=C_k">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=n">
<meta property="og:image" content="https://math.now.sh?inline=D_n">
<meta property="og:image" content="https://math.now.sh?inline=D_%7Bi%20k%7D%3DD_%7Bi%7D%20%5Ccap%20C_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?inline=%7CD%7C">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=g%28D%2C%20A%29">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=H%28D%29">
<meta property="og:image" content="https://math.now.sh?from=H%28D%29%3D-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20%5Cfrac%7B%5Cleft%7CC_%7Bk%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CC_%7Bk%7D%5Cright%7C%7D%7B%7CD%7C%7D%5Cnonumber%0A">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=H%28D%20%5Cmid%20A%29">
<meta property="og:image" content="https://math.now.sh?from=H%28D%20%5Cmid%20A%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%20H%5Cleft(D_%7Bi%7D%5Cright)%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%20k%7D%5Cright%7C%7D%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%20k%7D%5Cright%7C%7D%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%5Cnonumber%0A">
<meta property="og:image" content="https://math.now.sh?from=g%28D%2C%20A%29%3DH(D)-H(D%20%5Cmid%20A)%5Cnonumber%0A">
<meta property="og:image" content="https://math.now.sh?inline=D%2C">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=%5Cvarepsilon%20%3B">
<meta property="og:image" content="https://math.now.sh?inline=%5C%5C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=C_%7Bk%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=C_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=A%3D%5Cvarnothing%2C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=C_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=A_%7Bg%7D">
<meta property="og:image" content="https://math.now.sh?inline=A_%7Bg%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Cvarepsilon%2C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=C_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=A_%7Bg%7D">
<meta property="og:image" content="https://math.now.sh?inline=a_%7Bi%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=A_%7Bg%7D%3Da_%7Bi%7D">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=D_%7Bi%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=D_%7Bi%7D">
<meta property="og:image" content="https://math.now.sh?inline=T%2C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=i">
<meta property="og:image" content="https://math.now.sh?inline=D_%7Bi%7D">
<meta property="og:image" content="https://math.now.sh?inline=A-%5Cleft%5C%7BA_%7Bg%7D%5Cright%5C%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Csim">
<meta property="og:image" content="https://math.now.sh?inline=%285%29%2C">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bi%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bi%7D%20%5Ccirc">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=%5Cvarepsilon">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=C_%7Bk%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=C_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=A%3D%5Cvarnothing%2C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=C_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=A_%7Bg%7D">
<meta property="og:image" content="https://math.now.sh?inline=A_%7Bg%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Cvarepsilon%2C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=C_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=A_%7Bg%7D">
<meta property="og:image" content="https://math.now.sh?inline=a_%7Bi%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=A_%7Bg%7D%3Da_%7Bi%7D">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=D_%7Bi%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=D_%7Bi%7D">
<meta property="og:image" content="https://math.now.sh?inline=T%2C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=i">
<meta property="og:image" content="https://math.now.sh?inline=D_%7Bi%7D">
<meta property="og:image" content="https://math.now.sh?inline=A-%5Cleft%5C%7BA_%7Bg%7D%5Cright%5C%7D">
<meta property="og:image" content="https://math.now.sh?inline=%285%29%2C">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bi%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bi%7D">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=%7CT%7C">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=N_t">
<meta property="og:image" content="https://math.now.sh?inline=K">
<meta property="og:image" content="https://math.now.sh?inline=N_%7Btk%7D%EF%BC%9B%5C%3Bk%3D1%2C2%2C...%2CK">
<meta property="og:image" content="https://math.now.sh?inline=H_t%28T%29">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha%20%5Cgeq0">
<meta property="og:image" content="https://math.now.sh?from=C_%7B%5Calpha%7D%28T%29%3D%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20N_%7Bt%7D%20H_%7Bt%7D(T)%2B%5Calpha%7CT%7C%0A">
<meta property="og:image" content="https://math.now.sh?inline=H_t%28T%29">
<meta property="og:image" content="https://math.now.sh?from=H_%7Bt%7D%28T%29%3D-%5Csum_%7Bk%7D%20%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%20%5Clog%20%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=C%28T%29">
<meta property="og:image" content="https://math.now.sh?from=C%28T%29%3D%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20N_%7Bt%7D%20H_%7Bt%7D(T)%3D-%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20N_%7Bt%20k%7D%20%5Clog%20%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%0A">
<meta property="og:image" content="https://math.now.sh?from=C_%7B%5Calpha%7D%28T%29%3DC(T)%2B%5Calpha%7CT%7C%0A">
<meta property="og:image" content="https://math.now.sh?inline=C%28T%29">
<meta property="og:image" content="https://math.now.sh?inline=%7CT%7C">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B%5Calpha%7D%20%5Ccirc">
<meta property="og:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D.png">
<meta property="og:image" content="https://math.now.sh?inline=T_%7BB%7D">
<meta property="og:image" content="https://math.now.sh?inline=T_%7BA%7D">
<meta property="og:image" content="https://math.now.sh?inline=C_%7B%5Calpha%7D%5Cleft%28T_%7BB%7D%5Cright%29">
<meta property="og:image" content="https://math.now.sh?inline=C_%7B%5Calpha%7D%5Cleft%28T_%7BA%7D%5Cright%29%2C">
<meta property="og:image" content="https://math.now.sh?from=C_%7B%5Calpha%7D%5Cleft%28T_%7BA%7D%5Cright%29%20%5Cleqslant%20C_%7B%5Calpha%7D%5Cleft(T_%7BB%7D%5Cright)%0A">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B%5Calpha%5E%7B%5Ccirc%7D%7D">
<meta property="og:image" content="https://math.now.sh?inline=X">
<meta property="og:image" content="https://math.now.sh?inline=Y">
<meta property="og:image" content="https://math.now.sh?inline=D%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D">
<meta property="og:image" content="https://math.now.sh?inline=Y">
<meta property="og:image" content="https://math.now.sh?inline=M">
<meta property="og:image" content="https://math.now.sh?inline=R_%7B1%7D%2C%20R_%7B2%7D%2C%20%5Ccdots%2C%20R_%7BM%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=R_%7Bm%7D">
<meta property="og:image" content="https://math.now.sh?inline=c_%7Bm%7D%2C">
<meta property="og:image" content="https://math.now.sh?from=f%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20c_%7Bm%7D%20I%5Cleft(x%20%5Cin%20R_%7Bm%7D%5Cright)%0A">
<meta property="og:image" content="https://math.now.sh?inline=x">
<meta property="og:image" content="https://math.now.sh?inline=c_m">
<meta property="og:image" content="https://math.now.sh?inline=%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7Bm%7D%7D%5Cleft%28y_%7Bi%7D-f%5Cleft(x_%7Bi%7D%5Cright%29%5Cright)%5E%7B2%7D">
<meta property="og:image" content="https://math.now.sh?inline=x_i%5Cin%20R_m">
<meta property="og:image" content="https://math.now.sh?inline=R_%7Bm%7D">
<meta property="og:image" content="https://math.now.sh?inline=c_%7Bm%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Chat%7Bc%7D_%7Bm%7D">
<meta property="og:image" content="https://math.now.sh?inline=R_%7Bm%7D">
<meta property="og:image" content="https://math.now.sh?inline=x_%7Bi%7D">
<meta property="og:image" content="https://math.now.sh?inline=y_%7Bi%7D">
<meta property="og:image" content="https://math.now.sh?from=%5Chat%7Bc%7D_%7Bm%7D%3D%5Coperatorname%7Bave%7D%5Cleft%28y_%7Bi%7D%20%5Cmid%20x_%7Bi%7D%20%5Cin%20R_%7Bm%7D%5Cright%29%5Cnonumber%0A">
<meta property="og:image" content="https://math.now.sh?inline=j">
<meta property="og:image" content="https://math.now.sh?inline=x%5E%7B%28j%29%7D">
<meta property="og:image" content="https://math.now.sh?inline=s">
<meta property="og:image" content="https://math.now.sh?from=R_%7B1%7D%28j%2C%20s%29%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%20%5Cleqslant%20s%5Cright%5C%7D%20%5Cquad%20%5Ctext%20%7B%20%E5%92%8C%20%7D%20%5Cquad%20R_%7B2%7D(j%2C%20s)%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%3Es%5Cright%5C%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=j">
<meta property="og:image" content="https://math.now.sh?inline=s">
<meta property="og:image" content="https://math.now.sh?from=%5Cmin%20_%7Bj%2C%20s%7D%5Cleft%5B%5Cmin%20_%7Bc_%7B1%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B1%7D%28j%2C%20s%29%7D%5Cleft(y_%7Bi%7D-c_%7B1%7D%5Cright)%5E%7B2%7D%2B%5Cmin%20_%7Bc_%7B2%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%7D%5Cleft(y_%7Bi%7D-c_%7B2%7D%5Cright)%5E%7B2%7D%5Cright%5D%0A">
<meta property="og:image" content="https://math.now.sh?inline=x%5E%7B%28j%29%7D">
<meta property="og:image" content="https://math.now.sh?inline=j">
<meta property="og:image" content="https://math.now.sh?inline=s">
<meta property="og:image" content="https://math.now.sh?inline=x_n">
<meta property="og:image" content="https://math.now.sh?inline=n">
<meta property="og:image" content="https://math.now.sh?inline=c_1%2Cc_2">
<meta property="og:image" content="https://math.now.sh?inline=c_m">
<meta property="og:image" content="https://math.now.sh?from=%5Chat%7Bc%7D_%7B1%7D%3D%5Coperatorname%7Bave%7D%5Cleft%28y_%7Bi%7D%20%5Cmid%20x_%7Bi%7D%20%5Cin%20R_%7B1%7D(j%2C%20s%29%5Cright)%20%5Cquad%20%5Ctext%20%7B%20%E5%92%8C%20%7D%20%5Cquad%20%5Chat%7Bc%7D_%7B2%7D%3D%5Coperatorname%7Bave%7D%5Cleft(y_%7Bi%7D%20%5Cmid%20x_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%5Cright)%0A">
<meta property="og:image" content="https://math.now.sh?inline=j">
<meta property="og:image" content="https://math.now.sh?inline=%28j%2Cs%29">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=f%28x%29">
<meta property="og:image" content="https://math.now.sh?inline=j">
<meta property="og:image" content="https://math.now.sh?inline=s">
<meta property="og:image" content="https://math.now.sh?from=%5Cmin%20_%7Bj%2C%20s%7D%5Cleft%5B%5Cmin%20_%7Bc_%7B1%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B1%7D%28j%2C%20s%29%7D%5Cleft(y_%7Bi%7D-c_%7B1%7D%5Cright)%5E%7B2%7D%2B%5Cmin%20_%7Bc_%7B2%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%7D%5Cleft(y_%7Bi%7D-c_%7B2%7D%5Cright)%5E%7B2%7D%5Cright%5D%0A">
<meta property="og:image" content="https://math.now.sh?inline=j">
<meta property="og:image" content="https://math.now.sh?inline=j">
<meta property="og:image" content="https://math.now.sh?inline=s">
<meta property="og:image" content="https://math.now.sh?inline=%28j%2C%20s%29">
<meta property="og:image" content="https://math.now.sh?inline=%28j%2C%20s%29">
<meta property="og:image" content="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bc%7D%0AR_%7B1%7D%28j%2C%20s%29%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%20%5Cleqslant%20s%5Cright%5C%7D%2C%20%5Cquad%20R_%7B2%7D(j%2C%20s)%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%3Es%5Cright%5C%7D%20%5C%5C%0A%5Chat%7Bc%7D_%7Bm%7D%3D%5Cfrac%7B1%7D%7BN_%7Bm%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7Bm%7D(j%2C%20s)%7D%20y_%7Bi%7D%2C%20%5Cquad%20x%20%5Cin%20R_%7Bm%7D%2C%20%5Cquad%20m%3D1%2C2%0A%5Cend%7Barray%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=%281%29%2C(2)%2C">
<meta property="og:image" content="https://math.now.sh?inline=M">
<meta property="og:image" content="https://math.now.sh?inline=R_%7B1%7D%2C%20R_%7B2%7D%2C%20%5Ccdots%2C%20R_%7BM%7D%2C">
<meta property="og:image" content="https://math.now.sh?from=f%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20%5Chat%7Bc%7D_%7Bm%7D%20I%5Cleft(x%20%5Cin%20R_%7Bm%7D%5Cright)%0A">
<meta property="og:image" content="https://math.now.sh?inline=x%5E1">
<meta property="og:image" content="https://math.now.sh?inline=x%5E2">
<meta property="og:image" content="https://math.now.sh?inline=x%5E3">
<meta property="og:image" content="https://math.now.sh?inline=y">
<meta property="og:image" content="https://math.now.sh?inline=x_1">
<meta property="og:image" content="https://math.now.sh?inline=x_2">
<meta property="og:image" content="https://math.now.sh?inline=x_3">
<meta property="og:image" content="https://math.now.sh?inline=x_4">
<meta property="og:image" content="https://math.now.sh?inline=j%3Dx%5E1">
<meta property="og:image" content="https://math.now.sh?inline=s%3D1.5">
<meta property="og:image" content="https://math.now.sh?inline=c_1%3D1%2C%5C%3Bc_2%3D2.5">
<meta property="og:image" content="https://math.now.sh?inline=%5Cmin%20_%7Bc_%7B1%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B1%7D%28j%2C%20s%29%7D%5Cleft(y_%7Bi%7D-c_%7B1%7D%5Cright)%5E%7B2%7D%2B%5Cmin%20_%7Bc_%7B2%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%7D%5Cleft(y_%7Bi%7D-c_%7B2%7D%5Cright)%5E%7B2%7D%3D%5C%5C%20%5B(1-1)%5E2%2B(1-1)%5E2%5D%2B%5B(2-2.5)%5E2%2B(3-2.5)%5E2%5D%3D0.5">
<meta property="og:image" content="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E1%2C1.6)%2C%5C%3B0.67">
<meta property="og:image" content="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E1%2C1.2)%2C%5C%3B(j%2Cs)%3D(x%5E1%2C1.8)">
<meta property="og:image" content="https://math.now.sh?inline=j%3Dx%5E1">
<meta property="og:image" content="https://math.now.sh?inline=s%3D1.5">
<meta property="og:image" content="https://math.now.sh?inline=error%3D0.5">
<meta property="og:image" content="https://math.now.sh?inline=j%3Dx%5E2">
<meta property="og:image" content="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E2%2C4)%2C%5C%3Berror%3D0.5">
<meta property="og:image" content="https://math.now.sh?inline=j%3Dx%5E3">
<meta property="og:image" content="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E3%2C2.75)%2C%5C%3Berror%3D2">
<meta property="og:image" content="https://math.now.sh?inline=%5Cmin%20_%7Bj%2C%20s%7D%3D%5Cmin%5B0.5%2C%5C%3B0.5%2C%5C%3B2%5D%3D0.5">
<meta property="og:image" content="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E1%2C1.5)">
<meta property="og:image" content="https://math.now.sh?inline=R_1%2C%5C%3BR_2">
<meta property="og:image" content="https://math.now.sh?inline=R_1">
<meta property="og:image" content="https://math.now.sh?inline=x%5E1">
<meta property="og:image" content="https://math.now.sh?inline=x%5E2">
<meta property="og:image" content="https://math.now.sh?inline=x%5E3">
<meta property="og:image" content="https://math.now.sh?inline=y">
<meta property="og:image" content="https://math.now.sh?inline=x_1">
<meta property="og:image" content="https://math.now.sh?inline=x_2">
<meta property="og:image" content="https://math.now.sh?inline=R_2">
<meta property="og:image" content="https://math.now.sh?inline=x%5E1">
<meta property="og:image" content="https://math.now.sh?inline=x%5E2">
<meta property="og:image" content="https://math.now.sh?inline=x%5E3">
<meta property="og:image" content="https://math.now.sh?inline=y">
<meta property="og:image" content="https://math.now.sh?inline=x_3">
<meta property="og:image" content="https://math.now.sh?inline=x_4">
<meta property="og:image" content="https://math.now.sh?inline=%5Chat%7Bc%7D_1%3D1%2C%5C%3B%5Chat%7Bc%7D_2%3D2.5">
<meta property="og:image" content="https://math.now.sh?inline=%5Cquad">
<meta property="og:image" content="https://math.now.sh?inline=K">
<meta property="og:image" content="https://math.now.sh?inline=k">
<meta property="og:image" content="https://math.now.sh?inline=p_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28p%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20p_%7Bk%7D%5Cleft(1-p_%7Bk%7D%5Cright)%3D1-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20p_%7Bk%7D%5E%7B2%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=p%2C">
<meta property="og:image" content="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28p%29%3D2%20p(1-p)%0A">
<meta property="og:image" content="https://math.now.sh?inline=D%2C">
<meta property="og:image" content="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28D%29%3D1-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Cleft(%5Cfrac%7B%5Cleft%7CC_%7Bk%7D%5Cright%7C%7D%7B%7CD%7C%7D%5Cright)%5E%7B2%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=C_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=k">
<meta property="og:image" content="https://math.now.sh?inline=K">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=a">
<meta property="og:image" content="https://math.now.sh?inline=D_%7B1%7D">
<meta property="og:image" content="https://math.now.sh?inline=D_%7B2%7D">
<meta property="og:image" content="https://math.now.sh?from=D_%7B1%7D%3D%5C%7B%28x%2C%20y%29%20%5Cin%20D%20%5Cmid%20A(x)%3Da%5C%7D%2C%20%5Cquad%20D_%7B2%7D%3DD-D_%7B1%7D%5Cnonumber%0A">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28D%2C%20A%29%3D%5Cfrac%7B%5Cleft%7CD_%7B1%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Coperatorname%7BGini%7D%5Cleft(D_%7B1%7D%5Cright)%2B%5Cfrac%7B%5Cleft%7CD_%7B2%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Coperatorname%7BGini%7D%5Cleft(D_%7B2%7D%5Cright)%0A">
<meta property="og:image" content="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%9F%BA%E5%B0%BC%E3%80%81%E7%86%B5%E4%B8%8E%E5%88%86%E7%B1%BB%E8%AF%AF%E5%B7%AE.png">
<meta property="og:image" content="https://math.now.sh?inline=D%2C">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=A%2C">
<meta property="og:image" content="https://math.now.sh?inline=a%2C">
<meta property="og:image" content="https://math.now.sh?inline=A%3Da">
<meta property="og:image" content="https://math.now.sh?inline=D">
<meta property="og:image" content="https://math.now.sh?inline=D_%7B1%7D">
<meta property="og:image" content="https://math.now.sh?inline=D_%7B2%7D">
<meta property="og:image" content="https://math.now.sh?inline=A%3Da">
<meta property="og:image" content="https://math.now.sh?inline=A">
<meta property="og:image" content="https://math.now.sh?inline=a">
<meta property="og:image" content="https://math.now.sh?inline=T_0">
<meta property="og:image" content="https://math.now.sh?inline=T_0">
<meta property="og:image" content="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D">
<meta property="og:image" content="https://math.now.sh?from=C_%7B%5Calpha%7D%28T%29%3DC(T)%2B%5Calpha%7CT%7C%5Cnonumber%0A">
<meta property="og:image" content="https://math.now.sh?inline=C%28T%29%3D%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20N_%7Bt%7D%5Cleft(1-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Cleft(%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%5Cright)%5E%7B2%7D%5Cright)%2C%7CT%7C">
<meta property="og:image" content="https://math.now.sh?inline=K">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B%5Calpha%7D">
<meta property="og:image" content="https://math.now.sh?inline=C_%7B%5Calpha%7D%28T%29">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B%5Calpha%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha%20%5Crightarrow%20%5Cinfty">
<meta property="og:image" content="https://math.now.sh?inline=T_0">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=0%3D%5Calpha_%7B0%7D%3C">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha_%7B1%7D%3C%5Ccdots%3C%5Calpha_%7Bn%7D%3C%2B%5Cinfty%2C">
<meta property="og:image" content="https://math.now.sh?inline=%5Cleft%5B%5Calpha_%7Bi%7D%2C%20%5Calpha_%7Bi%2B1%7D%5Cright%29%2C%20i%3D0%2C1%2C%20%5Ccdots%2C%20n%20%3B">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha%20%5Cin%5Cleft%5B%5Calpha_%7Bi%7D%2C%20%5Calpha_%7Bi%2B1%7D%5Cright%29%2C%20i%3D0%2C1%2C%20%5Ccdots%2C%20n">
<meta property="og:image" content="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=T_0">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha%3D0">
<meta property="og:image" content="https://math.now.sh?inline=T_0">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?from=C_%7B%5Calpha%7D%28t%29%3DC(t)%2B%5Calpha%0A">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bt%7D">
<meta property="og:image" content="https://math.now.sh?from=C_%7B%5Calpha%7D%5Cleft%28T_%7Bt%7D%5Cright%29%3DC%5Cleft(T_%7Bt%7D%5Cright)%2B%5Calpha%5Cleft%7CT_%7Bt%7D%5Cright%7C%0A">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha%3D0">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?from=C_%7B%5Calpha%7D%5Cleft%28T_%7Bt%7D%5Cright%29%3CC_%7B%5Calpha%7D(t)%0A">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?from=%5Cbegin%7Balign%7D%0AC_%7B%5Calpha%7D%5Cleft%28T_%7Bt%7D%5Cright%29%20%26%3DC_%7B%5Calpha%7D(t)%5C%5C%0A%5Calpha%20%26%3D%5Cfrac%7BC(t)-C%5Cleft(T_%7Bt%7D%5Cright)%7D%7B%5Cleft%7CT_%7Bt%7D%5Cright%7C-1%7D%0A%5Cend%7Balign%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=T_t">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bt%7D">
<meta property="og:image" content="https://math.now.sh?inline=T_0">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?from=g%28t%29%3D%5Cfrac%7BC(t)-C%5Cleft(T_%7Bt%7D%5Cright)%7D%7B%5Cleft%7CT_%7Bt%7D%5Cright%7C-1%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=g%28t%29">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B0%7D">
<meta property="og:image" content="https://math.now.sh?inline=g%28t%29">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bt%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B1%7D%2C">
<meta property="og:image" content="https://math.now.sh?inline=g%28t%29">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha_%7B1%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Cleft%5B%5Calpha_%7B1%7D%2C%20%5Calpha_%7B2%7D%5Cright%29">
<meta property="og:image" content="https://math.now.sh?inline=g%28t%29%3D%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B%5Calpha%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B0%7D">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B%5Calpha%20%5Ccirc%7D">
<meta property="og:image" content="https://math.now.sh?inline=k%3D0%2C%20T%3DT_%7B0%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha%3D%2B%5Cinfty">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=C%5Cleft%28T_%7Bt%7D%5Cright%29%2C%5Cleft%7CT_%7Bt%7D%5Cright%7C">
<meta property="og:image" content="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0A%0Ag%28t%29%20%26%3D%5Cfrac%7BC(t)-C%5Cleft(T_%7Bt%7D%5Cright)%7D%7B%5Cleft%7CT_%7Bt%7D%5Cright%7C-1%7D%20%5C%5C%0A%5Calpha%20%26%3D%5Cmin%20(%5Calpha%2C%20g(t))%0A%5Cend%7Baligned%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bt%7D">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=C%5Cleft%28T_%7Bt%7D%5Cright%29">
<meta property="og:image" content="https://math.now.sh?inline=%5Cleft%7CT_%7Bt%7D%5Cright%7C">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bt%7D">
<meta property="og:image" content="https://math.now.sh?inline=g%28t%29%3D%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=k%3Dk%2B1%2C%20%5Calpha_%7Bk%7D%3D%5Calpha%2C%20T_%7Bk%7D%3DT_%7B%5Ccirc%7D">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bk%7D">
<meta property="og:image" content="https://math.now.sh?inline=%3B">
<meta property="og:image" content="https://math.now.sh?inline=T_%7Bk%7D%3DT_%7Bn%7D">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D">
<meta property="og:image" content="https://math.now.sh?inline=T_%7B%5Calpha%5E%7B%5Ccirc%7D%7D">
<meta property="og:image" content="https://math.now.sh?inline=C%28T%29">
<meta property="og:image" content="https://math.now.sh?inline=2%5E2">
<meta property="article:published_time" content="2020-12-02T08:14:56.880Z">
<meta property="article:modified_time" content="2020-12-18T08:01:49.754Z">
<meta property="article:author" content="CHongfeng Ling 凌崇锋">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://math.now.sh?inline=%5Cquad">


<link rel="canonical" href="http://example.com/2020/12/02/[SLM-5]%20%E5%86%B3%E7%AD%96%E6%A0%91%20Decision%20Tree/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title> | Chongfeng Ling</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chongfeng Ling</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">subtitle</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">决策树 Decision Tree</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Model"><span class="nav-number">1.1.</span> <span class="nav-text">1. Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8Eif-then%E8%A7%84%E5%88%99"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 决策树与if-then规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E7%BB%99%E5%AE%9A%E7%89%B9%E5%BE%81%E6%9D%A1%E4%BB%B6%E4%B8%8B%E7%9A%84%E7%B1%BB%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 决策树与给定特征条件下的类条件概率分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 决策树的学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-Feature-Selection"><span class="nav-number">1.2.</span> <span class="nav-text">2. 特征选择 Feature Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A-Information-Gain"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 信息增益 Information Gain</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-%E7%86%B5-Entropy%E3%80%81%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">2.1.1 熵 Entropy、信息熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-%E6%9D%A1%E4%BB%B6%E7%86%B5-Conditional-Entropy"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">2.1.2 条件熵 Conditional Entropy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">2.1.3 信息增益</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-4-%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E6%AF%94-information-gain-ratio"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">2.1.4 信息增益比 information gain ratio</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-5-%E7%89%B9%E5%BE%81%E5%A2%9E%E7%9B%8A%E7%9A%84%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.1.5.</span> <span class="nav-text">2.1.5 特征增益的算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-ID3-Algorithm"><span class="nav-number">1.3.</span> <span class="nav-text">3. ID3 Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-C4-5-Algorithm"><span class="nav-number">1.4.</span> <span class="nav-text">4. C4.5 Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Pruning"><span class="nav-number">1.5.</span> <span class="nav-text">5. Pruning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Pruning-algorithm"><span class="nav-number">1.5.1.</span> <span class="nav-text">5.1 Pruning algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-CART-Algorithm"><span class="nav-number">1.6.</span> <span class="nav-text">6. CART Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-CART%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-number">1.6.1.</span> <span class="nav-text">6.1 CART的生成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-1-%E5%9B%9E%E5%BD%92%E6%A0%91%E7%94%9F%E6%88%90"><span class="nav-number">1.6.1.1.</span> <span class="nav-text">6.1.1 回归树生成</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-1-1-Model"><span class="nav-number">1.6.1.1.1.</span> <span class="nav-text">6.1.1.1  Model</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-1-2-Algorithm"><span class="nav-number">1.6.1.1.2.</span> <span class="nav-text">6.1.1.2 Algorithm</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-1-3-Example"><span class="nav-number">1.6.1.1.3.</span> <span class="nav-text">6.1.1.3 Example</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-2-%E5%88%86%E7%B1%BB%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-number">1.6.1.2.</span> <span class="nav-text">6.1.2 分类树的生成</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-2-1-Gini-Index"><span class="nav-number">1.6.1.2.1.</span> <span class="nav-text">6.1.2.1 Gini Index</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-2-2-Algorithm"><span class="nav-number">1.6.1.2.2.</span> <span class="nav-text">6.1.2.2 Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-CART%E7%9A%84%E5%89%AA%E6%9E%9D"><span class="nav-number">1.6.2.</span> <span class="nav-text">6.2 CART的剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-%E5%89%AA%E6%9E%9D%E6%88%90%E4%B8%80%E4%B8%AA%E5%AD%90%E6%A0%91%E5%BA%8F%E5%88%97"><span class="nav-number">1.6.2.1.</span> <span class="nav-text">6.2.1 剪枝成一个子树序列</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">1.6.2.2.</span> <span class="nav-text">6.2.2 交叉验证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-3-Algorithm"><span class="nav-number">1.6.2.3.</span> <span class="nav-text">6.2.3 Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Question"><span class="nav-number">1.7.</span> <span class="nav-text">7. Question</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Code"><span class="nav-number">1.8.</span> <span class="nav-text">8. Code</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-ID3-create-test-visualization"><span class="nav-number">1.8.1.</span> <span class="nav-text">8.1 ID3: create, test, visualization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-Reference"><span class="nav-number">1.9.</span> <span class="nav-text">9. Reference</span></a></li></ol></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">CHongfeng Ling 凌崇锋</p>
  <div class="site-description" itemprop="description">description</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/02/[SLM-5]%20%E5%86%B3%E7%AD%96%E6%A0%91%20Decision%20Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-12-02 16:14:56" itemprop="dateCreated datePublished" datetime="2020-12-02T16:14:56+08:00">2020-12-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-18 16:01:49" itemprop="dateModified" datetime="2020-12-18T16:01:49+08:00">2020-12-18</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1>决策树 Decision Tree</h1>
<p>决策树是一种基本的分类与回归方法，这里主要讨论分类问题。**他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。**学习过程主要包括3个步骤：特征选择、决策树的生成、决策树的修建，并根据损失函数最小化的原则建立决策树模型。</p>
<h2 id="1-Model">1. Model</h2>
<blockquote>
<p>定义 5.1 (决策树) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点 ( node ) 和有向边 ( directed edge ) 组成。结点有两种类型: 内部结点 ( internal node ) 和叶结,点 ( leaf node ) 。内部结点表示一个特征或属性, 叶结点表示一个类。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">		A--是--&gt;C[叶结点]</span><br><span class="line">		A((根节点))--否--&gt;B((内部结点))</span><br><span class="line">		B--是--&gt;D[叶结点]</span><br><span class="line">		B--否--&gt;E[叶结点]</span><br></pre></td></tr></table></figure>
<h3 id="1-1-决策树与if-then规则">1.1 决策树与if-then规则</h3>
<ul>
<li>根结点到叶结点的每一条路径是一条规则</li>
<li>路径上的内部节点对应规则的条件</li>
<li>叶结点的类对应规则的结论</li>
<li>if-then规则是<strong>互斥且完备</strong>，即每一个实例有且仅有被一条路径/规则覆盖。</li>
</ul>
<h3 id="1-2-决策树与给定特征条件下的类条件概率分布">1.2 决策树与给定特征条件下的类条件概率分布</h3>
<p><strong>决策树的生成等价于对特征空间的划分(partition)</strong>，从而划分成互不相交的单元(cell)/区域(region)，再每一个单元定义一个类的概率分布就构成的一个条件概率分布。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。</p>
<p><img src="https://math.now.sh?inline=P%28Y%7CX%29%2C%5C%3BX%3A%E7%89%B9%E5%BE%81%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%5C%3B%5C%3B%5C%3B%5C%3BY%3A%E7%B1%BB%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F" style="display:inline-block;margin: 0;"/></p>
<p>条件概率<img src="https://math.now.sh?inline=P%28Y%7CX%29" style="display:inline-block;margin: 0;"/>往往偏大于某一类<img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/>。分类时把该节点实例强行分到条件概率大的那一类。</p>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLm-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87.png" alt="image-20201202210309048" style="zoom:50%;" />
<h3 id="1-3-决策树的学习">1.3 决策树的学习</h3>
<p>学习本质：为训练数据集归纳出一组分类规则</p>
<ul>
<li>正确分类训练数据集的决策树可能有很多个，可能没有</li>
<li>需要找到一个与训练数据集的误差较小、泛化能力高的决策树</li>
</ul>
<p>损失函数与策略：最小化正则化后的极大似然函数</p>
<p>算法：递归的选择最优特征。ID3、C4.5、CART</p>
<p>剪枝：树枝多，复杂，泛化能力低。自下而上进行剪枝。树的生成考虑局部最优，树的剪枝考虑全局最优。</p>
<h2 id="2-特征选择-Feature-Selection">2. 特征选择 Feature Selection</h2>
<p>通过信息增益（比）来选取具有分类能力的特征（指那些与随机分类有较大查别的特征），从而提高决策树的学习效率。</p>
<h3 id="2-1-信息增益-Information-Gain">2.1 信息增益 Information Gain</h3>
<h4 id="2-1-1-熵-Entropy、信息熵">2.1.1 熵 Entropy、信息熵</h4>
<blockquote>
<p>定义 5.01（熵）		熵是表示随机变量不确定性的度量。设<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>为离散随机变量，概率分布为</p>
<p style=""><img src="https://math.now.sh?from=P%5Cleft%28X%3Dx_%7Bi%7D%5Cright%29%3Dp_%7Bi%7D%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%0A" /></p><p>随机变量<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的熵为</p>
<p style=""><img src="https://math.now.sh?from=H%28X%29%3DH(p)%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p_%7Bi%7D%20%5Clog%20p_%7Bi%7D%0A" /></p><p style=""><img src="https://math.now.sh?from=%E5%AF%B9p_i%3D0%EF%BC%8C%E5%AE%9A%E4%B9%890log0%3D0%5Cnonumber%0A" /></p></blockquote>
<ul>
<li>在(2)中，对数底为2/<img src="https://math.now.sh?inline=e" style="display:inline-block;margin: 0;"/>时，熵单位为比特bit/纳特nat</li>
<li>熵只和<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的分布有关，与取值<img src="https://math.now.sh?inline=x_i" style="display:inline-block;margin: 0;"/>无关</li>
<li>熵越大，随机变量的不确定性就越大</li>
<li><img src="https://math.now.sh?inline=0%20%5Cleqslant%20H%28p%29%20%5Cleqslant%20%5Clog%20n" style="display:inline-block;margin: 0;"/></li>
</ul>
<h4 id="2-1-2-条件熵-Conditional-Entropy">2.1.2 条件熵 Conditional Entropy</h4>
<blockquote>
<p>定义 5.02（条件熵）		对随机变量(X,Y)，联合概率分布为</p>
<p style=""><img src="https://math.now.sh?from=P%5Cleft%28X%3Dx_%7Bi%7D%2C%20Y%3Dy_%7Bj%7D%5Cright%29%3Dp_%7Bi%20j%7D%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%20%3B%20%5Cquad%20j%3D1%2C2%2C%20%5Ccdots%2C%20m%5Cnonumber%0A" /></p><p>条件熵<img src="https://math.now.sh?inline=H%28Y%7CX%29" style="display:inline-block;margin: 0;"/>表示一直随机变量<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的情况下随机变量<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>的不确定性。<strong>定义为给定<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>后<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>的条件概率分布的熵对<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的数学期望</strong>，即</p>
<p style=""><img src="https://math.now.sh?from=H%28Y%20%5Cmid%20X%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p_%7Bi%7D%20H%5Cleft(Y%20%5Cmid%20X%3Dx_%7Bi%7D%5Cright)%5C%5C%0Ap_%7Bi%7D%3DP%5Cleft(X%3Dx_%7Bi%7D%5Cright)%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%0A" /></p></blockquote>
<p>当熵和条件熵的概率有极大似然估计得到时，对应的为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy)</p>
<h4 id="2-1-3-信息增益">2.1.3 信息增益</h4>
<p><strong>信息增益表示得知特征<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的信息从而使得类<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>的信息的不确定性减少程度。</strong></p>
<blockquote>
<p>定义 5.2 (信息增益) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 对训练数据集<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>的信息增益 <img src="https://math.now.sh?inline=g%28D%2C%20A%29%2C" style="display:inline-block;margin: 0;"/> 定义为集合<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>的经验熵<img src="https://math.now.sh?inline=H%28D%29" style="display:inline-block;margin: 0;"/> 与特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 给定条件下<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>的经验条件熵 <img src="https://math.now.sh?inline=H%28D%20%5Cmid%20A%29" style="display:inline-block;margin: 0;"/> 之差，即</p>
<p style=""><img src="https://math.now.sh?from=g%28D%2C%20A%29%3DH(D)-H(D%20%5Cmid%20A)%5Cnonumber%0A" /></p></blockquote>
<ul>
<li>熵和条件熵之差称为互信息 mutual information，在决策树中即为信息增益</li>
<li><img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>一般为标签</li>
</ul>
<h4 id="2-1-4-信息增益比-information-gain-ratio">2.1.4 信息增益比 information gain ratio</h4>
<p>在训练集里，某一个特征较多时，信息增益会偏大。因此采用信息增益比来校正。</p>
<blockquote>
<p>定义 5.3 $ (信息增益比) $$\quad$ 特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 对训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的信息增益比 <img src="https://math.now.sh?inline=g_%7BR%7D%28D%2C%20A%29" style="display:inline-block;margin: 0;"/> 定义为其信息增益 <img src="https://math.now.sh?inline=g%28D%2C%20A%29" style="display:inline-block;margin: 0;"/> 与训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 关于特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 的值的熵 <img src="https://math.now.sh?inline=H_%7BA%7D%28D%29" style="display:inline-block;margin: 0;"/> 之比，即</p>
<p style=""><img src="https://math.now.sh?from=g_%7BR%7D%28D%2C%20A%29%3D%5Cfrac%7Bg(D%2C%20A)%7D%7BH_%7BA%7D(D)%7D%0A" /></p><p>其中 <img src="https://math.now.sh?inline=%2C%20H_%7BA%7D%28D%29%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%2C%20n" style="display:inline-block;margin: 0;"/> 是特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 取值的个数。</p>
</blockquote>
<h4 id="2-1-5-特征增益的算法">2.1.5 特征增益的算法</h4>
<p>对数据集<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>，有<img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/>个类<img src="https://math.now.sh?inline=C_k" style="display:inline-block;margin: 0;"/>，对特征<img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/>有<img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"/>个取值<img src="https://math.now.sh?inline=D_n" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=D_%7Bi%20k%7D%3DD_%7Bi%7D%20%5Ccap%20C_%7Bk%7D" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=%7CD%7C" style="display:inline-block;margin: 0;"/>为数据集中的样本个数</p>
<blockquote>
<p>算法 5.1 (信息增益的算法)<br>
输入: 训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 和特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/>;<br>
输出: 特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 对训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的信息增益 <img src="https://math.now.sh?inline=g%28D%2C%20A%29" style="display:inline-block;margin: 0;"/> 。<br>
(1) 计算数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的经验熵<img src="https://math.now.sh?inline=H%28D%29" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=H%28D%29%3D-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20%5Cfrac%7B%5Cleft%7CC_%7Bk%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CC_%7Bk%7D%5Cright%7C%7D%7B%7CD%7C%7D%5Cnonumber%0A" /></p><p>(2) 计算特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 对数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的经验条件熵 <img src="https://math.now.sh?inline=H%28D%20%5Cmid%20A%29" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=H%28D%20%5Cmid%20A%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%20H%5Cleft(D_%7Bi%7D%5Cright)%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%20k%7D%5Cright%7C%7D%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%20k%7D%5Cright%7C%7D%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%5Cnonumber%0A" /></p><p>(3) 计算信息增益</p>
<p style=""><img src="https://math.now.sh?from=g%28D%2C%20A%29%3DH(D)-H(D%20%5Cmid%20A)%5Cnonumber%0A" /></p></blockquote>
<ul>
<li>没有特征B！A是特征的符号</li>
</ul>
<h2 id="3-ID3-Algorithm">3. ID3 Algorithm</h2>
<p>决策树各个节点熵应用信息增益准则选择特征，递归构建决策树。</p>
<p>从根结点开始，计算所有可能的特征，选取信息增益最大的特征作为节点特征。并由该特征的不同取值点建立子节点。递归调用。</p>
<blockquote>
<p>算法 5.2 (ID3 算法)<br>
输入: 训练数据集 <img src="https://math.now.sh?inline=D%2C" style="display:inline-block;margin: 0;"/> 特征集 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 间值 <img src="https://math.now.sh?inline=%5Cvarepsilon%20%3B" style="display:inline-block;margin: 0;"/><img src="https://math.now.sh?inline=%5C%5C" style="display:inline-block;margin: 0;"/></p>
<p>输出：决策树 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 。</p>
<p>(1) 若 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中所有实例属于同一类 <img src="https://math.now.sh?inline=C_%7Bk%7D%2C" style="display:inline-block;margin: 0;"/> 则 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点 的类标记，返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(2) 若 <img src="https://math.now.sh?inline=A%3D%5Cvarnothing%2C" style="display:inline-block;margin: 0;"/> 则 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中实例数最大的类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点的类标记，返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> :<br>
(3) 否则，按算法 5.1 计算 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 中各特征对 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的信息增益，选择信息增益最大的特征 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> :</p>
<p>(4) 如果 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> 的信息增益小于间值 <img src="https://math.now.sh?inline=%5Cvarepsilon%2C" style="display:inline-block;margin: 0;"/> 则置 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中实例数最大 的类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点的类标记，返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> :<br>
(5) 否则，对 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> 的每一可能值 <img src="https://math.now.sh?inline=a_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 依 <img src="https://math.now.sh?inline=A_%7Bg%7D%3Da_%7Bi%7D" style="display:inline-block;margin: 0;"/> 将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 分割为若干非空子集 <img src="https://math.now.sh?inline=D_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 将 <img src="https://math.now.sh?inline=D_%7Bi%7D" style="display:inline-block;margin: 0;"/> 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 <img src="https://math.now.sh?inline=T%2C" style="display:inline-block;margin: 0;"/> 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(6) 对第 <img src="https://math.now.sh?inline=i" style="display:inline-block;margin: 0;"/> 个子结点，以 <img src="https://math.now.sh?inline=D_%7Bi%7D" style="display:inline-block;margin: 0;"/> 为训练集，以 <img src="https://math.now.sh?inline=A-%5Cleft%5C%7BA_%7Bg%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/> 为特征集，逆归地调用步 (1)<img src="https://math.now.sh?inline=%5Csim" style="display:inline-block;margin: 0;"/> 步 <img src="https://math.now.sh?inline=%285%29%2C" style="display:inline-block;margin: 0;"/> 得到子树 <img src="https://math.now.sh?inline=T_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 返回 <img src="https://math.now.sh?inline=T_%7Bi%7D%20%5Ccirc" style="display:inline-block;margin: 0;"/></p>
</blockquote>
<ul>
<li>极大似然法进行概率估计？</li>
<li>只有树的生成，没有剪枝，容易过拟合</li>
</ul>
<h2 id="4-C4-5-Algorithm">4. C4.5 Algorithm</h2>
<p>ID3算法的改进，用信息增益比来选择特征</p>
<blockquote>
<p>算法 5.3 (C4.5 的生成算法)</p>
<p>输入: 训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>, 特征集 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 间值 <img src="https://math.now.sh?inline=%5Cvarepsilon" style="display:inline-block;margin: 0;"/>;</p>
<p>输出：决策树 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 。<br>
(1) 如果 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中所有实例属于同一类 <img src="https://math.now.sh?inline=C_%7Bk%7D%2C" style="display:inline-block;margin: 0;"/> 则置 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结 点的类, 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(2) 如果 <img src="https://math.now.sh?inline=A%3D%5Cvarnothing%2C" style="display:inline-block;margin: 0;"/> 则置 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中实例数最大的类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点 的类, 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(3) 否则，按式 (5.10) 计算 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 中各特征对 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的<em><strong>信息增益比</strong></em>, 选择信息增益比最大 的特征 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/>;<br>
(4) 如果 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> 的信息增益比小于间值 <img src="https://math.now.sh?inline=%5Cvarepsilon%2C" style="display:inline-block;margin: 0;"/> 则置 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中实例数最 大的类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点的类, 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(5) 否则, 对 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> 的每一可能值 <img src="https://math.now.sh?inline=a_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 依 <img src="https://math.now.sh?inline=A_%7Bg%7D%3Da_%7Bi%7D" style="display:inline-block;margin: 0;"/> 将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 分割为子集若干非空 <img src="https://math.now.sh?inline=D_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 将 <img src="https://math.now.sh?inline=D_%7Bi%7D" style="display:inline-block;margin: 0;"/> 中实例数最大的类作为标记，构建子结点, 由结点及其子结点构成树 <img src="https://math.now.sh?inline=T%2C" style="display:inline-block;margin: 0;"/> 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(6) 对结点 <img src="https://math.now.sh?inline=i" style="display:inline-block;margin: 0;"/>, 以 <img src="https://math.now.sh?inline=D_%7Bi%7D" style="display:inline-block;margin: 0;"/> 为训练集，以 <img src="https://math.now.sh?inline=A-%5Cleft%5C%7BA_%7Bg%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/> 为特征集，递归地调用步 (1) 步 <img src="https://math.now.sh?inline=%285%29%2C" style="display:inline-block;margin: 0;"/> 得到子树 <img src="https://math.now.sh?inline=T_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 返回 <img src="https://math.now.sh?inline=T_%7Bi%7D" style="display:inline-block;margin: 0;"/> 。</p>
</blockquote>
<h2 id="5-Pruning">5. Pruning</h2>
<p>考虑树的复杂度，对生成的决策树进行剪枝，减掉子树或叶结点</p>
<p><strong>策略：极小化决策树的损失函数</strong>，即正则化的极大似然估计</p>
<p>设树<img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>，叶节点个数<img src="https://math.now.sh?inline=%7CT%7C" style="display:inline-block;margin: 0;"/>，树<img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>的叶结点<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>，此叶结点的样本点个数<img src="https://math.now.sh?inline=N_t" style="display:inline-block;margin: 0;"/>；此叶结点有<img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"/>类样本点的个数<img src="https://math.now.sh?inline=N_%7Btk%7D%EF%BC%9B%5C%3Bk%3D1%2C2%2C...%2CK" style="display:inline-block;margin: 0;"/>；此叶结点的经验熵<img src="https://math.now.sh?inline=H_t%28T%29" style="display:inline-block;margin: 0;"/>，函数参数<img src="https://math.now.sh?inline=%5Calpha%20%5Cgeq0" style="display:inline-block;margin: 0;"/>。此决策树的损失函数定义为</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%28T%29%3D%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20N_%7Bt%7D%20H_%7Bt%7D(T)%2B%5Calpha%7CT%7C%0A" /></p><p>经验熵<img src="https://math.now.sh?inline=H_t%28T%29" style="display:inline-block;margin: 0;"/>为</p>
<p style=""><img src="https://math.now.sh?from=H_%7Bt%7D%28T%29%3D-%5Csum_%7Bk%7D%20%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%20%5Clog%20%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%0A" /></p><p>记(6)的左边为<img src="https://math.now.sh?inline=C%28T%29" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=C%28T%29%3D%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20N_%7Bt%7D%20H_%7Bt%7D(T)%3D-%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20N_%7Bt%20k%7D%20%5Clog%20%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%0A" /></p><p>最后得到</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%28T%29%3DC(T)%2B%5Calpha%7CT%7C%0A" /></p><ul>
<li><img src="https://math.now.sh?inline=C%28T%29" style="display:inline-block;margin: 0;"/>为样本点个数与经验熵的积，表示模型对训练数据的预测误差</li>
<li><img src="https://math.now.sh?inline=%7CT%7C" style="display:inline-block;margin: 0;"/>为模型复杂度
<ul>
<li>越大说明叶结点越多，树越复杂</li>
<li><img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>越大，选择较简单的模型树</li>
</ul>
</li>
</ul>
<h3 id="5-1-Pruning-algorithm">5.1 Pruning algorithm</h3>
<blockquote>
<p>算法 5.4 (树的剪枝算法)<br>
输入: 生成算法产生的整个树 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>, 参数 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>;<br>
输出：修剪后的子树 <img src="https://math.now.sh?inline=T_%7B%5Calpha%7D%20%5Ccirc" style="display:inline-block;margin: 0;"/><br>
（1）计算每个结点的经验熵。<br>
（2）递归地从树的叶结点向上回缩。</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D.png" alt="image-20201205222057313"></p>
<p>​	设一组叶结点回缩到其父结点之前与之后的整体树分别为 <img src="https://math.now.sh?inline=T_%7BB%7D" style="display:inline-block;margin: 0;"/> 与 <img src="https://math.now.sh?inline=T_%7BA%7D" style="display:inline-block;margin: 0;"/>, 其对应的 损失函数值分别是 <img src="https://math.now.sh?inline=C_%7B%5Calpha%7D%5Cleft%28T_%7BB%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 与 <img src="https://math.now.sh?inline=C_%7B%5Calpha%7D%5Cleft%28T_%7BA%7D%5Cright%29%2C" style="display:inline-block;margin: 0;"/> 如果</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%5Cleft%28T_%7BA%7D%5Cright%29%20%5Cleqslant%20C_%7B%5Calpha%7D%5Cleft(T_%7BB%7D%5Cright)%0A" /></p><p>则进行剪枝，即将父结点变为新的叶结点。<br>
(3) 返回 ( 2 )，直至不能继续为止，得到损失函数最小的子树 <img src="https://math.now.sh?inline=T_%7B%5Calpha%5E%7B%5Ccirc%7D%7D" style="display:inline-block;margin: 0;"/></p>
</blockquote>
<ul>
<li>动态规划的算法实现</li>
</ul>
<h2 id="6-CART-Algorithm">6. CART Algorithm</h2>
<p>分类与回归树(classification and regression tree)是给定随机变量<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的条件下给出随机变量<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>的条件概率分布的学习方法。决策树是二叉树，左分支是“是”分支，右否。<strong>通过递归的二分每个特征，使得特征空间划分成有限个单元</strong>，在这些单元上确定概率分布。</p>
<h3 id="6-1-CART的生成">6.1 CART的生成</h3>
<h4 id="6-1-1-回归树生成">6.1.1 回归树生成</h4>
<h5 id="6-1-1-1-Model">6.1.1.1  Model</h5>
<p>用<strong>平方误差最小化准则</strong>，进行特征选择，递归的构建二叉决策树。</p>
<p>对数据集<img src="https://math.now.sh?inline=D%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>是连续变量，生成回归树。</p>
<p>回归树对应的是<strong>特征空间的划分</strong>以及<strong>划分单元上的输出值</strong>。</p>
<blockquote>
<p>定义 5.03 （回归树模型）</p>
<p>假设已将输入空间划分为 <img src="https://math.now.sh?inline=M" style="display:inline-block;margin: 0;"/> 个单元 <img src="https://math.now.sh?inline=R_%7B1%7D%2C%20R_%7B2%7D%2C%20%5Ccdots%2C%20R_%7BM%7D%2C" style="display:inline-block;margin: 0;"/> 并且在每个单元 <img src="https://math.now.sh?inline=R_%7Bm%7D" style="display:inline-block;margin: 0;"/> 上 有一个固定的输出值 <img src="https://math.now.sh?inline=c_%7Bm%7D%2C" style="display:inline-block;margin: 0;"/> 于是回归树模型可表示为</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20c_%7Bm%7D%20I%5Cleft(x%20%5Cin%20R_%7Bm%7D%5Cright)%0A" /></p></blockquote>
<ul>
<li>
<p><img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/>是特征向量</p>
</li>
<li>
<p>当输入空间划分确定，利用平方误差最小准则可以确定<img src="https://math.now.sh?inline=c_m" style="display:inline-block;margin: 0;"/>的最优值</p>
<ul>
<li>
<p>平方误差定义为<img src="https://math.now.sh?inline=%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7Bm%7D%7D%5Cleft%28y_%7Bi%7D-f%5Cleft(x_%7Bi%7D%5Cright%29%5Cright)%5E%7B2%7D" style="display:inline-block;margin: 0;"/>。注意<img src="https://math.now.sh?inline=x_i%5Cin%20R_m" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>利用平方误差最小原则，单元 <img src="https://math.now.sh?inline=R_%7Bm%7D" style="display:inline-block;margin: 0;"/> 上的 <img src="https://math.now.sh?inline=c_%7Bm%7D" style="display:inline-block;margin: 0;"/> 的最优值 <img src="https://math.now.sh?inline=%5Chat%7Bc%7D_%7Bm%7D" style="display:inline-block;margin: 0;"/> 是 <img src="https://math.now.sh?inline=R_%7Bm%7D" style="display:inline-block;margin: 0;"/> 上的所有输入实例 <img src="https://math.now.sh?inline=x_%7Bi%7D" style="display:inline-block;margin: 0;"/> 对应的输出 <img src="https://math.now.sh?inline=y_%7Bi%7D" style="display:inline-block;margin: 0;"/> 的均值，即</p>
<p style=""><img src="https://math.now.sh?from=%5Chat%7Bc%7D_%7Bm%7D%3D%5Coperatorname%7Bave%7D%5Cleft%28y_%7Bi%7D%20%5Cmid%20x_%7Bi%7D%20%5Cin%20R_%7Bm%7D%5Cright%29%5Cnonumber%0A" /></p></li>
</ul>
</li>
</ul>
<p><strong>问题1：如何划分特征空间，即选择划分点</strong></p>
<p>通过启发式算法，选择第 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/> 个变量 <img src="https://math.now.sh?inline=x%5E%7B%28j%29%7D" style="display:inline-block;margin: 0;"/>和它取的特征值 <img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>, 作为切分变量（splitting variable）和切分点 (splitting point)，并定义两个区域</p>
<p style=""><img src="https://math.now.sh?from=R_%7B1%7D%28j%2C%20s%29%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%20%5Cleqslant%20s%5Cright%5C%7D%20%5Cquad%20%5Ctext%20%7B%20%E5%92%8C%20%7D%20%5Cquad%20R_%7B2%7D(j%2C%20s)%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%3Es%5Cright%5C%7D%0A" /></p><p>在一开始的选择基础上，寻找最优切分变量<img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/>和最优切分点<img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>。具体的，求解</p>
<p style=""><img src="https://math.now.sh?from=%5Cmin%20_%7Bj%2C%20s%7D%5Cleft%5B%5Cmin%20_%7Bc_%7B1%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B1%7D%28j%2C%20s%29%7D%5Cleft(y_%7Bi%7D-c_%7B1%7D%5Cright)%5E%7B2%7D%2B%5Cmin%20_%7Bc_%7B2%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%7D%5Cleft(y_%7Bi%7D-c_%7B2%7D%5Cright)%5E%7B2%7D%5Cright%5D%0A" /></p><ul>
<li>(11)中，<img src="https://math.now.sh?inline=x%5E%7B%28j%29%7D" style="display:inline-block;margin: 0;"/>是第<img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/>个特征，<img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>是此特征维度上的值。<img src="https://math.now.sh?inline=x_n" style="display:inline-block;margin: 0;"/>是第<img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"/>个输入向量。<a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/93667566">ref</a></li>
<li>(12)中，括号里的对于<img src="https://math.now.sh?inline=c_1%2Cc_2" style="display:inline-block;margin: 0;"/>取极小值是已知的。</li>
<li>划分特征空间时是把高维矩形划分为2个子高维矩形。</li>
</ul>
<p><strong>问题2：如何确定好输出值<img src="https://math.now.sh?inline=c_m" style="display:inline-block;margin: 0;"/></strong></p>
<p>特征空间切分好后，根据最小化平方误差准则，得到相应的最优输出值</p>
<p style=""><img src="https://math.now.sh?from=%5Chat%7Bc%7D_%7B1%7D%3D%5Coperatorname%7Bave%7D%5Cleft%28y_%7Bi%7D%20%5Cmid%20x_%7Bi%7D%20%5Cin%20R_%7B1%7D(j%2C%20s%29%5Cright)%20%5Cquad%20%5Ctext%20%7B%20%E5%92%8C%20%7D%20%5Cquad%20%5Chat%7Bc%7D_%7B2%7D%3D%5Coperatorname%7Bave%7D%5Cleft(y_%7Bi%7D%20%5Cmid%20x_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%5Cright)%0A" /></p><p>**总结：**一开始，我们遍历所有输入特征，找到最优的切分特征变量<img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/>，构成一对<img src="https://math.now.sh?inline=%28j%2Cs%29" style="display:inline-block;margin: 0;"/>，根据这个构成的超平面，讲特征空间划分成2个区域。对每个子区域重复过程，直到满足停止条件。这种回归树称为最小二乘回归树(least squares regression tree)</p>
<h5 id="6-1-1-2-Algorithm">6.1.1.2 Algorithm</h5>
<blockquote>
<p>算法 5.5 (最小二乘回归树生成算法)<br>
输入: 训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>;<br>
输出：回归树 <img src="https://math.now.sh?inline=f%28x%29" style="display:inline-block;margin: 0;"/> 。<br>
在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树:<br>
（1）选择最优切分变量 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/> 与切分点 <img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>, 求解</p>
<p style=""><img src="https://math.now.sh?from=%5Cmin%20_%7Bj%2C%20s%7D%5Cleft%5B%5Cmin%20_%7Bc_%7B1%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B1%7D%28j%2C%20s%29%7D%5Cleft(y_%7Bi%7D-c_%7B1%7D%5Cright)%5E%7B2%7D%2B%5Cmin%20_%7Bc_%7B2%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%7D%5Cleft(y_%7Bi%7D-c_%7B2%7D%5Cright)%5E%7B2%7D%5Cright%5D%0A" /></p><p>遍历变量 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/>, 对固定的切分变量 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/> 扫描切分点 <img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>, 选择使式 (14) 达到最小值的对<img src="https://math.now.sh?inline=%28j%2C%20s%29" style="display:inline-block;margin: 0;"/><br>
（2）用选定的对 <img src="https://math.now.sh?inline=%28j%2C%20s%29" style="display:inline-block;margin: 0;"/> 划分区域并决定相应的输出值:</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bc%7D%0AR_%7B1%7D%28j%2C%20s%29%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%20%5Cleqslant%20s%5Cright%5C%7D%2C%20%5Cquad%20R_%7B2%7D(j%2C%20s)%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%3Es%5Cright%5C%7D%20%5C%5C%0A%5Chat%7Bc%7D_%7Bm%7D%3D%5Cfrac%7B1%7D%7BN_%7Bm%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7Bm%7D(j%2C%20s)%7D%20y_%7Bi%7D%2C%20%5Cquad%20x%20%5Cin%20R_%7Bm%7D%2C%20%5Cquad%20m%3D1%2C2%0A%5Cend%7Barray%7D%0A" /></p><p>（3）继续对两个子区域调用步骤 <img src="https://math.now.sh?inline=%281%29%2C(2)%2C" style="display:inline-block;margin: 0;"/> 直至满足停止条件。<br>
（4）将输入空间划分为 <img src="https://math.now.sh?inline=M" style="display:inline-block;margin: 0;"/> 个区域 <img src="https://math.now.sh?inline=R_%7B1%7D%2C%20R_%7B2%7D%2C%20%5Ccdots%2C%20R_%7BM%7D%2C" style="display:inline-block;margin: 0;"/> 生成决策树:</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20%5Chat%7Bc%7D_%7Bm%7D%20I%5Cleft(x%20%5Cin%20R_%7Bm%7D%5Cright)%0A" /></p></blockquote>
<h5 id="6-1-1-3-Example">6.1.1.3 Example</h5>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E1" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E2" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E3" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_1" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.2</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_2" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">3.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_3" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.6.</td>
<td style="text-align:center">6</td>
<td style="text-align:center">2.75</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_4" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">9</td>
<td style="text-align:center">2.25</td>
<td style="text-align:center">3</td>
</tr>
</tbody>
</table>
<p>**Q：**对以上数据集基于最小化平方误差生成二叉回归树</p>
<ol>
<li>
<p>设<img src="https://math.now.sh?inline=j%3Dx%5E1" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=s%3D1.5" style="display:inline-block;margin: 0;"/>时</p>
<p><img src="https://math.now.sh?inline=c_1%3D1%2C%5C%3Bc_2%3D2.5" style="display:inline-block;margin: 0;"/></p>
<p><img src="https://math.now.sh?inline=%5Cmin%20_%7Bc_%7B1%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B1%7D%28j%2C%20s%29%7D%5Cleft(y_%7Bi%7D-c_%7B1%7D%5Cright)%5E%7B2%7D%2B%5Cmin%20_%7Bc_%7B2%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%7D%5Cleft(y_%7Bi%7D-c_%7B2%7D%5Cright)%5E%7B2%7D%3D%5C%5C%20%5B(1-1)%5E2%2B(1-1)%5E2%5D%2B%5B(2-2.5)%5E2%2B(3-2.5)%5E2%5D%3D0.5" style="display:inline-block;margin: 0;"/></p>
<p><img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E1%2C1.6)%2C%5C%3B0.67" style="display:inline-block;margin: 0;"/></p>
<p><img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E1%2C1.2)%2C%5C%3B(j%2Cs)%3D(x%5E1%2C1.8)" style="display:inline-block;margin: 0;"/>的结果一定偏大</p>
<p>对固定<img src="https://math.now.sh?inline=j%3Dx%5E1" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=s%3D1.5" style="display:inline-block;margin: 0;"/>是最佳切分点，<img src="https://math.now.sh?inline=error%3D0.5" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>设<img src="https://math.now.sh?inline=j%3Dx%5E2" style="display:inline-block;margin: 0;"/></p>
<p>最佳切分为<img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E2%2C4)%2C%5C%3Berror%3D0.5" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>设<img src="https://math.now.sh?inline=j%3Dx%5E3" style="display:inline-block;margin: 0;"/></p>
<p>最佳切分为<img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E3%2C2.75)%2C%5C%3Berror%3D2" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p><img src="https://math.now.sh?inline=%5Cmin%20_%7Bj%2C%20s%7D%3D%5Cmin%5B0.5%2C%5C%3B0.5%2C%5C%3B2%5D%3D0.5" style="display:inline-block;margin: 0;"/>，选择<img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E1%2C1.5)" style="display:inline-block;margin: 0;"/>作为最优划分。划分后的子集<img src="https://math.now.sh?inline=R_1%2C%5C%3BR_2" style="display:inline-block;margin: 0;"/>为</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong><img src="https://math.now.sh?inline=R_1" style="display:inline-block;margin: 0;"/>左分支</strong></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E1" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E2" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E3" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_1" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.2</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_2" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">3.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><strong><img src="https://math.now.sh?inline=R_2" style="display:inline-block;margin: 0;"/>右分支</strong></td>
<td style="text-align:center"><img src="https://math.now.sh?inline=x%5E1" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center"><img src="https://math.now.sh?inline=x%5E2" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center"><img src="https://math.now.sh?inline=x%5E3" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center"><img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/></td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_3" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.6.</td>
<td style="text-align:center">6</td>
<td style="text-align:center">2.75</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_4" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">9</td>
<td style="text-align:center">2.25</td>
<td style="text-align:center">3</td>
</tr>
</tbody>
</table>
<p><img src="https://math.now.sh?inline=%5Chat%7Bc%7D_1%3D1%2C%5C%3B%5Chat%7Bc%7D_2%3D2.5" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>对左右分支继续迭代1-4的步骤，直到满足停止条件</p>
</li>
</ol>
<h4 id="6-1-2-分类树的生成">6.1.2 分类树的生成</h4>
<p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<h5 id="6-1-2-1-Gini-Index">6.1.2.1 Gini Index</h5>
<blockquote>
<p>定义 5.4 (基尼指数) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 分类问题中，假设有 <img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"/> 个类，样本点属于第 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 类的概率<br>
为 <img src="https://math.now.sh?inline=p_%7Bk%7D" style="display:inline-block;margin: 0;"/>, 则棍率分布的基尼指数定义为</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28p%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20p_%7Bk%7D%5Cleft(1-p_%7Bk%7D%5Cright)%3D1-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20p_%7Bk%7D%5E%7B2%7D%0A" /></p><p>对于二类分类问题, 若样本点属于第 1 个类的概率是 <img src="https://math.now.sh?inline=p%2C" style="display:inline-block;margin: 0;"/> 则概率分布的基尼指数为</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28p%29%3D2%20p(1-p)%0A" /></p><p>对于给定的样本集合 <img src="https://math.now.sh?inline=D%2C" style="display:inline-block;margin: 0;"/> 其基尼指数为</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28D%29%3D1-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Cleft(%5Cfrac%7B%5Cleft%7CC_%7Bk%7D%5Cright%7C%7D%7B%7CD%7C%7D%5Cright)%5E%7B2%7D%0A" /></p><p>这里, <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 是 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中属于第 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 类的样本子集， <img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"/> 是类的个数。</p>
<p>如果样本集合 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 根据<strong>特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/></strong> 是否取某一可能值 <img src="https://math.now.sh?inline=a" style="display:inline-block;margin: 0;"/> 被分割成 <img src="https://math.now.sh?inline=D_%7B1%7D" style="display:inline-block;margin: 0;"/> 和 <img src="https://math.now.sh?inline=D_%7B2%7D" style="display:inline-block;margin: 0;"/> 两部分，即</p>
<p style=""><img src="https://math.now.sh?from=D_%7B1%7D%3D%5C%7B%28x%2C%20y%29%20%5Cin%20D%20%5Cmid%20A(x)%3Da%5C%7D%2C%20%5Cquad%20D_%7B2%7D%3DD-D_%7B1%7D%5Cnonumber%0A" /></p><p>则在特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 的条件下，集合 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的基尼指数定义为</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28D%2C%20A%29%3D%5Cfrac%7B%5Cleft%7CD_%7B1%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Coperatorname%7BGini%7D%5Cleft(D_%7B1%7D%5Cright)%2B%5Cfrac%7B%5Cleft%7CD_%7B2%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Coperatorname%7BGini%7D%5Cleft(D_%7B2%7D%5Cright)%0A" /></p></blockquote>
<ul>
<li>基尼指数表示集合的不确定性。基尼指数越大，集合的不确定性越大。和熵相似。</li>
<li>二元分类中基尼指数、单位比特熵和分类误差的关系。x轴：概率p。y轴：损失。<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%9F%BA%E5%B0%BC%E3%80%81%E7%86%B5%E4%B8%8E%E5%88%86%E7%B1%BB%E8%AF%AF%E5%B7%AE.png" alt="image-20201208142218988" style="zoom:50%;" /></li>
</ul>
<h5 id="6-1-2-2-Algorithm">6.1.2.2 Algorithm</h5>
<blockquote>
<p>算法 5.6 (CART 生成算法)</p>
<p>输入: 训练数据集 <img src="https://math.now.sh?inline=D%2C" style="display:inline-block;margin: 0;"/> 停止计算的条件;<br>
输出: CART 决策树。</p>
<p>根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树:<br>
（1） 设结点的训练数据集为 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>, 计算现有特征对该数据集的基尼指数。此时，对每一个特征 <img src="https://math.now.sh?inline=A%2C" style="display:inline-block;margin: 0;"/> 对其可能取的每个值 <img src="https://math.now.sh?inline=a%2C" style="display:inline-block;margin: 0;"/> 根据样本点对 <img src="https://math.now.sh?inline=A%3Da" style="display:inline-block;margin: 0;"/> 的测试为“是”或“否”，将<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 分割成 <img src="https://math.now.sh?inline=D_%7B1%7D" style="display:inline-block;margin: 0;"/> 和 <img src="https://math.now.sh?inline=D_%7B2%7D" style="display:inline-block;margin: 0;"/> 两部分，利用式 (20) 计算 <img src="https://math.now.sh?inline=A%3Da" style="display:inline-block;margin: 0;"/> 时的基尼指数。<br>
（2）在所有可能的特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 以及它们所有可能的切分点 <img src="https://math.now.sh?inline=a" style="display:inline-block;margin: 0;"/> 中，选择基尼指数最小的<br>
持征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现<br>
结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。<br>
（3）对两个子结点递归地调用 ( 1 )， (2) , 直至满足停止条件。<br>
（4）生成 CART 决策树。</p>
</blockquote>
<h3 id="6-2-CART的剪枝">6.2 CART的剪枝</h3>
<p><strong>步骤：</strong></p>
<ol>
<li>对生成算法产生的决策树<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>底端开始不断剪枝，直到<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>的根结点，形成子树序列<img src="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/></li>
<li>通过交叉验证法，在独立的验证数据集上对字数序列进行测试，选择最优子书。</li>
</ol>
<h4 id="6-2-1-剪枝成一个子树序列">6.2.1 剪枝成一个子树序列</h4>
<p>子树的损失函数为</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%28T%29%3DC(T)%2B%5Calpha%7CT%7C%5Cnonumber%0A" /></p><ul>
<li>
<p><img src="https://math.now.sh?inline=C%28T%29%3D%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20N_%7Bt%7D%5Cleft(1-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Cleft(%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%5Cright)%5E%7B2%7D%5Cright)%2C%7CT%7C" style="display:inline-block;margin: 0;"/> 是叶结点个数，<img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"/> 是类别个数</p>
</li>
<li>
<p>定义推导同(5)-(8)</p>
</li>
<li>
<p>对固定<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>，<strong>唯一存在</strong>最优子树<img src="https://math.now.sh?inline=T_%7B%5Calpha%7D" style="display:inline-block;margin: 0;"/>，使得损失函数<img src="https://math.now.sh?inline=C_%7B%5Calpha%7D%28T%29" style="display:inline-block;margin: 0;"/>最小。</p>
<ul>
<li>此处“最优”的意义是指使得损失函数最小</li>
<li><img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>越大，最优子树<img src="https://math.now.sh?inline=T_%7B%5Calpha%7D" style="display:inline-block;margin: 0;"/>越小。当 <img src="https://math.now.sh?inline=%5Calpha%20%5Crightarrow%20%5Cinfty" style="display:inline-block;margin: 0;"/> 时，叶结点不断被剪，根结点组成的单结点树是最优的。</li>
</ul>
</li>
</ul>
<p>对一个整体树<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>，它的子树是<strong>有限个</strong>的。因此，对一个连续参数<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>，我们得到了最优子树<img src="https://math.now.sh?inline=T'%28%5Calpha%29" style="display:inline-block;margin: 0;"/>。<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>不断增大，在增大到**跳跃点<img src="https://math.now.sh?inline=%5Calpha'" style="display:inline-block;margin: 0;"/>**之前，<img src="https://math.now.sh?inline=T'%28%5Calpha%29" style="display:inline-block;margin: 0;"/>依然是最优子树。即<img src="https://math.now.sh?inline=T'%28%5Calpha%29%3DT'(%5Calpha%2B%5CDelta%5Calpha)" style="display:inline-block;margin: 0;"/>。再跳跃点之后，易知最优子树<img src="https://math.now.sh?inline=T'%28%5Calpha%E2%80%99%29%5Cin%20T'(%5Calpha)" style="display:inline-block;margin: 0;"/>。</p>
<p>上面可以表述为：将 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/> 从小增大, <img src="https://math.now.sh?inline=0%3D%5Calpha_%7B0%7D%3C" style="display:inline-block;margin: 0;"/><img src="https://math.now.sh?inline=%5Calpha_%7B1%7D%3C%5Ccdots%3C%5Calpha_%7Bn%7D%3C%2B%5Cinfty%2C" style="display:inline-block;margin: 0;"/> 产生一系列的区间 <img src="https://math.now.sh?inline=%5Cleft%5B%5Calpha_%7Bi%7D%2C%20%5Calpha_%7Bi%2B1%7D%5Cright%29%2C%20i%3D0%2C1%2C%20%5Ccdots%2C%20n%20%3B" style="display:inline-block;margin: 0;"/> 剪枝得到的子树序列对应着区间 <img src="https://math.now.sh?inline=%5Calpha%20%5Cin%5Cleft%5B%5Calpha_%7Bi%7D%2C%20%5Calpha_%7Bi%2B1%7D%5Cright%29%2C%20i%3D0%2C1%2C%20%5Ccdots%2C%20n" style="display:inline-block;margin: 0;"/> 的最优子树序列 <img src="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D%2C" style="display:inline-block;margin: 0;"/> <strong>序列中的子树是嵌套的</strong>。</p>
<p>具体来说，从整体树<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=%5Calpha%3D0" style="display:inline-block;margin: 0;"/>开始剪枝。对<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>内的任意内部结点<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>：</p>
<p>以<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>为单结点树的损失函数为</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%28t%29%3DC(t)%2B%5Calpha%0A" /></p><p>以 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 为根结点的子树 <img src="https://math.now.sh?inline=T_%7Bt%7D" style="display:inline-block;margin: 0;"/> 的损失函数为</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%5Cleft%28T_%7Bt%7D%5Cright%29%3DC%5Cleft(T_%7Bt%7D%5Cright)%2B%5Calpha%5Cleft%7CT_%7Bt%7D%5Cright%7C%0A" /></p><p>当 <img src="https://math.now.sh?inline=%5Calpha%3D0" style="display:inline-block;margin: 0;"/> 及 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/> 充分小时，有不等式</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%5Cleft%28T_%7Bt%7D%5Cright%29%3CC_%7B%5Calpha%7D(t)%0A" /></p><p>当 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/> 增大时，在某一 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/> 有</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Balign%7D%0AC_%7B%5Calpha%7D%5Cleft%28T_%7Bt%7D%5Cright%29%20%26%3DC_%7B%5Calpha%7D(t)%5C%5C%0A%5Calpha%20%26%3D%5Cfrac%7BC(t)-C%5Cleft(T_%7Bt%7D%5Cright)%7D%7B%5Cleft%7CT_%7Bt%7D%5Cright%7C-1%7D%0A%5Cend%7Balign%7D%0A" /></p><p>**此时<img src="https://math.now.sh?inline=T_t" style="display:inline-block;margin: 0;"/>和<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>有相同的损失函数值，因为<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>的结点更少，所以取<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>，剪去以 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 为根结点的子树 <img src="https://math.now.sh?inline=T_%7Bt%7D" style="display:inline-block;margin: 0;"/> **</p>
<p>根据这个性质，我们可以找到系列区间以及对应的最优子树序列</p>
<p>对<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>的每一个内部节点<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>，计算</p>
<p style=""><img src="https://math.now.sh?from=g%28t%29%3D%5Cfrac%7BC(t)-C%5Cleft(T_%7Bt%7D%5Cright)%7D%7B%5Cleft%7CT_%7Bt%7D%5Cright%7C-1%7D%0A" /></p><p><img src="https://math.now.sh?inline=g%28t%29" style="display:inline-block;margin: 0;"/>表示剪枝后整体损失函数减小的程度。在 <img src="https://math.now.sh?inline=T_%7B0%7D" style="display:inline-block;margin: 0;"/> 中剪去 <img src="https://math.now.sh?inline=g%28t%29" style="display:inline-block;margin: 0;"/> 最小的 <img src="https://math.now.sh?inline=T_%7Bt%7D%2C" style="display:inline-block;margin: 0;"/> 将得到的子树作为 <img src="https://math.now.sh?inline=T_%7B1%7D%2C" style="display:inline-block;margin: 0;"/> 同时将最小的 <img src="https://math.now.sh?inline=g%28t%29" style="display:inline-block;margin: 0;"/> 设为 <img src="https://math.now.sh?inline=%5Calpha_%7B1%7D" style="display:inline-block;margin: 0;"/>。$ T_{1}$ 为区间 <img src="https://math.now.sh?inline=%5Cleft%5B%5Calpha_%7B1%7D%2C%20%5Calpha_%7B2%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 的最优子树。循环直到得到根结点。</p>
<ul>
<li>在这个过程中，<img src="https://math.now.sh?inline=g%28t%29%3D%5Calpha" style="display:inline-block;margin: 0;"/>是不断增大的？</li>
</ul>
<h4 id="6-2-2-交叉验证">6.2.2 交叉验证</h4>
<p>利用独立的验证数据集，测试子树序列 <img src="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/>中每个子树的平方误差/基尼指数，选择最优决策树<img src="https://math.now.sh?inline=T_%7B%5Calpha%7D" style="display:inline-block;margin: 0;"/>。</p>
<ul>
<li>子树序列 <img src="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/>在剪枝的时候是对应<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>的最优子序列</li>
</ul>
<h4 id="6-2-3-Algorithm">6.2.3 Algorithm</h4>
<blockquote>
<p>算法 5.7 (CART 剪枝算法)<br>
输入: CART 算法生成的决策树 <img src="https://math.now.sh?inline=T_%7B0%7D" style="display:inline-block;margin: 0;"/>;<br>
输出：最优决策树 <img src="https://math.now.sh?inline=T_%7B%5Calpha%20%5Ccirc%7D" style="display:inline-block;margin: 0;"/><br>
(1) 设 <img src="https://math.now.sh?inline=k%3D0%2C%20T%3DT_%7B0%7D" style="display:inline-block;margin: 0;"/> 。<br>
(2) 设 <img src="https://math.now.sh?inline=%5Calpha%3D%2B%5Cinfty" style="display:inline-block;margin: 0;"/> 。<br>
(3) 自下而上地对各内部结点 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 计算 <img src="https://math.now.sh?inline=C%5Cleft%28T_%7Bt%7D%5Cright%29%2C%5Cleft%7CT_%7Bt%7D%5Cright%7C" style="display:inline-block;margin: 0;"/> 以及</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0A%0Ag%28t%29%20%26%3D%5Cfrac%7BC(t)-C%5Cleft(T_%7Bt%7D%5Cright)%7D%7B%5Cleft%7CT_%7Bt%7D%5Cright%7C-1%7D%20%5C%5C%0A%5Calpha%20%26%3D%5Cmin%20(%5Calpha%2C%20g(t))%0A%5Cend%7Baligned%7D%0A" /></p><p>这里, <img src="https://math.now.sh?inline=T_%7Bt%7D" style="display:inline-block;margin: 0;"/> 表示以 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 为根结点的子树, <img src="https://math.now.sh?inline=C%5Cleft%28T_%7Bt%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 是对训练数据的预测误差, <img src="https://math.now.sh?inline=%5Cleft%7CT_%7Bt%7D%5Cright%7C" style="display:inline-block;margin: 0;"/> 是 <img src="https://math.now.sh?inline=T_%7Bt%7D" style="display:inline-block;margin: 0;"/>的叶结点个数。<br>
(4) 对 <img src="https://math.now.sh?inline=g%28t%29%3D%5Calpha" style="display:inline-block;margin: 0;"/> 的内部结点 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 进行剪枝，并对叶结点 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 以多数表决法决定其类，得到树 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 。<br>
(5) 设 <img src="https://math.now.sh?inline=k%3Dk%2B1%2C%20%5Calpha_%7Bk%7D%3D%5Calpha%2C%20T_%7Bk%7D%3DT_%7B%5Ccirc%7D" style="display:inline-block;margin: 0;"/><br>
(6) 如果 <img src="https://math.now.sh?inline=T_%7Bk%7D" style="display:inline-block;margin: 0;"/> 不是由根结点及两个叶结点构成的树，则回到步骤 (2)<img src="https://math.now.sh?inline=%3B" style="display:inline-block;margin: 0;"/> 否则令 <img src="https://math.now.sh?inline=T_%7Bk%7D%3DT_%7Bn%7D" style="display:inline-block;margin: 0;"/><br>
(7) 采用交义验证法在子树序列 <img src="https://math.now.sh?inline=T_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D" style="display:inline-block;margin: 0;"/> 中选取最优子树 <img src="https://math.now.sh?inline=T_%7B%5Calpha%5E%7B%5Ccirc%7D%7D" style="display:inline-block;margin: 0;"/></p>
</blockquote>
<h2 id="7-Question">7. Question</h2>
<ol>
<li>为什么<img src="https://math.now.sh?inline=C%28T%29" style="display:inline-block;margin: 0;"/>能表示模型对训练数据的预测误差</li>
<li><s>正则化的极大似然估计？</s></li>
<li><s><strong>1.2</strong>中的’构成一个条件概率分布’，不是叶结点咋办？</s></li>
<li><s>6.1.1 启发式算法？</s></li>
<li>CART决策时有没有可能对一个特征二叉再接个二叉，成<img src="https://math.now.sh?inline=2%5E2" style="display:inline-block;margin: 0;"/>个叉？</li>
</ol>
<h2 id="8-Code">8. Code</h2>
<h3 id="8-1-ID3-create-test-visualization">8.1 ID3: create, test, visualization</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">CARTClassification</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">CARTRegression</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ID3_create</span>(<span class="params">self, train_set, features, labels, tol=[<span class="number">0.1</span>, <span class="number">2</span>], visible=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        create ID3 tree</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray. m: samples, n: features</span></span><br><span class="line"><span class="string">        :param features: n size vector</span></span><br><span class="line"><span class="string">        :param labels: m size ndarray</span></span><br><span class="line"><span class="string">        :param tol: tolerate for pre-pruning</span></span><br><span class="line"><span class="string">        :return: ID3 tree in dict type</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        three conditions to stop iteration:</span></span><br><span class="line"><span class="string">          1. all labelss are the same</span></span><br><span class="line"><span class="string">          2. no feature</span></span><br><span class="line"><span class="string">          3. info_gain &lt; tol[0], samples &gt; tol[1]. pre-pruning</span></span><br><span class="line"><span class="string">          4. same training values but different labels</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        train_set = np.array(train_set)</span><br><span class="line">        labels = np.array(labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(np.unique(labels)) == <span class="number">1</span>:  <span class="comment"># condition 1</span></span><br><span class="line">            <span class="keyword">return</span> labels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> train_set.shape[<span class="number">1</span>] == <span class="number">0</span>:  <span class="comment"># condition 2</span></span><br><span class="line">            <span class="keyword">return</span> np.sort(labels)[-<span class="number">1</span>]  <span class="comment"># return the most frequency value</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># condition 3 &amp; 4</span></span><br><span class="line">        <span class="comment"># not finished</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get best feature</span></span><br><span class="line">        best_feature_index, best_info_gain = self.ID3_best_feature(train_set, labels)</span><br><span class="line">        best_feature_name = features[best_feature_index]</span><br><span class="line">        print(<span class="string">&#x27;best selected feature is &#x27;</span>, best_feature_name, <span class="string">&#x27;, its information gain is &#x27;</span>, best_info_gain)</span><br><span class="line"></span><br><span class="line">        ID3Tree = &#123;best_feature_name: &#123;&#125;&#125;  <span class="comment"># return feature name as a dict key</span></span><br><span class="line">        <span class="comment"># return unique values under the feature and as the node(key)</span></span><br><span class="line">        tree_nodes = np.unique(train_set.T[best_feature_index])</span><br><span class="line">        <span class="comment"># small feature set for dealing feature set depending on its index</span></span><br><span class="line">        features = np.delete(features, best_feature_index)</span><br><span class="line">        <span class="comment"># iteration in these nodes</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> tree_nodes:</span><br><span class="line">            train_sample_index = train_set.T[best_feature_index] == node</span><br><span class="line">            node_labels = labels[train_sample_index]</span><br><span class="line">            <span class="comment"># small train set with node feature&#x27;s column equal node value</span></span><br><span class="line">            node_train_set = self.spilt_dataset(train_set, best_feature_index, node)</span><br><span class="line">            <span class="comment"># small train set without node feature</span></span><br><span class="line">            node_train_set = np.delete(node_train_set, best_feature_index, axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># iteration</span></span><br><span class="line">            ID3Tree[best_feature_name][node] = self.ID3_create(node_train_set, features, node_labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> visible <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            treePlotter.ID3_Tree(ID3Tree)</span><br><span class="line">        <span class="keyword">return</span> ID3Tree</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">array</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        calculate bit entropy</span></span><br><span class="line"><span class="string">        :param array: 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: entropy in bit</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        count_array = np.unique(array, return_counts=<span class="literal">True</span>)[<span class="number">1</span>]  <span class="comment"># unique values and its occurrences</span></span><br><span class="line">        probability = count_array / array.size  <span class="comment"># probability of values</span></span><br><span class="line">        h_p = np.dot(-probability, np.log2(probability))  <span class="comment"># entropy</span></span><br><span class="line">        <span class="keyword">return</span> h_p</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conditional_entropy</span>(<span class="params">Y, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        get conditional entropy H(Y|X)</span></span><br><span class="line"><span class="string">        :param Y: random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :param X: given random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: conditional entropy of Y given X, H(Y|X)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        Y = np.array(Y)</span><br><span class="line">        X = np.array(X)</span><br><span class="line">        hY_X = <span class="number">0</span>  <span class="comment"># initialization</span></span><br><span class="line">        X_value, X_count = np.unique(X, return_counts=<span class="literal">True</span>)  <span class="comment"># unique values and its occurrences</span></span><br><span class="line">        <span class="keyword">for</span> xi <span class="keyword">in</span> X_value:</span><br><span class="line">            index = np.argwhere(X == xi)  <span class="comment"># get index of X=xi</span></span><br><span class="line">            p_xi = index.size / X.size  <span class="comment"># P(X=xi)</span></span><br><span class="line">            Yi = Y[index]  <span class="comment"># get yi given xi</span></span><br><span class="line">            hYi_xi = DecisionTree.entropy(np.array(Yi))  <span class="comment"># H(Y|X=xi)</span></span><br><span class="line">            hY_X += p_xi * hYi_xi</span><br><span class="line">        <span class="keyword">return</span> hY_X</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain</span>(<span class="params">Y, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        get information gain G(Y,X)</span></span><br><span class="line"><span class="string">        :param Y: random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :param X: given random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: information gain of Y given X, G(Y|X)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> DecisionTree.entropy(Y) - DecisionTree.conditional_entropy(Y, X)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spilt_dataset</span>(<span class="params">dataset, colume, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dataset with small samples</span></span><br><span class="line"><span class="string">        :param dataset: m*n ndarray</span></span><br><span class="line"><span class="string">        :param colume:  axis</span></span><br><span class="line"><span class="string">        :param value: compared value</span></span><br><span class="line"><span class="string">        :return: l*n ndarray, l&lt;m</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dataset = pd.DataFrame(dataset)</span><br><span class="line">        df = dataset[dataset[colume] == value]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.array(df)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ID3_best_feature</span>(<span class="params">train_set, labels</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        return the feature with the highest infomation gain</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray. m: samples, n: features</span></span><br><span class="line"><span class="string">        :param labels: m size ndarray.</span></span><br><span class="line"><span class="string">        :return: best feature index and its infomation gain</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        features = train_set.shape[<span class="number">1</span>]  <span class="comment"># number of features</span></span><br><span class="line">        tmp = np.ones(features) * -<span class="number">1</span>  <span class="comment"># store info gain</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(features):  <span class="comment"># calculate info gain of each features</span></span><br><span class="line">            feature_list = train_set.T[i]</span><br><span class="line">            gain = DecisionTree.info_gain(labels, feature_list)</span><br><span class="line">            tmp[i] = gain</span><br><span class="line">            print(<span class="string">&quot;the info gain of %d th feature in ID3 is: %.3f&quot;</span> % (i, gain))</span><br><span class="line">        best_feature = np.argmax(tmp)</span><br><span class="line">        best_info_gain = tmp[best_feature]</span><br><span class="line">        <span class="keyword">return</span> best_feature, best_info_gain</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">tree, sample, features</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param tree: dict</span></span><br><span class="line"><span class="string">        :param sample: 1-d ndarray</span></span><br><span class="line"><span class="string">        :param features: 1-d ndarray</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        first_str = <span class="built_in">list</span>(tree.keys())[<span class="number">0</span>] <span class="comment"># root name</span></span><br><span class="line">        small_tree = tree[first_str] <span class="comment">#</span></span><br><span class="line">        feature_index = features.index(first_str)</span><br><span class="line">        label = <span class="string">&#x27;None&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> small_tree.keys():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(sample[feature_index]) == key:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">type</span>(small_tree[key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:</span><br><span class="line">                    label = DecisionTree.classify(small_tree[key], sample, features)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    label = small_tree[key]</span><br><span class="line">        <span class="keyword">return</span> label</span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<h2 id="9-Reference">9. Reference</h2>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/93667566">分类与回归树（classification and regression tree，CART）之回归</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/101721467">回归树</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/93667566">分类与回归树（classification and regression tree，CART）之回归</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Erikfather/Decision_tree-python">Erikfather/Decision_tree-python</a></p>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/11/27/%5BSLM-4%5D%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%20Naive%20Bayes/" rel="prev" title="">
                  <i class="fa fa-chevron-left"></i> 
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/12/11/%5BSLM-7%5D%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%20SVM/" rel="next" title="">
                   <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CHongfeng Ling 凌崇锋</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  







  





  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
