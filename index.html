<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","version":"8.1.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Chongfeng Ling">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Chongfeng Ling">
<meta property="og:description" content="description">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="CHongfeng Ling 凌崇锋">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>
<title>Chongfeng Ling</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chongfeng Ling</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">subtitle</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">CHongfeng Ling 凌崇锋</p>
  <div class="site-description" itemprop="description">description</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/19/blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/19/blog/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-12-19 23:59:20" itemprop="dateCreated datePublished" datetime="2020-12-19T23:59:20+08:00">2020-12-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-20 00:04:19" itemprop="dateModified" datetime="2020-12-20T00:04:19+08:00">2020-12-20</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1>感知机</h1>
<p>感知机（perceptron）是<strong>二类线性分类</strong>模型。</p>
<h2 id="1-Model">1. Model</h2>
<blockquote>
<p>定义 2.1 (感知机) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/>假设输入空间（特征空间 ) 是 <img src="https://math.now.sh?inline=%5Cmathcal%7BX%7D%20%5Csubseteq%20%5Cmathbf%7BR%7D%5E%7Bn%7D%2C" style="display:inline-block;margin: 0;"/> 输出空间是 <img src="https://math.now.sh?inline=%5Cmathcal%7BY%7D%3D%5C%7B%2B1%2C-1%5C%7D_%7B%5Ccirc%7D" style="display:inline-block;margin: 0;"/> 输入 <img src="https://math.now.sh?inline=x%20%5Cin%20%5Cmathcal%7BX%7D" style="display:inline-block;margin: 0;"/> 表示实例的特征向量，对应于输入空间 ( 特征空间 <img src="https://math.now.sh?inline=%29" style="display:inline-block;margin: 0;"/> 的点; 输出 <img src="https://math.now.sh?inline=y%20%5Cin%20%5Cmathcal%7BY%7D" style="display:inline-block;margin: 0;"/> 表示实例的类别。由输入空间到输出空间的如下函数:</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D(w%20%5Ccdot%20x%2Bb)%0A" /></p><p>称为感知机。其中， <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"/> 和 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"/> 为感知机模型参数, <img src="https://math.now.sh?inline=w%20%5Cin%20%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/> 叫作权值（weight ) 或权值向量（weight vector) <img src="https://math.now.sh?inline=%2C%20b%20%5Cin%20%5Cmathbf%7BR%7D" style="display:inline-block;margin: 0;"/> 叫作偏置 ( bias <img src="https://math.now.sh?inline=%29%2C%20w%20%5Ccdot%20x" style="display:inline-block;margin: 0;"/> 表示 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"/> 和 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的内积。sign 是符号 函数，即</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7Bsign%7D%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%0A%2B1%2C%20%26%20x%20%5Cgeqslant%200%20%5C%5C%0A-1%2C%20%26%20x%3C0%0A%5Cend%7Barray%7D%5Cright.%0A" /></p></blockquote>
<ul>
<li>
<p>假设空间：特征空间中的所有线性分类模型，即函数集合<img src="https://math.now.sh?inline=%5C%7Bf%7Cf%28x%29%3Dw%20%5Ccdot%20x%2Bb%5C%7D" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>几何解释：线性方程<img src="https://math.now.sh?inline=w%20%5Ccdot%20x%2Bb%3D0" style="display:inline-block;margin: 0;"/>是特征空间<img src="https://math.now.sh?inline=%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/>的一个超平面<img src="https://math.now.sh?inline=%5Cmathbf%7BS%7D" style="display:inline-block;margin: 0;"/>，把特征空间分成2个部分，使得特征向量分别划入正负两类。<img src="https://math.now.sh?inline=%5Cmathbf%7BS%7D" style="display:inline-block;margin: 0;"/>也叫分离超平面（separating hyperplane)</p>
<ul>
<li>超平面<img src="https://math.now.sh?inline=%5Cmathbf%7BS%7D%5Csubseteq%20%5Cmathbf%7BR%7D%5E%7Bn-1%7D" style="display:inline-block;margin: 0;"/>并且截距为0。在这需要把b当成特征而不是截距才说的通。</li>
</ul>
</li>
<li>
<p>例子：</p>
<ul>
<li>通过<img src="https://math.now.sh?inline=%5C%7B%E6%88%BF%E5%B1%8B%E9%9D%A2%E7%A7%AF%EF%BC%8C%E6%88%BF%E9%BE%84%EF%BC%8C...%5C%7D" style="display:inline-block;margin: 0;"/>来判断房子总价是否高于价格<img src="https://math.now.sh?inline=a" style="display:inline-block;margin: 0;"/>
<ul>
<li>假设数据集线性可分</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-Strategy">2. Strategy</h2>
<h3 id="2-1-数据可分性">2.1 数据可分性</h3>
<blockquote>
<p>定义 2.2 (数据集的线性可分性) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 给定一个数据集</p>
<p style=""><img src="https://math.now.sh?from=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%0A" /></p><p>其中， <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20y_%7Bi%7D%20%5Cin%20%5Cmathcal%7BY%7D%3D%5C%7B%2B1%2C-1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%2C" style="display:inline-block;margin: 0;"/> 如果存在某个超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=w%20%5Ccdot%20x%2Bb%3D0%0A" /></p><p>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有<br>
<img src="https://math.now.sh?inline=y_%7Bi%7D%3D%2B1" style="display:inline-block;margin: 0;"/> 的实例 <img src="https://math.now.sh?inline=i%2C" style="display:inline-block;margin: 0;"/> 有 <img src="https://math.now.sh?inline=w%20%5Ccdot%20x_%7Bi%7D%2Bb%3E0%2C" style="display:inline-block;margin: 0;"/> 对所有 <img src="https://math.now.sh?inline=y_%7Bi%7D%3D-1" style="display:inline-block;margin: 0;"/> 的实例 <img src="https://math.now.sh?inline=i%2C" style="display:inline-block;margin: 0;"/> 有 <img src="https://math.now.sh?inline=w%20%5Ccdot%20x_%7Bi%7D%2Bb%3C0%2C" style="display:inline-block;margin: 0;"/> 则 称数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为线性可分数据集（linearly separable data set ) ; 否则，称数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 线性<br>
不可分。</p>
</blockquote>
<ul>
<li>存在超平面$\mathbf{S}\Longrightarrow $数据集线性可分</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/19/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/19/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-12-19 23:09:08" itemprop="dateCreated datePublished" datetime="2020-12-19T23:09:08+08:00">2020-12-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-20 00:36:12" itemprop="dateModified" datetime="2020-12-20T00:36:12+08:00">2020-12-20</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a> test <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>.</p>
<h2 id="Quick-Start">Quick Start</h2>
<h3 id="Create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/11/[SLM-7]%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%20SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/11/%5BSLM-7%5D%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%20SVM/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-12-11 17:06:20" itemprop="dateCreated datePublished" datetime="2020-12-11T17:06:20+08:00">2020-12-11</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-20 01:26:52" itemprop="dateModified" datetime="2020-12-20T01:26:52+08:00">2020-12-20</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1>支持向量机 Support Vector Machines</h1>
<p>支持向量机(Support Vector Machines)是一种<strong>二元分类</strong>模型。<strong>基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。<strong>基本模型是</strong>特征空间上的间隔最大</strong>的线性分类器，区别于感知机。学习策略为间隔最大化，可化为求解凸二次规划convex quadratic programming。学习算法为求解凸二次规划的最优算法。</p>
<h2 id="1-线性可分支持向量机与硬间隔最大化">1. 线性可分支持向量机与硬间隔最大化</h2>
<h3 id="1-1-Model">1.1 Model</h3>
<blockquote>
<p>定义 7.1 (线性可分支持向量机 ) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 给定<strong>线性可分</strong>训练数据集</p>
<p style=""><img src="https://math.now.sh?from=T%3D%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%2C%5Cnonumber%5C%5C%0Ax_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20y_%7Bi%7D%20%5Cin%20%5Cmathcal%7BY%7D%3D%5C%7B%2B1%2C-1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A" /></p><p>通过<strong>间隔最大化或等价地求解相应的凸二次规划问题</strong>学习得到的分离超平面为</p>
<p style=""><img src="https://math.now.sh?from=w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%3D0%0A" /></p><p>以及相应的分类决策函数</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%5Cright)%0A" /></p><p>称为线性可分支持向量机。</p>
</blockquote>
<ul>
<li>对线性可分数据集，存在无穷超平面分离数据。感知机用误分类最小策略，得到的解无穷多个。SVM用间隔最大化策略，得到的解唯一。</li>
</ul>
<h3 id="1-2-间隔">1.2 间隔</h3>
<ol>
<li>
<p>点到分离超平面的远近表示对分类预测的确信程度。</p>
</li>
<li>
<p><img src="https://math.now.sh?inline=y_%7Bpre%7D" style="display:inline-block;margin: 0;"/>与<img src="https://math.now.sh?inline=%5Chat%7By%7D" style="display:inline-block;margin: 0;"/>的符号一致与否表示分类预测的准确性。</p>
</li>
</ol>
<h4 id="1-2-1-函数间隔-Functional-Margin">1.2.1 函数间隔 Functional Margin</h4>
<blockquote>
<p>定义 7.2 (函数间隔) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 对于给定的训练数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 和超平面 <img src="https://math.now.sh?inline=%28w%2C%20b%29%2C" style="display:inline-block;margin: 0;"/> 定义超平面<img src="https://math.now.sh?inline=%28w%2C%20b%29" style="display:inline-block;margin: 0;"/> <strong>关于样本点 <img src="https://math.now.sh?inline=%5Cleft%28x_%7Bi%7D%2C%20y_%7Bi%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 的函数间隔</strong>为</p>
<p style=""><img src="https://math.now.sh?from=%5Chat%7B%5Cgamma%7D_%7Bi%7D%3Dy_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%0A" /></p><p>定义超平面 <img src="https://math.now.sh?inline=%28w%2C%20b%29" style="display:inline-block;margin: 0;"/> <strong>关于训练数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 的函数间隔</strong>为超平面 <img src="https://math.now.sh?inline=%28w%2C%20b%29" style="display:inline-block;margin: 0;"/> 对于 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 中所有样本点 <img src="https://math.now.sh?inline=%5Cleft%28x_%7Bi%7D%2C%20y_%7Bi%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 的函数间隔之最小值，即</p>
<p style=""><img src="https://math.now.sh?from=%5Chat%7B%5Cgamma%7D%3D%5Cmin%20_%7Bi%3D1%2C%20%5Ccdots%2C%20N%7D%20%5Chat%7B%5Cgamma%7D_%7Bi%7D%0A" /></p></blockquote>
<ul>
<li>
<p>(3)表示分类的准确性与准确程度</p>
</li>
<li>
<p>对超平面<img src="https://math.now.sh?inline=w%20%5Ccdot%20x%2Bb%3D0" style="display:inline-block;margin: 0;"/>，成倍的改变<img src="https://math.now.sh?inline=w%2C%5C%3Bb" style="display:inline-block;margin: 0;"/>不会改变该平面，但是会成倍的改变函数间隔，且<strong>倍数相等</strong></p>
</li>
</ul>
<h4 id="1-2-2-几何间隔-Geometric-Margin">1.2.2 几何间隔 Geometric Margin</h4>
<p>对分离超平面<img src="https://math.now.sh?inline=w%20%5Ccdot%20x%2Bb%3D0" style="display:inline-block;margin: 0;"/>的法向量<img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"/>进行正规化，使得<img src="https://math.now.sh?inline=%7C%7Cw'%7C%7C%3D%5Cfrac%7Bw%7D%7B%5C%7Cw%5C%7C%7D" style="display:inline-block;margin: 0;"/>（同时对<img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"/>也是）。</p>
<blockquote>
<p>定义 7.3 (几何间隔) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 对于给定的训练数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 和超平面 <img src="https://math.now.sh?inline=%28w%2C%20b%29%2C" style="display:inline-block;margin: 0;"/> 定义超平面<img src="https://math.now.sh?inline=%28w%2C%20b%29" style="display:inline-block;margin: 0;"/> 关于样本,点 <img src="https://math.now.sh?inline=%5Cleft%28x_%7Bi%7D%2C%20y_%7Bi%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 的几何间隔为</p>
<p style=""><img src="https://math.now.sh?from=%5Cgamma_%7Bi%7D%3Dy_%7Bi%7D%5Cleft%28%5Cfrac%7Bw%7D%7B%5C%7Cw%5C%7C%7D%20%5Ccdot%20x_%7Bi%7D%2B%5Cfrac%7Bb%7D%7B%5C%7Cw%5C%7C%7D%5Cright%29%0A" /></p><p>定义超平面 <img src="https://math.now.sh?inline=%28w%2C%20b%29" style="display:inline-block;margin: 0;"/> 对于训练数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 的几何间隔为超平面 <img src="https://math.now.sh?inline=%28w%2C%20b%29" style="display:inline-block;margin: 0;"/> 关千 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 中所有样本点 <img src="https://math.now.sh?inline=%5Cleft%28x_%7Bi%7D%2C%20y_%7Bi%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 的几何间隔之最小值，即</p>
<p style=""><img src="https://math.now.sh?from=%5Cgamma%3D%5Cmin%20_%7Bi%3D1%2C%20%5Ccdots%2C%20N%7D%20%5Cgamma_%7Bi%7D%0A" /></p></blockquote>
<ul>
<li>几何间隔不随参数的改变而改变</li>
</ul>
<h3 id="1-3-间隔最大化">1.3 间隔最大化</h3>
<ul>
<li>对线性可分数据集，线性可分分离超平面有无穷多个（即感知机），但几何间隔最大的分离超平面唯一。</li>
<li>对线性可分训练集，间隔最大化又称硬间隔最大化。</li>
<li>间隔最大化，即以充分大的确信度，对训练数据进行分类；也就是说，在正负实例分开的同时，对离超平面最近的点也能有足够大的确信度。</li>
</ul>
<h4 id="1-3-1-Algorithm">1.3.1 Algorithm</h4>
<p>由几何间隔的定义可知，硬间隔最大化可以表示为</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmax%20_%7Bw%2C%20b%7D%20%26%20%5Cgamma%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20y_%7Bi%7D%5Cleft%28%5Cfrac%7Bw%7D%7B%5C%7Cw%5C%7C%7D%20%5Ccdot%20x_%7Bi%7D%2B%5Cfrac%7Bb%7D%7B%5C%7Cw%5C%7C%7D%5Cright%29%20%5Cgeqslant%20%5Cgamma%3E0%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><p>由几何间隔和函数间隔的定义，(7)转化成</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmax%20_%7Bu%20%5Ccdot%20b%7D%26%20%5Cfrac%7B%5Chat%7B%5Cgamma%7D%7D%7B%5C%7Cw%5C%7C%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%20%5Cgeqslant%20%5Chat%7B%5Cgamma%7D%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><p>我们要求<img src="https://math.now.sh?inline=w%2C%5C%3Bb" style="display:inline-block;margin: 0;"/>，使得<img src="https://math.now.sh?inline=%5Cfrac%7B%5Chat%7B%5Cgamma%7D%7D%7B%5C%7Cw%5C%7C%7D" style="display:inline-block;margin: 0;"/>最大化，在这个目标函数与约束条件中，<img src="https://math.now.sh?inline=%5Chat%7By%7D" style="display:inline-block;margin: 0;"/>的取值对最后的超平面没有影响。于是我们设<img src="https://math.now.sh?inline=%5Chat%7By%7D%3D1" style="display:inline-block;margin: 0;"/>，当成单位1。同时最大化<img src="https://math.now.sh?inline=%5Cfrac%7B1%7D%7B%5C%7Cw%5C%7C%7D" style="display:inline-block;margin: 0;"/> 和最小化 <img src="https://math.now.sh?inline=%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D" style="display:inline-block;margin: 0;"/>等价。(8)转化成</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmin%20_%7Bw%2C%20b%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29-1%20%5Cgeqslant%200%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><ul>
<li>(9)为凸二次规划问题convex quadratic programming</li>
</ul>
<p>求出(9)的解<img src="https://math.now.sh?inline=w%5E*%2C%5C%3Bb%5E*" style="display:inline-block;margin: 0;"/>，我们可以得出最大间隔分离超平面<img src="https://math.now.sh?inline=w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%3D0" style="display:inline-block;margin: 0;"/> 及分类决策函数<img src="https://math.now.sh?inline=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%5Cright)" style="display:inline-block;margin: 0;"/>，即线性可分向量机模型。</p>
<blockquote>
<p>算法 7.1 (线性可分支持向量机学习算法————最大间隔法)<br>
输入: 线性可分训练数据集 <img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=x_%7Bi%7D%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20y_%7Bi%7D%20%5Cin%20%5Cmathcal%7BY%7D%3D%5C%7B-1%2C%2B1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N" style="display:inline-block;margin: 0;"/><br>
输出：最大间隔分离超平面和分类决策函数。</p>
<p>（1）构造并求解约束最优化问题:</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmin%20_%7Bw%2C%20b%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29-1%20%5Cgeqslant%200%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><p>求得最优解 <img src="https://math.now.sh?inline=w%5E%7B*%7D%2C%20b%5E%7B*%7D" style="display:inline-block;margin: 0;"/> 。<br>
（2）由此得到分离超平面:</p>
<p style=""><img src="https://math.now.sh?from=w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%3D0%0A" /></p><p>分类决策函数</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%5Cright)%0A" /></p></blockquote>
<h4 id="1-3-2-解的存在性与唯一性">1.3.2 解的存在性与唯一性</h4>
<blockquote>
<p>定理 7.1 (最大间隔分离超平面的存在唯一性) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 若训练数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</p>
</blockquote>
<h4 id="1-3-3-支持向量-Support-Vector-间隔边界">1.3.3 支持向量 Support Vector &amp; 间隔边界</h4>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png" alt="image-20201217151120228"></p>
<p>对线性可分数据集：</p>
<ul>
<li>支持向量：训练样本点中与分离超平面距离最近的<strong>实例们<img src="https://math.now.sh?inline=%28x_i%2C%5C%3Bb_i%29" style="display:inline-block;margin: 0;"/></strong>，使得约束调节(10)取等号。即位于<img src="https://math.now.sh?inline=H_%7B1%2C2%7D%3A%20w%20%5Ccdot%20x%2Bb%3D%5Cpm1" style="display:inline-block;margin: 0;"/>上的两（或更多）点。</li>
<li>间隔边界：<img src="https://math.now.sh?inline=H_1" style="display:inline-block;margin: 0;"/>和<img src="https://math.now.sh?inline=H_2" style="display:inline-block;margin: 0;"/></li>
</ul>
<p><strong>此模型的结果只由这些少数个支持向量决定</strong>，故此得名。</p>
<h3 id="1-4-对偶算法-Dual-Problem">1.4 对偶算法 Dual Problem</h3>
<p>求解最优化问题(10)，我们可以把其当成原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。</p>
<ol>
<li>对偶问题更容易求解。高维甚至无限维的最优化问题难求解。</li>
<li>引入核函数</li>
</ol>
<h4 id="1-4-1-对偶问题的导出">1.4.1 对偶问题的导出</h4>
<p>对于拘束最优化问题(10)</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmin%20_%7Bw%2C%20b%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29-1%20%5Cgeqslant%200%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%5Cnonumber%20%5Ctag%7B10%7D%0A%5Cend%7Barray%7D%0A" /></p><p>的每一个约束条件，我们引入一个拉格朗日乘子 Lagrange multiplier <img src="https://math.now.sh?inline=%5Calpha_%7Bi%7D%20%5Cgeqslant%200" style="display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=i%3D1%2C2%2C%20%5Ccdots%2C%20N" style="display:inline-block;margin: 0;"/>，定义拉格朗日函数 Generalized Lagrange Function，<img src="https://math.now.sh?inline=%5Calpha%3D%5Cleft%28%5Calpha_%7B1%7D%2C%20%5Calpha_%7B2%7D%2C%20%5Ccdots%2C%20%5Calpha_%7BN%7D%5Cright%29%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/> 为拉格朗日乘子向量。</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Balign%7D%0AL%28w%2C%20b%2C%20%5Calpha%29%26%3D%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D(y_%7Bi%7D%5Cleft(w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright)-1)%5Cnonumber%0A%5C%5C%26%3D%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%5Cleft(w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright)%2B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%0A%5Cend%7Balign%7D%0A" /></p><p>带拘束最优化问题(10)可以变成无拘束优化问题(14)</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cmin%20_%7Bw%2C%20b%7D%20%5Cmax%20_%7B%5Clambda%7D%20%5Cmathcal%7BL%7D%28w%2C%20b%2C%20%5Calpha%29%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%5Clambda_%7Bi%7D%20%5Cgeqslant%200%0A%5Cend%7Barray%7D%0A" /></p><p>由强对偶关系，拘束问题(14)可以化为无拘束优化问题</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cmax%20_%7B%5Clambda%7D%20%5Cmin%20_%7Bw%2C%20b%7D%20%5Cmathcal%7BL%7D%28w%2C%20b%2C%20%5Calpha%29%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%5Clambda_%7Bi%7D%20%5Cgeqslant%200%0A%5Cend%7Barray%7D%0A" /></p><p>由上面的拉格朗日（强）对偶性可得，原始问题(10)的对偶问题是极大极小问题(15)</p>
<ul>
<li>详细推导见：【拉格朗日对偶，等价对偶以及KKT条件】（文件没保存，有缘再续）</li>
</ul>
<h4 id="1-4-2-对偶问题的计算">1.4.2 对偶问题的计算</h4>
<p><strong>1.4.2.1: 求<img src="https://math.now.sh?inline=%5Cmin%20_%7Bw%2C%20b%7D%20L%28w%2C%20b%2C%20%5Calpha%29%3D%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%5Cleft(w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright)%2B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D" style="display:inline-block;margin: 0;"/></strong></p>
<p>计算<img src="https://math.now.sh?inline=%5Cmin%20_%7Bw%2C%20b%7D%20L%28w%2C%20b%2C%20%5Calpha%29" style="display:inline-block;margin: 0;"/>对变量<img src="https://math.now.sh?inline=w%2C%5C%3Bb" style="display:inline-block;margin: 0;"/>的偏导为0</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cnabla_%7Bw%7D%20L%28w%2C%20b%2C%20%5Calpha%29%3Dw-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%20x_%7Bi%7D%3D0%20%5C%5C%0A%5Cnabla_%7Bb%7D%20L(w%2C%20b%2C%20%5Calpha)%3D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%3D0%5Cnonumber%5C%5C%0A%5Cend%7Barray%7D%0A" /></p><p>得</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bl%7D%0Aw%26%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%20x_%7Bi%7D%5C%5C%0A%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%26%3D0%0A%5Cend%7Barray%7D%0A" /></p><p>把(16)带入到(13)，得</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0AL%28w%2C%20b%2C%20%5Calpha%29%20%26%3D%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%5Cleft(x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright)-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%5Cleft(%5Cleft(%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bj%7D%20y_%7Bj%7D%20x_%7Bj%7D%5Cright)%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright)%2B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5C%5C%0A%26%3D-%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%5Cleft(x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright)%2B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%0A%5Cend%7Baligned%7D%0A" /></p><p>得到结果</p>
<p style=""><img src="https://math.now.sh?from=%5Cmin%20_%7Bw%2C%20b%7D%20L%28w%2C%20b%2C%20%5Calpha%29%3D-%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%5Cleft(x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright)%2B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%0A" /></p><p><strong>求<img src="https://math.now.sh?inline=%5Cmin%20_%7Bw%2C%20b%7D%20L%28w%2C%20b%2C%20%5Calpha%29%20%5Ctext%20%7B%20%E5%AF%B9%20%7D%20%5Calpha%20%5Ctext%20%7B%20%E7%9A%84%E6%9E%81%E5%A4%A7%20%7D" style="display:inline-block;margin: 0;"/></strong></p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0A%5Cmax%20_%7B%5Calpha%7D%20%26%20-%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%5Cleft%28x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright%29%2B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%3D0%20%5C%5C%0A%26%20%5Calpha_%7Bi%7D%20%5Cgeqslant%200%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Baligned%7D%0A" /></p><p>调整符号，得与原始问题等价的对偶最优化问题</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7B1%7D%0A%5Cmin%20_%7B%5Calpha%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%5Cleft%28x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright%29-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%3D0%20%5C%5C%0A%26%20%5Calpha_%7Bi%7D%20%5Cgeqslant%200%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><ul>
<li>$ \sum_{i=1}^{N} \alpha_{i} y_{i}=0<img src="https://math.now.sh?inline=%E6%84%8F%E5%91%B3%E7%9D%80%E5%BE%88%E5%A4%A7%E9%83%A8%E5%88%86%E7%9A%84" style="display:inline-block;margin: 0;"/>\alpha=0$，即决定超平面的只是很小一部分的支持向量。</li>
</ul>
<h4 id="1-4-3-对偶问题的解">1.4.3 对偶问题的解</h4>
<blockquote>
<p>定理 7.2 设 <img src="https://math.now.sh?inline=%5Calpha%5E%7B*%7D%3D%5Cleft%28%5Calpha_%7B1%7D%5E%7B*%7D%2C%20%5Calpha_%7B2%7D%5E%7B*%7D%2C%20%5Ccdots%2C%20%5Calpha_%7Bl%7D%5E%7B*%7D%5Cright%29%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/> 是对偶最优化问题 (7.22)<img src="https://math.now.sh?inline=%5Csim%287.24%29" style="display:inline-block;margin: 0;"/> 的解，则存在下标 <img src="https://math.now.sh?inline=j%2C" style="display:inline-block;margin: 0;"/> 使得 <img src="https://math.now.sh?inline=%5Calpha_%7Bj%7D%5E%7B*%7D%3E0%2C" style="display:inline-block;margin: 0;"/> 并可按下式求得原始最优化问题 (7.13)<img src="https://math.now.sh?inline=%5Csim%287.14%29" style="display:inline-block;margin: 0;"/> 的解 <img src="https://math.now.sh?inline=w%5E%7B*%7D%2C%20b%5E%7B*%7D" style="display:inline-block;margin: 0;"/> :</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bc%7D%0Aw%5E%7B*%7D%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20x_%7Bi%7D%20%5C%5C%0Ab%5E%7B*%7D%3Dy_%7Bj%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%5Cleft%28x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright%29%0A%5Cend%7Barray%7D%0A" /></p></blockquote>
<p>分离超平面为<img src="https://math.now.sh?inline=%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%5Cleft%28x%20%5Ccdot%20x_%7Bi%7D%5Cright%29%2Bb%5E%7B*%7D%3D0" style="display:inline-block;margin: 0;"/></p>
<p>分类决策函数为<img src="https://math.now.sh?inline=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%5Cleft(x%20%5Ccdot%20x_%7Bi%7D%5Cright)%2Bb%5E%7B*%7D%5Cright)" style="display:inline-block;margin: 0;"/></p>
<h4 id="1-4-4-Algorithm">1.4.4 Algorithm</h4>
<blockquote>
<p>算法 7.2 (线性可分支持向量机学习算法)<br>
输入: 线性可分训练集 <img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%2C" style="display:inline-block;margin: 0;"/> 其中 <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=y_%7Bi%7D%20%5Cin%20%5Cmathcal%7BY%7D%3D%5C%7B-1%2C%2B1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N" style="display:inline-block;margin: 0;"/><br>
输出：分离超平面和分类决策函数。<br>
（1）构造并求解约束最优化问题</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmin%20_%7B%5Calpha%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%5Cleft%28x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright%29-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%3D0%20%5C%5C%0A%26%20%5Calpha_%7Bi%7D%20%5Cgeqslant%200%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><p>求得最优解 <img src="https://math.now.sh?inline=%5Calpha%5E%7B*%7D%3D%5Cleft%28%5Calpha_%7B1%7D%5E%7B*%7D%2C%20%5Calpha_%7B2%7D%5E%7B*%7D%2C%20%5Ccdots%2C%20%5Calpha_%7BN%7D%5E%7B*%7D%5Cright%29%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/> 。<br>
(2) 计算</p>
<p style=""><img src="https://math.now.sh?from=w%5E%7B*%7D%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20x_%7Bi%7D%0A" /></p><p>并选择 <img src="https://math.now.sh?inline=%5Calpha%5E%7B*%7D" style="display:inline-block;margin: 0;"/> 的一个正分量 <img src="https://math.now.sh?inline=%5Calpha_%7Bj%7D%5E%7B*%7D%3E0%2C" style="display:inline-block;margin: 0;"/> 计算</p>
<p style=""><img src="https://math.now.sh?from=b%5E%7B*%7D%3Dy_%7Bj%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%5Cleft%28x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright%29%0A" /></p><p>（3）求得分离超平面</p>
<p style=""><img src="https://math.now.sh?from=w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%3D0%0A" /></p><p>分类决策函数:</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%5Cright)%0A" /></p></blockquote>
<h2 id="2-线性支持向量机与软间隔最大化">2. 线性支持向量机与软间隔最大化</h2>
<p>对线性不可分数据集，存在一些特异点（outlier）使得不等式不能全部满足，即函数间隔大于1。</p>
<p>因此我们引入松弛变量<img src="https://math.now.sh?inline=%5Cxi_i%20%5Cgeq0" style="display:inline-block;margin: 0;"/>，使得函数间隔大于等于<img src="https://math.now.sh?inline=1-%5Cxi_i" style="display:inline-block;margin: 0;"/>。同时对松弛变量付出一个惩罚参数。</p>
<ul>
<li>
<p>约束条件</p>
<p style=""><img src="https://math.now.sh?from=y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%20%5Cgeqslant%201-%5Cxi_%7Bi%7D%0A" /></p></li>
<li>
<p>目标函数</p>
<p style=""><img src="https://math.now.sh?from=%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D%2BC%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Cxi_%7Bi%7D%0A" /></p><ul>
<li><img src="https://math.now.sh?inline=C" style="display:inline-block;margin: 0;"/>为惩罚参数。<img src="https://math.now.sh?inline=C" style="display:inline-block;margin: 0;"/>越大，对误分类的惩罚越大</li>
<li>最小化目标函数，即使<img src="https://math.now.sh?inline=%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D" style="display:inline-block;margin: 0;"/>尽可能小，间隔尽可能大</li>
</ul>
</li>
</ul>
<h3 id="2-1-原始问题">2.1 原始问题</h3>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmin%20_%7Bw%2C%20b%2C%20%5Cxi%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D%2BC%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Cxi_%7Bi%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%20%5Cgeqslant%201-%5Cxi_%7Bi%7D%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%20%5C%5C%0A%26%20%5Cxi_%7Bi%7D%20%5Cgeqslant%200%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><ul>
<li>凸二次规划问题</li>
<li><img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"/>的解唯一</li>
<li><img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"/>的解可能不唯一，而是一个区间</li>
</ul>
<h3 id="2-2-拉格朗日函数">2.2 拉格朗日函数</h3>
<p style=""><img src="https://math.now.sh?from=L%28w%2C%20b%2C%20%5Cxi%2C%20%5Calpha%2C%20%5Cmu%29%20%5Cequiv%20%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D%2BC%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Cxi_%7Bi%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5Cleft(y_%7Bi%7D%5Cleft(w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright)-1%2B%5Cxi_%7Bi%7D%5Cright)-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Cmu_%7Bi%7D%20%5Cxi_%7Bi%7D%5C%5C%0A%5Calpha_i%5Cgeq%200%2C%5C%3B%5Cmu_i%5Cgeq0%0A" /></p><h3 id="2-3-对偶问题">2.3 对偶问题</h3>
<p>原始问题(27)的对偶问题使拉格朗日函数的极大极小问题。</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmin%20_%7B%5Calpha%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%5Cleft%28x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright%29-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%3D0%20%5C%5C%0A%26%200%20%5Cleqslant%20%5Calpha_%7Bi%7D%20%5Cleqslant%20C%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><h3 id="2-4-解的等价性">2.4 解的等价性</h3>
<blockquote>
<p>定理 7.3 设 <img src="https://math.now.sh?inline=%5Calpha%5E%7B*%7D%3D%5Cleft%28%5Calpha_%7B1%7D%5E%7B*%7D%2C%20%5Calpha_%7B2%7D%5E%7B*%7D%2C%20%5Ccdots%2C%20%5Calpha_%7BN%7D%5E%7B*%7D%5Cright%29%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/> 是对偶问题的一个解，若存在 <img src="https://math.now.sh?inline=%5Calpha%5E%7B*%7D" style="display:inline-block;margin: 0;"/> 的一个分量 <img src="https://math.now.sh?inline=%5Calpha_%7Bj%7D%5E%7B*%7D%2C%200%3C%5Calpha_%7Bj%7D%5E%7B*%7D%3CC%2C" style="display:inline-block;margin: 0;"/> 则原始问题的解 <img src="https://math.now.sh?inline=w%5E%7B*%7D%2C%20b%5E%7B*%7D" style="display:inline-block;margin: 0;"/> 可按下式 求得:</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bc%7D%0Aw%5E%7B*%7D%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20x_%7Bi%7D%20%5C%5C%0Ab%5E%7B*%7D%3Dy_%7Bj%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20y_%7Bi%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%5Cleft%28x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright%29%0A%5Cend%7Barray%7D%0A" /></p></blockquote>
<blockquote>
<p>算法 7.3 (线性支持向量机学习算法)<br>
输入: 训练数据集 <img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%2C" style="display:inline-block;margin: 0;"/> 其中， <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=y_%7Bi%7D%20%5Cin%20%5Cmathcal%7BY%7D%3D%5C%7B-1%2C%2B1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N" style="display:inline-block;margin: 0;"/><br>
输出：分离超平面和分类决策函数。<br>
（1）选择惩罚参数 <img src="https://math.now.sh?inline=C%3E0%2C" style="display:inline-block;margin: 0;"/> 构造并求解凸二次规划问题</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmin%20_%7B%5Calpha%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%5Cleft%28x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright%29-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%3D0%20%5C%5C%0A%26%200%20%5Cleqslant%20%5Calpha_%7Bi%7D%20%5Cleqslant%20C%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><p>求得最优解 <img src="https://math.now.sh?inline=%5Calpha%5E%7B*%7D%3D%5Cleft%28%5Calpha_%7B1%7D%5E%7B*%7D%2C%20%5Calpha_%7B2%7D%5E%7B*%7D%2C%20%5Ccdots%2C%20%5Calpha_%7BN%7D%5E%7B*%7D%5Cright%29%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/><br>
（2） 计算 <img src="https://math.now.sh?inline=w%5E%7B*%7D%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20x_%7Bi%7D" style="display:inline-block;margin: 0;"/><br>
选择 <img src="https://math.now.sh?inline=%5Calpha%5E%7B*%7D" style="display:inline-block;margin: 0;"/> 的一个分量 <img src="https://math.now.sh?inline=%5Calpha_%7Bj%7D%5E%7B*%7D" style="display:inline-block;margin: 0;"/> 适合条件 <img src="https://math.now.sh?inline=0%3C%5Calpha_%7Bj%7D%5E%7B*%7D%3CC" style="display:inline-block;margin: 0;"/> ，计算</p>
<p style=""><img src="https://math.now.sh?from=b%5E%7B*%7D%3Dy_%7Bj%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20y_%7Bi%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%5Cleft%28x_%7Bi%7D%20%5Ccdot%20x_%7Bj%7D%5Cright%29%0A" /></p><p>（3）求得分离超平面</p>
<p style=""><img src="https://math.now.sh?from=w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%3D0%0A" /></p><p>分类决策函数:</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(w%5E%7B*%7D%20%5Ccdot%20x%2Bb%5E%7B*%7D%5Cright)%0A" /></p></blockquote>
<ul>
<li>(32)中，每一个合适的<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>都会得出一个<img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"/>，所以<img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"/>不唯一</li>
</ul>
<h3 id="2-4-支持向量">2.4 支持向量</h3>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E8%BD%AF%E9%97%B4%E9%9A%94%E5%88%86%E7%A6%BB%E5%90%91%E9%87%8F.png" alt="image-20201216151045483" style="zoom:50%;" />
<p>虚线是间隔边界，实线是分离超平面。对每一个实例点<img src="https://math.now.sh?inline=%28x_i%2Cy_i%29" style="display:inline-block;margin: 0;"/>和他们的<img src="https://math.now.sh?inline=%5Calpha%20_i%5E*%2C%5C%3B%5Cxi_i" style="display:inline-block;margin: 0;"/>，我们有</p>
<ul>
<li><img src="https://math.now.sh?inline=%5Calpha_%7Bi%7D%5E%7B*%7D%3CC" style="display:inline-block;margin: 0;"/>，则 <img src="https://math.now.sh?inline=%5Cxi_%7Bi%7D%3D0" style="display:inline-block;margin: 0;"/></li>
<li><img src="https://math.now.sh?inline=%5Calpha_%7Bi%7D%5E%7B*%7D%3DC%2C%200%3C%5Cxi_%7Bi%7D%3C1" style="display:inline-block;margin: 0;"/>，则</li>
<li><img src="https://math.now.sh?inline=%5Calpha_%7Bi%7D%5E%7B*%7D%3DC%2C%20%5Cxi_%7Bi%7D%3D1" style="display:inline-block;margin: 0;"/>，则</li>
<li><img src="https://math.now.sh?inline=%5Calpha_%7Bi%7D%5E%7B*%7D%3DC%2C%20%5Cxi_%7Bi%7D%3E1" style="display:inline-block;margin: 0;"/>，则</li>
<li>没有<img src="https://math.now.sh?inline=%5Calpha_%7Bi%7D%5E%7B*%7D%3DC%2C%200%3C%5Cxi_%7Bi%7D%3C1" style="display:inline-block;margin: 0;"/></li>
</ul>
<h3 id="2-5-合页损失函数-Hinge-Loss-Function">2.5 合页损失函数 Hinge Loss Function</h3>
<blockquote>
<p>定理 7.4 线性支持向量机原始最优化问题:</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmin%20_%7Bw%2C%20b%2C%20%5Cxi%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D%2BC%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Cxi_%7Bi%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%20%5Cgeqslant%201-%5Cxi_%7Bi%7D%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%20%5C%5C%0A%26%20%5Cxi_%7Bi%7D%20%5Cgeqslant%200%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><p>等价于最优化问题</p>
<p style=""><img src="https://math.now.sh?from=%5Cmin%20_%7Bw%2C%20b%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%5B1-y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%5Cright%5D_%7B%2B%7D%2B%5Clambda%5C%7Cw%5C%7C%5E%7B2%7D%0A" /></p></blockquote>
<ul>
<li><img src="https://math.now.sh?inline=%5Bz%5D_%7B%2B%7D%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7Dz%2C%20%26%20z%3E0%20%5C%5C%200%2C%20%26%20z%20%5Cleqslant%200%5Cend%7Barray%7D%5Cright." style="display:inline-block;margin: 0;"/>表示取正值的函数。</li>
<li><img src="https://math.now.sh?inline=L%28y(w%20%5Ccdot%20x%2Bb%29)%3D%5B1-y(w%20%5Ccdot%20x%2Bb)%5D_%7B%2B%7D" style="display:inline-block;margin: 0;"/> 经验损失。但实例点被正确分类且函数间隔大于1，损失才是0。位于间隔边界上的实例点损失不为0。</li>
<li><img src="https://math.now.sh?inline=%5Clambda%5C%7Cw%5C%7C%5E%7B2%7D" style="display:inline-block;margin: 0;"/> 正则化项</li>
</ul>
<h4 id="2-5-1-损失函数对比">2.5.1 损失函数对比</h4>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt="image-20201216155233841" style="zoom:50%;" />
<p>虚线为感知机的损失函数<img src="https://math.now.sh?inline=%5Cleft%5B-y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%5Cright%5D_%7B%2B%7D" style="display:inline-block;margin: 0;"/></p>
<ul>
<li>0-1损失函数为二元分类真正的损失函数。<strong>不过因其不连续可导，无法直接优化它所构成的函数</strong>。</li>
<li>合页损失函数是0-1损失函数的上界。<strong>线性支持向量机是优化由0-1损失函数的上界构成的目标函数</strong>。</li>
<li>合页损失函数对学习有更高要求。不但需要分类正确，还要确信度（距离）足够高才为0损失。</li>
</ul>
<h2 id="3-非线性支持向量机与核函数">3. 非线性支持向量机与核函数</h2>
<p>对于一个非线性数据集，首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。</p>
<p>由于特征空间是高维甚至无限维，加上对偶问题中多组高维向量计算内积。因此引入核函数减少计算量。</p>
<h3 id="3-1-核技巧-Kernel-Trick">3.1 核技巧 Kernel Trick</h3>
<p>核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间 (欧氏空间 <img src="https://math.now.sh?inline=%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/> 或离散集合) 对应于一个特征空间 (希尔伯特空间 <img src="https://math.now.sh?inline=%5Cmathcal%7BH%7D" style="display:inline-block;margin: 0;"/> )，使得在输入空间 <img src="https://math.now.sh?inline=%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/> 中的超曲面模型对应于特征空间 <img src="https://math.now.sh?inline=%5Cmathcal%7BH%7D" style="display:inline-block;margin: 0;"/> 中的超平面模型支持向量机）。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。</p>
<blockquote>
<p>定义 7.6 (核函数) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 设 <img src="https://math.now.sh?inline=%5Cmathcal%7BX%7D" style="display:inline-block;margin: 0;"/> 是输入空间 ( 欧氏空间 <img src="https://math.now.sh?inline=%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/> 的子集或离散集合 <img src="https://math.now.sh?inline=%29%2C" style="display:inline-block;margin: 0;"/> 又设 <img src="https://math.now.sh?inline=%5Cmathcal%7BH%7D" style="display:inline-block;margin: 0;"/> 为特征空间 <img src="https://math.now.sh?inline=%28" style="display:inline-block;margin: 0;"/> 希 尔伯特空间 <img src="https://math.now.sh?inline=%29%2C" style="display:inline-block;margin: 0;"/> 如果存在一个从 <img src="https://math.now.sh?inline=%5Cmathcal%7BX%7D" style="display:inline-block;margin: 0;"/> 到 <img src="https://math.now.sh?inline=%5Cmathcal%7BH%7D" style="display:inline-block;margin: 0;"/> 的映射</p>
<p style=""><img src="https://math.now.sh?from=%5Cphi%28x%29%3A%20%5Cmathcal%7BX%7D%20%5Crightarrow%20%5Cmathcal%7BH%7D%0A" /></p><p>使得对所有 <img src="https://math.now.sh?inline=x%2C%20z%20%5Cin%20%5Cmathcal%7BX%7D%2C" style="display:inline-block;margin: 0;"/> 函数 <img src="https://math.now.sh?inline=K%28x%2C%20z%29" style="display:inline-block;margin: 0;"/> 满足条件</p>
<p style=""><img src="https://math.now.sh?from=K%28x%2C%20z%29%3D%5Cphi(x)%20%5Ccdot%20%5Cphi(z)%0A" /></p><p>则称 <strong><img src="https://math.now.sh?inline=K%28x%2C%20z%29" style="display:inline-block;margin: 0;"/> 为核函数， <img src="https://math.now.sh?inline=%5Cphi%28x%29" style="display:inline-block;margin: 0;"/> 为映射函数</strong>，式中 <img src="https://math.now.sh?inline=%5Cphi%28x%29%20%5Ccdot%20%5Cphi(z)" style="display:inline-block;margin: 0;"/> 为 <img src="https://math.now.sh?inline=%5Cphi%28x%29" style="display:inline-block;margin: 0;"/> 和 <img src="https://math.now.sh?inline=%5Cphi%28z%29" style="display:inline-block;margin: 0;"/> 的内积。</p>
</blockquote>
<ul>
<li><strong>在使用过程中只定义核函数<img src="https://math.now.sh?inline=K%28x%2Cz%29" style="display:inline-block;margin: 0;"/>，不显式定义映射函数<img src="https://math.now.sh?inline=%5Cphi" style="display:inline-block;margin: 0;"/></strong></li>
<li>特征空间<img src="https://math.now.sh?inline=%5Cmathcal%7BH%7D" style="display:inline-block;margin: 0;"/>一般是高维甚至无限维</li>
<li>给定核<img src="https://math.now.sh?inline=K%28x%2Cz%29" style="display:inline-block;margin: 0;"/>，特征空间和映射函数不唯一</li>
</ul>
<p>在支持向量机中，通过映射函数<img src="https://math.now.sh?inline=%5Cphi" style="display:inline-block;margin: 0;"/>把输入空间变换到新的特征空间，在新的特征空间中训练线性支持向量机。此时对偶问题的目标函数成为<img src="https://math.now.sh?inline=W%28%5Calpha%29%3D%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%20K%5Cleft(x_%7Bi%7D%2C%20x_%7Bj%7D%5Cright)-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D" style="display:inline-block;margin: 0;"/>，分类决策函数成为<img src="https://math.now.sh?inline=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(%5Csum_%7Bi%3D1%7D%5E%7BN_%7Bs%7D%7D%20a_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20%5Cphi%5Cleft(x_%7Bi%7D%5Cright)%20%5Ccdot%20%5Cphi(x)%2Bb%5E%7B*%7D%5Cright)%3D%5Coperatorname%7Bsign%7D%5Cleft(%5Csum_%7Bi%3D1%7D%5E%7BN_%7Bs%7D%7D%20a_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20K%5Cleft(x_%7Bi%7D%2C%20x%5Cright)%2Bb%5E%7B*%7D%5Cright)" style="display:inline-block;margin: 0;"/></p>
<h3 id="3-2-正定核">3.2 正定核</h3>
<blockquote>
<p>定理 7.5 (正定核的充要条件) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 设 <img src="https://math.now.sh?inline=K%3A%20%5Cmathcal%7BX%7D%20%5Ctimes%20%5Cmathcal%7BX%7D%20%5Crightarrow%20%5Cmathbf%7BR%7D" style="display:inline-block;margin: 0;"/> 是对称函数，则 <img src="https://math.now.sh?inline=K%28x%2C%20z%29" style="display:inline-block;margin: 0;"/> 为正 定核函数的充要条件是对任意 <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20m%2C%20K%28x%2C%20z%29" style="display:inline-block;margin: 0;"/> 对应的 Gram 矩阵:</p>
<p style=""><img src="https://math.now.sh?from=K%3D%5Cleft%5BK%5Cleft%28x_%7Bi%7D%2C%20x_%7Bj%7D%5Cright%29%5Cright%5D_%7Bm%20%5Ctimes%20m%7D%0A" /></p><p>是半正定矩阵。</p>
</blockquote>
<blockquote>
<p>定义 7.7 (正定核的等价定义) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 设 <img src="https://math.now.sh?inline=%5Cmathcal%7BX%7D%20%5Csubset%20%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20K%28x%2C%20z%29" style="display:inline-block;margin: 0;"/> 是定义在 <img src="https://math.now.sh?inline=%5Cmathcal%7BX%7D%20%5Ctimes%20%5Cmathcal%7BX%7D" style="display:inline-block;margin: 0;"/> 上的对称<br>
函数, 如果对任意 <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20m%2C%20K%28x%2C%20z%29" style="display:inline-block;margin: 0;"/> 对应的 Gram 矩阵</p>
<p style=""><img src="https://math.now.sh?from=K%3D%5Cleft%5BK%5Cleft%28x_%7Bi%7D%2C%20x_%7Bj%7D%5Cright%29%5Cright%5D_%7Bm%20%5Ctimes%20m%7D%0A" /></p></blockquote>
<h3 id="3-3-常用核函数">3.3 常用核函数</h3>
<h4 id="3-3-1-多项式核函数-Polynomial-Kernel-Function">3.3.1 多项式核函数 Polynomial Kernel Function</h4>
<p style=""><img src="https://math.now.sh?from=K%28x%2C%20z%29%3D(x%20%5Ccdot%20z%2B1)%5E%7Bp%7D%0A" /></p><p>对应的支持向量机是一个 <img src="https://math.now.sh?inline=p" style="display:inline-block;margin: 0;"/> 次多项式分类器。在此情形下，分类决策函数成为</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(%5Csum_%7Bi%3D1%7D%5E%7BN_%7Bs%7D%7D%20a_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%5Cleft(x_%7Bi%7D%20%5Ccdot%20x%2B1%5Cright)%5E%7Bp%7D%2Bb%5E%7B*%7D%5Cright)%0A" /></p><h4 id="3-3-2-高斯核函数-Gaussian-Kernel-Function">3.3.2 高斯核函数 Gaussian Kernel Function</h4>
<p style=""><img src="https://math.now.sh?from=K%28x%2C%20z%29%3D%5Cexp%20%5Cleft(-%5Cfrac%7B%5C%7Cx-z%5C%7C%5E%7B2%7D%7D%7B2%20%5Csigma%5E%7B2%7D%7D%5Cright)%0A" /></p><p>对应的支持向量机是高斯径向基函数（radial basis function）分类器。在此情形下，分类决策函数成为</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(%5Csum_%7Bi%3D1%7D%5E%7BN_%7Bs%7D%7D%20a_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20%5Cexp%20%5Cleft(-%5Cfrac%7B%5Cleft%5C%7Cx-x_%7Bi%7D%5Cright%5C%7C%5E%7B2%7D%7D%7B2%20%5Csigma%5E%7B2%7D%7D%5Cright)%2Bb%5E%7B*%7D%5Cright)%0A" /></p><h4 id="3-3-3-字符串核函数-String-Kernel-Function">3.3.3 字符串核函数 String Kernel Function</h4>
<p>定义在离散数据集合上。</p>
<h3 id="3-4-非线性支持向量机">3.4 非线性支持向量机</h3>
<blockquote>
<p>定义 7.8 (非线性支持向量机 <img src="https://math.now.sh?inline=%29%20%5Cquad" style="display:inline-block;margin: 0;"/> 从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划(46)学习得到的分类决策函数</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20K%5Cleft(x%2C%20x_%7Bi%7D%5Cright)%2Bb%5E%7B*%7D%5Cright)%0A" /></p><p>称为非线性支持向量机, <img src="https://math.now.sh?inline=K%28x%2C%20z%29" style="display:inline-block;margin: 0;"/> 是正定核涵数。</p>
</blockquote>
<blockquote>
<p>算法 7.4 (非线性支持向量机学习算法) 输入: 训练数据集 <img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%2C" style="display:inline-block;margin: 0;"/> 其中 <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20y_%7Bi%7D%20%5Cin" style="display:inline-block;margin: 0;"/><img src="https://math.now.sh?inline=%5Cmathcal%7BY%7D%3D%5C%7B-1%2C%2B1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N" style="display:inline-block;margin: 0;"/><br>
输出：分类决策函数。<br>
（1）选取适当的核函数 <img src="https://math.now.sh?inline=K%28x%2C%20z%29" style="display:inline-block;margin: 0;"/> 和适当的参数 <img src="https://math.now.sh?inline=C%2C" style="display:inline-block;margin: 0;"/> 构造并求解最优化问题</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7B1%7D%0A%5Cmin%20_%7B%5Calpha%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%20K%5Cleft%28x_%7Bi%7D%2C%20x_%7Bj%7D%5Cright%29-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%3D0%20%5C%5C%0A%26%200%20%5Cleqslant%20%5Calpha_%7Bi%7D%20%5Cleqslant%20C%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><p>求得最优解 <img src="https://math.now.sh?inline=%5Calpha%5E%7B*%7D%3D%5Cleft%28%5Calpha_%7B1%7D%5E%7B*%7D%2C%20%5Calpha_%7B2%7D%5E%7B*%7D%2C%20%5Ccdots%2C%20%5Calpha_%7BN%7D%5E%7B*%7D%5Cright%29%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/> 。<br>
（2）选择 <img src="https://math.now.sh?inline=%5Calpha%5E%7B*%7D" style="display:inline-block;margin: 0;"/> 的一个正分量 <img src="https://math.now.sh?inline=0%3C%5Calpha_%7Bj%7D%5E%7B*%7D%3CC%2C" style="display:inline-block;margin: 0;"/> 计算</p>
<p style=""><img src="https://math.now.sh?from=b%5E%7B*%7D%3Dy_%7Bj%7D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20K%5Cleft%28x_%7Bi%7D%2C%20x_%7Bj%7D%5Cright%29%0A" /></p><p>（3）构造决策函数:</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%5E%7B*%7D%20y_%7Bi%7D%20K%5Cleft(x%2C%20x_%7Bi%7D%5Cright)%2Bb%5E%7B*%7D%5Cright)%0A" /></p></blockquote>
<ul>
<li><img src="https://math.now.sh?inline=K%28x%2Cz%29" style="display:inline-block;margin: 0;"/>为正定核函数是，问题(46)是二次规划问题，解存在。</li>
</ul>
<h2 id="4-SMO-Algorithm-sequential-minimal-optimization">4. SMO Algorithm (sequential minimal optimization)</h2>
<p>序列最小算法用来求解凸二次规划的对偶问题</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cmin%20_%7B%5Calpha%7D%20%26%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5Calpha_%7Bj%7D%20y_%7Bi%7D%20y_%7Bj%7D%20K%5Cleft%28x_%7Bi%7D%2C%20x_%7Bj%7D%5Cright%29-%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20%5C%5C%0A%5Ctext%20%7B%20s.t.%20%7D%20%26%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%3D0%20%5C%5C%0A%26%200%20%5Cleqslant%20%5Calpha_%7Bi%7D%20%5Cleqslant%20C%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%0A%5Cend%7Barray%7D%0A" /></p><h2 id="5-Reference">5. Reference</h2>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/02/[SLM-5]%20%E5%86%B3%E7%AD%96%E6%A0%91%20Decision%20Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/02/%5BSLM-5%5D%20%E5%86%B3%E7%AD%96%E6%A0%91%20Decision%20Tree/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-12-02 16:14:56" itemprop="dateCreated datePublished" datetime="2020-12-02T16:14:56+08:00">2020-12-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-18 16:01:49" itemprop="dateModified" datetime="2020-12-18T16:01:49+08:00">2020-12-18</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1>决策树 Decision Tree</h1>
<p>决策树是一种基本的分类与回归方法，这里主要讨论分类问题。**他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。**学习过程主要包括3个步骤：特征选择、决策树的生成、决策树的修建，并根据损失函数最小化的原则建立决策树模型。</p>
<h2 id="1-Model">1. Model</h2>
<blockquote>
<p>定义 5.1 (决策树) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点 ( node ) 和有向边 ( directed edge ) 组成。结点有两种类型: 内部结点 ( internal node ) 和叶结,点 ( leaf node ) 。内部结点表示一个特征或属性, 叶结点表示一个类。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">		A--是--&gt;C[叶结点]</span><br><span class="line">		A((根节点))--否--&gt;B((内部结点))</span><br><span class="line">		B--是--&gt;D[叶结点]</span><br><span class="line">		B--否--&gt;E[叶结点]</span><br></pre></td></tr></table></figure>
<h3 id="1-1-决策树与if-then规则">1.1 决策树与if-then规则</h3>
<ul>
<li>根结点到叶结点的每一条路径是一条规则</li>
<li>路径上的内部节点对应规则的条件</li>
<li>叶结点的类对应规则的结论</li>
<li>if-then规则是<strong>互斥且完备</strong>，即每一个实例有且仅有被一条路径/规则覆盖。</li>
</ul>
<h3 id="1-2-决策树与给定特征条件下的类条件概率分布">1.2 决策树与给定特征条件下的类条件概率分布</h3>
<p><strong>决策树的生成等价于对特征空间的划分(partition)</strong>，从而划分成互不相交的单元(cell)/区域(region)，再每一个单元定义一个类的概率分布就构成的一个条件概率分布。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。</p>
<p><img src="https://math.now.sh?inline=P%28Y%7CX%29%2C%5C%3BX%3A%E7%89%B9%E5%BE%81%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%5C%3B%5C%3B%5C%3B%5C%3BY%3A%E7%B1%BB%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F" style="display:inline-block;margin: 0;"/></p>
<p>条件概率<img src="https://math.now.sh?inline=P%28Y%7CX%29" style="display:inline-block;margin: 0;"/>往往偏大于某一类<img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/>。分类时把该节点实例强行分到条件概率大的那一类。</p>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLm-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87.png" alt="image-20201202210309048" style="zoom:50%;" />
<h3 id="1-3-决策树的学习">1.3 决策树的学习</h3>
<p>学习本质：为训练数据集归纳出一组分类规则</p>
<ul>
<li>正确分类训练数据集的决策树可能有很多个，可能没有</li>
<li>需要找到一个与训练数据集的误差较小、泛化能力高的决策树</li>
</ul>
<p>损失函数与策略：最小化正则化后的极大似然函数</p>
<p>算法：递归的选择最优特征。ID3、C4.5、CART</p>
<p>剪枝：树枝多，复杂，泛化能力低。自下而上进行剪枝。树的生成考虑局部最优，树的剪枝考虑全局最优。</p>
<h2 id="2-特征选择-Feature-Selection">2. 特征选择 Feature Selection</h2>
<p>通过信息增益（比）来选取具有分类能力的特征（指那些与随机分类有较大查别的特征），从而提高决策树的学习效率。</p>
<h3 id="2-1-信息增益-Information-Gain">2.1 信息增益 Information Gain</h3>
<h4 id="2-1-1-熵-Entropy、信息熵">2.1.1 熵 Entropy、信息熵</h4>
<blockquote>
<p>定义 5.01（熵）		熵是表示随机变量不确定性的度量。设<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>为离散随机变量，概率分布为</p>
<p style=""><img src="https://math.now.sh?from=P%5Cleft%28X%3Dx_%7Bi%7D%5Cright%29%3Dp_%7Bi%7D%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%0A" /></p><p>随机变量<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的熵为</p>
<p style=""><img src="https://math.now.sh?from=H%28X%29%3DH(p)%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p_%7Bi%7D%20%5Clog%20p_%7Bi%7D%0A" /></p><p style=""><img src="https://math.now.sh?from=%E5%AF%B9p_i%3D0%EF%BC%8C%E5%AE%9A%E4%B9%890log0%3D0%5Cnonumber%0A" /></p></blockquote>
<ul>
<li>在(2)中，对数底为2/<img src="https://math.now.sh?inline=e" style="display:inline-block;margin: 0;"/>时，熵单位为比特bit/纳特nat</li>
<li>熵只和<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的分布有关，与取值<img src="https://math.now.sh?inline=x_i" style="display:inline-block;margin: 0;"/>无关</li>
<li>熵越大，随机变量的不确定性就越大</li>
<li><img src="https://math.now.sh?inline=0%20%5Cleqslant%20H%28p%29%20%5Cleqslant%20%5Clog%20n" style="display:inline-block;margin: 0;"/></li>
</ul>
<h4 id="2-1-2-条件熵-Conditional-Entropy">2.1.2 条件熵 Conditional Entropy</h4>
<blockquote>
<p>定义 5.02（条件熵）		对随机变量(X,Y)，联合概率分布为</p>
<p style=""><img src="https://math.now.sh?from=P%5Cleft%28X%3Dx_%7Bi%7D%2C%20Y%3Dy_%7Bj%7D%5Cright%29%3Dp_%7Bi%20j%7D%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%20%3B%20%5Cquad%20j%3D1%2C2%2C%20%5Ccdots%2C%20m%5Cnonumber%0A" /></p><p>条件熵<img src="https://math.now.sh?inline=H%28Y%7CX%29" style="display:inline-block;margin: 0;"/>表示一直随机变量<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的情况下随机变量<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>的不确定性。<strong>定义为给定<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>后<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>的条件概率分布的熵对<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的数学期望</strong>，即</p>
<p style=""><img src="https://math.now.sh?from=H%28Y%20%5Cmid%20X%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p_%7Bi%7D%20H%5Cleft(Y%20%5Cmid%20X%3Dx_%7Bi%7D%5Cright)%5C%5C%0Ap_%7Bi%7D%3DP%5Cleft(X%3Dx_%7Bi%7D%5Cright)%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20n%0A" /></p></blockquote>
<p>当熵和条件熵的概率有极大似然估计得到时，对应的为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy)</p>
<h4 id="2-1-3-信息增益">2.1.3 信息增益</h4>
<p><strong>信息增益表示得知特征<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的信息从而使得类<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>的信息的不确定性减少程度。</strong></p>
<blockquote>
<p>定义 5.2 (信息增益) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 对训练数据集<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>的信息增益 <img src="https://math.now.sh?inline=g%28D%2C%20A%29%2C" style="display:inline-block;margin: 0;"/> 定义为集合<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>的经验熵<img src="https://math.now.sh?inline=H%28D%29" style="display:inline-block;margin: 0;"/> 与特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 给定条件下<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>的经验条件熵 <img src="https://math.now.sh?inline=H%28D%20%5Cmid%20A%29" style="display:inline-block;margin: 0;"/> 之差，即</p>
<p style=""><img src="https://math.now.sh?from=g%28D%2C%20A%29%3DH(D)-H(D%20%5Cmid%20A)%5Cnonumber%0A" /></p></blockquote>
<ul>
<li>熵和条件熵之差称为互信息 mutual information，在决策树中即为信息增益</li>
<li><img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>一般为标签</li>
</ul>
<h4 id="2-1-4-信息增益比-information-gain-ratio">2.1.4 信息增益比 information gain ratio</h4>
<p>在训练集里，某一个特征较多时，信息增益会偏大。因此采用信息增益比来校正。</p>
<blockquote>
<p>定义 5.3 $ (信息增益比) $$\quad$ 特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 对训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的信息增益比 <img src="https://math.now.sh?inline=g_%7BR%7D%28D%2C%20A%29" style="display:inline-block;margin: 0;"/> 定义为其信息增益 <img src="https://math.now.sh?inline=g%28D%2C%20A%29" style="display:inline-block;margin: 0;"/> 与训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 关于特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 的值的熵 <img src="https://math.now.sh?inline=H_%7BA%7D%28D%29" style="display:inline-block;margin: 0;"/> 之比，即</p>
<p style=""><img src="https://math.now.sh?from=g_%7BR%7D%28D%2C%20A%29%3D%5Cfrac%7Bg(D%2C%20A)%7D%7BH_%7BA%7D(D)%7D%0A" /></p><p>其中 <img src="https://math.now.sh?inline=%2C%20H_%7BA%7D%28D%29%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%2C%20n" style="display:inline-block;margin: 0;"/> 是特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 取值的个数。</p>
</blockquote>
<h4 id="2-1-5-特征增益的算法">2.1.5 特征增益的算法</h4>
<p>对数据集<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>，有<img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/>个类<img src="https://math.now.sh?inline=C_k" style="display:inline-block;margin: 0;"/>，对特征<img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/>有<img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"/>个取值<img src="https://math.now.sh?inline=D_n" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=D_%7Bi%20k%7D%3DD_%7Bi%7D%20%5Ccap%20C_%7Bk%7D" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=%7CD%7C" style="display:inline-block;margin: 0;"/>为数据集中的样本个数</p>
<blockquote>
<p>算法 5.1 (信息增益的算法)<br>
输入: 训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 和特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/>;<br>
输出: 特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 对训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的信息增益 <img src="https://math.now.sh?inline=g%28D%2C%20A%29" style="display:inline-block;margin: 0;"/> 。<br>
(1) 计算数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的经验熵<img src="https://math.now.sh?inline=H%28D%29" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=H%28D%29%3D-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20%5Cfrac%7B%5Cleft%7CC_%7Bk%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CC_%7Bk%7D%5Cright%7C%7D%7B%7CD%7C%7D%5Cnonumber%0A" /></p><p>(2) 计算特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 对数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的经验条件熵 <img src="https://math.now.sh?inline=H%28D%20%5Cmid%20A%29" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=H%28D%20%5Cmid%20A%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%20H%5Cleft(D_%7Bi%7D%5Cright)%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%20k%7D%5Cright%7C%7D%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%20%5Clog%20_%7B2%7D%20%5Cfrac%7B%5Cleft%7CD_%7Bi%20k%7D%5Cright%7C%7D%7B%5Cleft%7CD_%7Bi%7D%5Cright%7C%7D%5Cnonumber%0A" /></p><p>(3) 计算信息增益</p>
<p style=""><img src="https://math.now.sh?from=g%28D%2C%20A%29%3DH(D)-H(D%20%5Cmid%20A)%5Cnonumber%0A" /></p></blockquote>
<ul>
<li>没有特征B！A是特征的符号</li>
</ul>
<h2 id="3-ID3-Algorithm">3. ID3 Algorithm</h2>
<p>决策树各个节点熵应用信息增益准则选择特征，递归构建决策树。</p>
<p>从根结点开始，计算所有可能的特征，选取信息增益最大的特征作为节点特征。并由该特征的不同取值点建立子节点。递归调用。</p>
<blockquote>
<p>算法 5.2 (ID3 算法)<br>
输入: 训练数据集 <img src="https://math.now.sh?inline=D%2C" style="display:inline-block;margin: 0;"/> 特征集 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 间值 <img src="https://math.now.sh?inline=%5Cvarepsilon%20%3B" style="display:inline-block;margin: 0;"/><img src="https://math.now.sh?inline=%5C%5C" style="display:inline-block;margin: 0;"/></p>
<p>输出：决策树 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 。</p>
<p>(1) 若 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中所有实例属于同一类 <img src="https://math.now.sh?inline=C_%7Bk%7D%2C" style="display:inline-block;margin: 0;"/> 则 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点 的类标记，返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(2) 若 <img src="https://math.now.sh?inline=A%3D%5Cvarnothing%2C" style="display:inline-block;margin: 0;"/> 则 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中实例数最大的类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点的类标记，返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> :<br>
(3) 否则，按算法 5.1 计算 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 中各特征对 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的信息增益，选择信息增益最大的特征 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> :</p>
<p>(4) 如果 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> 的信息增益小于间值 <img src="https://math.now.sh?inline=%5Cvarepsilon%2C" style="display:inline-block;margin: 0;"/> 则置 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中实例数最大 的类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点的类标记，返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> :<br>
(5) 否则，对 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> 的每一可能值 <img src="https://math.now.sh?inline=a_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 依 <img src="https://math.now.sh?inline=A_%7Bg%7D%3Da_%7Bi%7D" style="display:inline-block;margin: 0;"/> 将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 分割为若干非空子集 <img src="https://math.now.sh?inline=D_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 将 <img src="https://math.now.sh?inline=D_%7Bi%7D" style="display:inline-block;margin: 0;"/> 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 <img src="https://math.now.sh?inline=T%2C" style="display:inline-block;margin: 0;"/> 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(6) 对第 <img src="https://math.now.sh?inline=i" style="display:inline-block;margin: 0;"/> 个子结点，以 <img src="https://math.now.sh?inline=D_%7Bi%7D" style="display:inline-block;margin: 0;"/> 为训练集，以 <img src="https://math.now.sh?inline=A-%5Cleft%5C%7BA_%7Bg%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/> 为特征集，逆归地调用步 (1)<img src="https://math.now.sh?inline=%5Csim" style="display:inline-block;margin: 0;"/> 步 <img src="https://math.now.sh?inline=%285%29%2C" style="display:inline-block;margin: 0;"/> 得到子树 <img src="https://math.now.sh?inline=T_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 返回 <img src="https://math.now.sh?inline=T_%7Bi%7D%20%5Ccirc" style="display:inline-block;margin: 0;"/></p>
</blockquote>
<ul>
<li>极大似然法进行概率估计？</li>
<li>只有树的生成，没有剪枝，容易过拟合</li>
</ul>
<h2 id="4-C4-5-Algorithm">4. C4.5 Algorithm</h2>
<p>ID3算法的改进，用信息增益比来选择特征</p>
<blockquote>
<p>算法 5.3 (C4.5 的生成算法)</p>
<p>输入: 训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>, 特征集 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 间值 <img src="https://math.now.sh?inline=%5Cvarepsilon" style="display:inline-block;margin: 0;"/>;</p>
<p>输出：决策树 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 。<br>
(1) 如果 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中所有实例属于同一类 <img src="https://math.now.sh?inline=C_%7Bk%7D%2C" style="display:inline-block;margin: 0;"/> 则置 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结 点的类, 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(2) 如果 <img src="https://math.now.sh?inline=A%3D%5Cvarnothing%2C" style="display:inline-block;margin: 0;"/> 则置 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中实例数最大的类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点 的类, 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(3) 否则，按式 (5.10) 计算 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 中各特征对 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的<em><strong>信息增益比</strong></em>, 选择信息增益比最大 的特征 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/>;<br>
(4) 如果 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> 的信息增益比小于间值 <img src="https://math.now.sh?inline=%5Cvarepsilon%2C" style="display:inline-block;margin: 0;"/> 则置 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为单结点树，并将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中实例数最 大的类 <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 作为该结点的类, 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(5) 否则, 对 <img src="https://math.now.sh?inline=A_%7Bg%7D" style="display:inline-block;margin: 0;"/> 的每一可能值 <img src="https://math.now.sh?inline=a_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 依 <img src="https://math.now.sh?inline=A_%7Bg%7D%3Da_%7Bi%7D" style="display:inline-block;margin: 0;"/> 将 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 分割为子集若干非空 <img src="https://math.now.sh?inline=D_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 将 <img src="https://math.now.sh?inline=D_%7Bi%7D" style="display:inline-block;margin: 0;"/> 中实例数最大的类作为标记，构建子结点, 由结点及其子结点构成树 <img src="https://math.now.sh?inline=T%2C" style="display:inline-block;margin: 0;"/> 返回 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>;<br>
(6) 对结点 <img src="https://math.now.sh?inline=i" style="display:inline-block;margin: 0;"/>, 以 <img src="https://math.now.sh?inline=D_%7Bi%7D" style="display:inline-block;margin: 0;"/> 为训练集，以 <img src="https://math.now.sh?inline=A-%5Cleft%5C%7BA_%7Bg%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/> 为特征集，递归地调用步 (1) 步 <img src="https://math.now.sh?inline=%285%29%2C" style="display:inline-block;margin: 0;"/> 得到子树 <img src="https://math.now.sh?inline=T_%7Bi%7D%2C" style="display:inline-block;margin: 0;"/> 返回 <img src="https://math.now.sh?inline=T_%7Bi%7D" style="display:inline-block;margin: 0;"/> 。</p>
</blockquote>
<h2 id="5-Pruning">5. Pruning</h2>
<p>考虑树的复杂度，对生成的决策树进行剪枝，减掉子树或叶结点</p>
<p><strong>策略：极小化决策树的损失函数</strong>，即正则化的极大似然估计</p>
<p>设树<img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>，叶节点个数<img src="https://math.now.sh?inline=%7CT%7C" style="display:inline-block;margin: 0;"/>，树<img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>的叶结点<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>，此叶结点的样本点个数<img src="https://math.now.sh?inline=N_t" style="display:inline-block;margin: 0;"/>；此叶结点有<img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"/>类样本点的个数<img src="https://math.now.sh?inline=N_%7Btk%7D%EF%BC%9B%5C%3Bk%3D1%2C2%2C...%2CK" style="display:inline-block;margin: 0;"/>；此叶结点的经验熵<img src="https://math.now.sh?inline=H_t%28T%29" style="display:inline-block;margin: 0;"/>，函数参数<img src="https://math.now.sh?inline=%5Calpha%20%5Cgeq0" style="display:inline-block;margin: 0;"/>。此决策树的损失函数定义为</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%28T%29%3D%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20N_%7Bt%7D%20H_%7Bt%7D(T)%2B%5Calpha%7CT%7C%0A" /></p><p>经验熵<img src="https://math.now.sh?inline=H_t%28T%29" style="display:inline-block;margin: 0;"/>为</p>
<p style=""><img src="https://math.now.sh?from=H_%7Bt%7D%28T%29%3D-%5Csum_%7Bk%7D%20%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%20%5Clog%20%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%0A" /></p><p>记(6)的左边为<img src="https://math.now.sh?inline=C%28T%29" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=C%28T%29%3D%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20N_%7Bt%7D%20H_%7Bt%7D(T)%3D-%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20N_%7Bt%20k%7D%20%5Clog%20%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%0A" /></p><p>最后得到</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%28T%29%3DC(T)%2B%5Calpha%7CT%7C%0A" /></p><ul>
<li><img src="https://math.now.sh?inline=C%28T%29" style="display:inline-block;margin: 0;"/>为样本点个数与经验熵的积，表示模型对训练数据的预测误差</li>
<li><img src="https://math.now.sh?inline=%7CT%7C" style="display:inline-block;margin: 0;"/>为模型复杂度
<ul>
<li>越大说明叶结点越多，树越复杂</li>
<li><img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>越大，选择较简单的模型树</li>
</ul>
</li>
</ul>
<h3 id="5-1-Pruning-algorithm">5.1 Pruning algorithm</h3>
<blockquote>
<p>算法 5.4 (树的剪枝算法)<br>
输入: 生成算法产生的整个树 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/>, 参数 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>;<br>
输出：修剪后的子树 <img src="https://math.now.sh?inline=T_%7B%5Calpha%7D%20%5Ccirc" style="display:inline-block;margin: 0;"/><br>
（1）计算每个结点的经验熵。<br>
（2）递归地从树的叶结点向上回缩。</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D.png" alt="image-20201205222057313"></p>
<p>​	设一组叶结点回缩到其父结点之前与之后的整体树分别为 <img src="https://math.now.sh?inline=T_%7BB%7D" style="display:inline-block;margin: 0;"/> 与 <img src="https://math.now.sh?inline=T_%7BA%7D" style="display:inline-block;margin: 0;"/>, 其对应的 损失函数值分别是 <img src="https://math.now.sh?inline=C_%7B%5Calpha%7D%5Cleft%28T_%7BB%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 与 <img src="https://math.now.sh?inline=C_%7B%5Calpha%7D%5Cleft%28T_%7BA%7D%5Cright%29%2C" style="display:inline-block;margin: 0;"/> 如果</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%5Cleft%28T_%7BA%7D%5Cright%29%20%5Cleqslant%20C_%7B%5Calpha%7D%5Cleft(T_%7BB%7D%5Cright)%0A" /></p><p>则进行剪枝，即将父结点变为新的叶结点。<br>
(3) 返回 ( 2 )，直至不能继续为止，得到损失函数最小的子树 <img src="https://math.now.sh?inline=T_%7B%5Calpha%5E%7B%5Ccirc%7D%7D" style="display:inline-block;margin: 0;"/></p>
</blockquote>
<ul>
<li>动态规划的算法实现</li>
</ul>
<h2 id="6-CART-Algorithm">6. CART Algorithm</h2>
<p>分类与回归树(classification and regression tree)是给定随机变量<img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>的条件下给出随机变量<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>的条件概率分布的学习方法。决策树是二叉树，左分支是“是”分支，右否。<strong>通过递归的二分每个特征，使得特征空间划分成有限个单元</strong>，在这些单元上确定概率分布。</p>
<h3 id="6-1-CART的生成">6.1 CART的生成</h3>
<h4 id="6-1-1-回归树生成">6.1.1 回归树生成</h4>
<h5 id="6-1-1-1-Model">6.1.1.1  Model</h5>
<p>用<strong>平方误差最小化准则</strong>，进行特征选择，递归的构建二叉决策树。</p>
<p>对数据集<img src="https://math.now.sh?inline=D%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>是连续变量，生成回归树。</p>
<p>回归树对应的是<strong>特征空间的划分</strong>以及<strong>划分单元上的输出值</strong>。</p>
<blockquote>
<p>定义 5.03 （回归树模型）</p>
<p>假设已将输入空间划分为 <img src="https://math.now.sh?inline=M" style="display:inline-block;margin: 0;"/> 个单元 <img src="https://math.now.sh?inline=R_%7B1%7D%2C%20R_%7B2%7D%2C%20%5Ccdots%2C%20R_%7BM%7D%2C" style="display:inline-block;margin: 0;"/> 并且在每个单元 <img src="https://math.now.sh?inline=R_%7Bm%7D" style="display:inline-block;margin: 0;"/> 上 有一个固定的输出值 <img src="https://math.now.sh?inline=c_%7Bm%7D%2C" style="display:inline-block;margin: 0;"/> 于是回归树模型可表示为</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20c_%7Bm%7D%20I%5Cleft(x%20%5Cin%20R_%7Bm%7D%5Cright)%0A" /></p></blockquote>
<ul>
<li>
<p><img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/>是特征向量</p>
</li>
<li>
<p>当输入空间划分确定，利用平方误差最小准则可以确定<img src="https://math.now.sh?inline=c_m" style="display:inline-block;margin: 0;"/>的最优值</p>
<ul>
<li>
<p>平方误差定义为<img src="https://math.now.sh?inline=%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7Bm%7D%7D%5Cleft%28y_%7Bi%7D-f%5Cleft(x_%7Bi%7D%5Cright%29%5Cright)%5E%7B2%7D" style="display:inline-block;margin: 0;"/>。注意<img src="https://math.now.sh?inline=x_i%5Cin%20R_m" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>利用平方误差最小原则，单元 <img src="https://math.now.sh?inline=R_%7Bm%7D" style="display:inline-block;margin: 0;"/> 上的 <img src="https://math.now.sh?inline=c_%7Bm%7D" style="display:inline-block;margin: 0;"/> 的最优值 <img src="https://math.now.sh?inline=%5Chat%7Bc%7D_%7Bm%7D" style="display:inline-block;margin: 0;"/> 是 <img src="https://math.now.sh?inline=R_%7Bm%7D" style="display:inline-block;margin: 0;"/> 上的所有输入实例 <img src="https://math.now.sh?inline=x_%7Bi%7D" style="display:inline-block;margin: 0;"/> 对应的输出 <img src="https://math.now.sh?inline=y_%7Bi%7D" style="display:inline-block;margin: 0;"/> 的均值，即</p>
<p style=""><img src="https://math.now.sh?from=%5Chat%7Bc%7D_%7Bm%7D%3D%5Coperatorname%7Bave%7D%5Cleft%28y_%7Bi%7D%20%5Cmid%20x_%7Bi%7D%20%5Cin%20R_%7Bm%7D%5Cright%29%5Cnonumber%0A" /></p></li>
</ul>
</li>
</ul>
<p><strong>问题1：如何划分特征空间，即选择划分点</strong></p>
<p>通过启发式算法，选择第 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/> 个变量 <img src="https://math.now.sh?inline=x%5E%7B%28j%29%7D" style="display:inline-block;margin: 0;"/>和它取的特征值 <img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>, 作为切分变量（splitting variable）和切分点 (splitting point)，并定义两个区域</p>
<p style=""><img src="https://math.now.sh?from=R_%7B1%7D%28j%2C%20s%29%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%20%5Cleqslant%20s%5Cright%5C%7D%20%5Cquad%20%5Ctext%20%7B%20%E5%92%8C%20%7D%20%5Cquad%20R_%7B2%7D(j%2C%20s)%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%3Es%5Cright%5C%7D%0A" /></p><p>在一开始的选择基础上，寻找最优切分变量<img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/>和最优切分点<img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>。具体的，求解</p>
<p style=""><img src="https://math.now.sh?from=%5Cmin%20_%7Bj%2C%20s%7D%5Cleft%5B%5Cmin%20_%7Bc_%7B1%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B1%7D%28j%2C%20s%29%7D%5Cleft(y_%7Bi%7D-c_%7B1%7D%5Cright)%5E%7B2%7D%2B%5Cmin%20_%7Bc_%7B2%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%7D%5Cleft(y_%7Bi%7D-c_%7B2%7D%5Cright)%5E%7B2%7D%5Cright%5D%0A" /></p><ul>
<li>(11)中，<img src="https://math.now.sh?inline=x%5E%7B%28j%29%7D" style="display:inline-block;margin: 0;"/>是第<img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/>个特征，<img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>是此特征维度上的值。<img src="https://math.now.sh?inline=x_n" style="display:inline-block;margin: 0;"/>是第<img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"/>个输入向量。<a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/93667566">ref</a></li>
<li>(12)中，括号里的对于<img src="https://math.now.sh?inline=c_1%2Cc_2" style="display:inline-block;margin: 0;"/>取极小值是已知的。</li>
<li>划分特征空间时是把高维矩形划分为2个子高维矩形。</li>
</ul>
<p><strong>问题2：如何确定好输出值<img src="https://math.now.sh?inline=c_m" style="display:inline-block;margin: 0;"/></strong></p>
<p>特征空间切分好后，根据最小化平方误差准则，得到相应的最优输出值</p>
<p style=""><img src="https://math.now.sh?from=%5Chat%7Bc%7D_%7B1%7D%3D%5Coperatorname%7Bave%7D%5Cleft%28y_%7Bi%7D%20%5Cmid%20x_%7Bi%7D%20%5Cin%20R_%7B1%7D(j%2C%20s%29%5Cright)%20%5Cquad%20%5Ctext%20%7B%20%E5%92%8C%20%7D%20%5Cquad%20%5Chat%7Bc%7D_%7B2%7D%3D%5Coperatorname%7Bave%7D%5Cleft(y_%7Bi%7D%20%5Cmid%20x_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%5Cright)%0A" /></p><p>**总结：**一开始，我们遍历所有输入特征，找到最优的切分特征变量<img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/>，构成一对<img src="https://math.now.sh?inline=%28j%2Cs%29" style="display:inline-block;margin: 0;"/>，根据这个构成的超平面，讲特征空间划分成2个区域。对每个子区域重复过程，直到满足停止条件。这种回归树称为最小二乘回归树(least squares regression tree)</p>
<h5 id="6-1-1-2-Algorithm">6.1.1.2 Algorithm</h5>
<blockquote>
<p>算法 5.5 (最小二乘回归树生成算法)<br>
输入: 训练数据集 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>;<br>
输出：回归树 <img src="https://math.now.sh?inline=f%28x%29" style="display:inline-block;margin: 0;"/> 。<br>
在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树:<br>
（1）选择最优切分变量 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/> 与切分点 <img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>, 求解</p>
<p style=""><img src="https://math.now.sh?from=%5Cmin%20_%7Bj%2C%20s%7D%5Cleft%5B%5Cmin%20_%7Bc_%7B1%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B1%7D%28j%2C%20s%29%7D%5Cleft(y_%7Bi%7D-c_%7B1%7D%5Cright)%5E%7B2%7D%2B%5Cmin%20_%7Bc_%7B2%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%7D%5Cleft(y_%7Bi%7D-c_%7B2%7D%5Cright)%5E%7B2%7D%5Cright%5D%0A" /></p><p>遍历变量 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/>, 对固定的切分变量 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/> 扫描切分点 <img src="https://math.now.sh?inline=s" style="display:inline-block;margin: 0;"/>, 选择使式 (14) 达到最小值的对<img src="https://math.now.sh?inline=%28j%2C%20s%29" style="display:inline-block;margin: 0;"/><br>
（2）用选定的对 <img src="https://math.now.sh?inline=%28j%2C%20s%29" style="display:inline-block;margin: 0;"/> 划分区域并决定相应的输出值:</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bc%7D%0AR_%7B1%7D%28j%2C%20s%29%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%20%5Cleqslant%20s%5Cright%5C%7D%2C%20%5Cquad%20R_%7B2%7D(j%2C%20s)%3D%5Cleft%5C%7Bx%20%5Cmid%20x%5E%7B(j)%7D%3Es%5Cright%5C%7D%20%5C%5C%0A%5Chat%7Bc%7D_%7Bm%7D%3D%5Cfrac%7B1%7D%7BN_%7Bm%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7Bm%7D(j%2C%20s)%7D%20y_%7Bi%7D%2C%20%5Cquad%20x%20%5Cin%20R_%7Bm%7D%2C%20%5Cquad%20m%3D1%2C2%0A%5Cend%7Barray%7D%0A" /></p><p>（3）继续对两个子区域调用步骤 <img src="https://math.now.sh?inline=%281%29%2C(2)%2C" style="display:inline-block;margin: 0;"/> 直至满足停止条件。<br>
（4）将输入空间划分为 <img src="https://math.now.sh?inline=M" style="display:inline-block;margin: 0;"/> 个区域 <img src="https://math.now.sh?inline=R_%7B1%7D%2C%20R_%7B2%7D%2C%20%5Ccdots%2C%20R_%7BM%7D%2C" style="display:inline-block;margin: 0;"/> 生成决策树:</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%20%5Chat%7Bc%7D_%7Bm%7D%20I%5Cleft(x%20%5Cin%20R_%7Bm%7D%5Cright)%0A" /></p></blockquote>
<h5 id="6-1-1-3-Example">6.1.1.3 Example</h5>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E1" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E2" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E3" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_1" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.2</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_2" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">3.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_3" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.6.</td>
<td style="text-align:center">6</td>
<td style="text-align:center">2.75</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_4" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">9</td>
<td style="text-align:center">2.25</td>
<td style="text-align:center">3</td>
</tr>
</tbody>
</table>
<p>**Q：**对以上数据集基于最小化平方误差生成二叉回归树</p>
<ol>
<li>
<p>设<img src="https://math.now.sh?inline=j%3Dx%5E1" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=s%3D1.5" style="display:inline-block;margin: 0;"/>时</p>
<p><img src="https://math.now.sh?inline=c_1%3D1%2C%5C%3Bc_2%3D2.5" style="display:inline-block;margin: 0;"/></p>
<p><img src="https://math.now.sh?inline=%5Cmin%20_%7Bc_%7B1%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B1%7D%28j%2C%20s%29%7D%5Cleft(y_%7Bi%7D-c_%7B1%7D%5Cright)%5E%7B2%7D%2B%5Cmin%20_%7Bc_%7B2%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20R_%7B2%7D(j%2C%20s)%7D%5Cleft(y_%7Bi%7D-c_%7B2%7D%5Cright)%5E%7B2%7D%3D%5C%5C%20%5B(1-1)%5E2%2B(1-1)%5E2%5D%2B%5B(2-2.5)%5E2%2B(3-2.5)%5E2%5D%3D0.5" style="display:inline-block;margin: 0;"/></p>
<p><img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E1%2C1.6)%2C%5C%3B0.67" style="display:inline-block;margin: 0;"/></p>
<p><img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E1%2C1.2)%2C%5C%3B(j%2Cs)%3D(x%5E1%2C1.8)" style="display:inline-block;margin: 0;"/>的结果一定偏大</p>
<p>对固定<img src="https://math.now.sh?inline=j%3Dx%5E1" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=s%3D1.5" style="display:inline-block;margin: 0;"/>是最佳切分点，<img src="https://math.now.sh?inline=error%3D0.5" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>设<img src="https://math.now.sh?inline=j%3Dx%5E2" style="display:inline-block;margin: 0;"/></p>
<p>最佳切分为<img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E2%2C4)%2C%5C%3Berror%3D0.5" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>设<img src="https://math.now.sh?inline=j%3Dx%5E3" style="display:inline-block;margin: 0;"/></p>
<p>最佳切分为<img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E3%2C2.75)%2C%5C%3Berror%3D2" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p><img src="https://math.now.sh?inline=%5Cmin%20_%7Bj%2C%20s%7D%3D%5Cmin%5B0.5%2C%5C%3B0.5%2C%5C%3B2%5D%3D0.5" style="display:inline-block;margin: 0;"/>，选择<img src="https://math.now.sh?inline=%28j%2Cs%29%3D(x%5E1%2C1.5)" style="display:inline-block;margin: 0;"/>作为最优划分。划分后的子集<img src="https://math.now.sh?inline=R_1%2C%5C%3BR_2" style="display:inline-block;margin: 0;"/>为</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong><img src="https://math.now.sh?inline=R_1" style="display:inline-block;margin: 0;"/>左分支</strong></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E1" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E2" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=x%5E3" style="display:inline-block;margin: 0;"/></th>
<th style="text-align:center"><img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_1" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.2</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_2" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">3.5</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center"><strong><img src="https://math.now.sh?inline=R_2" style="display:inline-block;margin: 0;"/>右分支</strong></td>
<td style="text-align:center"><img src="https://math.now.sh?inline=x%5E1" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center"><img src="https://math.now.sh?inline=x%5E2" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center"><img src="https://math.now.sh?inline=x%5E3" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center"><img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/></td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_3" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.6.</td>
<td style="text-align:center">6</td>
<td style="text-align:center">2.75</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center"><img src="https://math.now.sh?inline=x_4" style="display:inline-block;margin: 0;"/></td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">9</td>
<td style="text-align:center">2.25</td>
<td style="text-align:center">3</td>
</tr>
</tbody>
</table>
<p><img src="https://math.now.sh?inline=%5Chat%7Bc%7D_1%3D1%2C%5C%3B%5Chat%7Bc%7D_2%3D2.5" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>对左右分支继续迭代1-4的步骤，直到满足停止条件</p>
</li>
</ol>
<h4 id="6-1-2-分类树的生成">6.1.2 分类树的生成</h4>
<p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<h5 id="6-1-2-1-Gini-Index">6.1.2.1 Gini Index</h5>
<blockquote>
<p>定义 5.4 (基尼指数) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 分类问题中，假设有 <img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"/> 个类，样本点属于第 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 类的概率<br>
为 <img src="https://math.now.sh?inline=p_%7Bk%7D" style="display:inline-block;margin: 0;"/>, 则棍率分布的基尼指数定义为</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28p%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20p_%7Bk%7D%5Cleft(1-p_%7Bk%7D%5Cright)%3D1-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%20p_%7Bk%7D%5E%7B2%7D%0A" /></p><p>对于二类分类问题, 若样本点属于第 1 个类的概率是 <img src="https://math.now.sh?inline=p%2C" style="display:inline-block;margin: 0;"/> 则概率分布的基尼指数为</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28p%29%3D2%20p(1-p)%0A" /></p><p>对于给定的样本集合 <img src="https://math.now.sh?inline=D%2C" style="display:inline-block;margin: 0;"/> 其基尼指数为</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28D%29%3D1-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Cleft(%5Cfrac%7B%5Cleft%7CC_%7Bk%7D%5Cright%7C%7D%7B%7CD%7C%7D%5Cright)%5E%7B2%7D%0A" /></p><p>这里, <img src="https://math.now.sh?inline=C_%7Bk%7D" style="display:inline-block;margin: 0;"/> 是 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 中属于第 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 类的样本子集， <img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"/> 是类的个数。</p>
<p>如果样本集合 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 根据<strong>特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/></strong> 是否取某一可能值 <img src="https://math.now.sh?inline=a" style="display:inline-block;margin: 0;"/> 被分割成 <img src="https://math.now.sh?inline=D_%7B1%7D" style="display:inline-block;margin: 0;"/> 和 <img src="https://math.now.sh?inline=D_%7B2%7D" style="display:inline-block;margin: 0;"/> 两部分，即</p>
<p style=""><img src="https://math.now.sh?from=D_%7B1%7D%3D%5C%7B%28x%2C%20y%29%20%5Cin%20D%20%5Cmid%20A(x)%3Da%5C%7D%2C%20%5Cquad%20D_%7B2%7D%3DD-D_%7B1%7D%5Cnonumber%0A" /></p><p>则在特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 的条件下，集合 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 的基尼指数定义为</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7BGini%7D%28D%2C%20A%29%3D%5Cfrac%7B%5Cleft%7CD_%7B1%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Coperatorname%7BGini%7D%5Cleft(D_%7B1%7D%5Cright)%2B%5Cfrac%7B%5Cleft%7CD_%7B2%7D%5Cright%7C%7D%7B%7CD%7C%7D%20%5Coperatorname%7BGini%7D%5Cleft(D_%7B2%7D%5Cright)%0A" /></p></blockquote>
<ul>
<li>基尼指数表示集合的不确定性。基尼指数越大，集合的不确定性越大。和熵相似。</li>
<li>二元分类中基尼指数、单位比特熵和分类误差的关系。x轴：概率p。y轴：损失。<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%9F%BA%E5%B0%BC%E3%80%81%E7%86%B5%E4%B8%8E%E5%88%86%E7%B1%BB%E8%AF%AF%E5%B7%AE.png" alt="image-20201208142218988" style="zoom:50%;" /></li>
</ul>
<h5 id="6-1-2-2-Algorithm">6.1.2.2 Algorithm</h5>
<blockquote>
<p>算法 5.6 (CART 生成算法)</p>
<p>输入: 训练数据集 <img src="https://math.now.sh?inline=D%2C" style="display:inline-block;margin: 0;"/> 停止计算的条件;<br>
输出: CART 决策树。</p>
<p>根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树:<br>
（1） 设结点的训练数据集为 <img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/>, 计算现有特征对该数据集的基尼指数。此时，对每一个特征 <img src="https://math.now.sh?inline=A%2C" style="display:inline-block;margin: 0;"/> 对其可能取的每个值 <img src="https://math.now.sh?inline=a%2C" style="display:inline-block;margin: 0;"/> 根据样本点对 <img src="https://math.now.sh?inline=A%3Da" style="display:inline-block;margin: 0;"/> 的测试为“是”或“否”，将<img src="https://math.now.sh?inline=D" style="display:inline-block;margin: 0;"/> 分割成 <img src="https://math.now.sh?inline=D_%7B1%7D" style="display:inline-block;margin: 0;"/> 和 <img src="https://math.now.sh?inline=D_%7B2%7D" style="display:inline-block;margin: 0;"/> 两部分，利用式 (20) 计算 <img src="https://math.now.sh?inline=A%3Da" style="display:inline-block;margin: 0;"/> 时的基尼指数。<br>
（2）在所有可能的特征 <img src="https://math.now.sh?inline=A" style="display:inline-block;margin: 0;"/> 以及它们所有可能的切分点 <img src="https://math.now.sh?inline=a" style="display:inline-block;margin: 0;"/> 中，选择基尼指数最小的<br>
持征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现<br>
结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。<br>
（3）对两个子结点递归地调用 ( 1 )， (2) , 直至满足停止条件。<br>
（4）生成 CART 决策树。</p>
</blockquote>
<h3 id="6-2-CART的剪枝">6.2 CART的剪枝</h3>
<p><strong>步骤：</strong></p>
<ol>
<li>对生成算法产生的决策树<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>底端开始不断剪枝，直到<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>的根结点，形成子树序列<img src="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/></li>
<li>通过交叉验证法，在独立的验证数据集上对字数序列进行测试，选择最优子书。</li>
</ol>
<h4 id="6-2-1-剪枝成一个子树序列">6.2.1 剪枝成一个子树序列</h4>
<p>子树的损失函数为</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%28T%29%3DC(T)%2B%5Calpha%7CT%7C%5Cnonumber%0A" /></p><ul>
<li>
<p><img src="https://math.now.sh?inline=C%28T%29%3D%5Csum_%7Bt%3D1%7D%5E%7B%7CT%7C%7D%20N_%7Bt%7D%5Cleft(1-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Cleft(%5Cfrac%7BN_%7Bt%20k%7D%7D%7BN_%7Bt%7D%7D%5Cright)%5E%7B2%7D%5Cright)%2C%7CT%7C" style="display:inline-block;margin: 0;"/> 是叶结点个数，<img src="https://math.now.sh?inline=K" style="display:inline-block;margin: 0;"/> 是类别个数</p>
</li>
<li>
<p>定义推导同(5)-(8)</p>
</li>
<li>
<p>对固定<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>，<strong>唯一存在</strong>最优子树<img src="https://math.now.sh?inline=T_%7B%5Calpha%7D" style="display:inline-block;margin: 0;"/>，使得损失函数<img src="https://math.now.sh?inline=C_%7B%5Calpha%7D%28T%29" style="display:inline-block;margin: 0;"/>最小。</p>
<ul>
<li>此处“最优”的意义是指使得损失函数最小</li>
<li><img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>越大，最优子树<img src="https://math.now.sh?inline=T_%7B%5Calpha%7D" style="display:inline-block;margin: 0;"/>越小。当 <img src="https://math.now.sh?inline=%5Calpha%20%5Crightarrow%20%5Cinfty" style="display:inline-block;margin: 0;"/> 时，叶结点不断被剪，根结点组成的单结点树是最优的。</li>
</ul>
</li>
</ul>
<p>对一个整体树<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>，它的子树是<strong>有限个</strong>的。因此，对一个连续参数<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>，我们得到了最优子树<img src="https://math.now.sh?inline=T'%28%5Calpha%29" style="display:inline-block;margin: 0;"/>。<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>不断增大，在增大到**跳跃点<img src="https://math.now.sh?inline=%5Calpha'" style="display:inline-block;margin: 0;"/>**之前，<img src="https://math.now.sh?inline=T'%28%5Calpha%29" style="display:inline-block;margin: 0;"/>依然是最优子树。即<img src="https://math.now.sh?inline=T'%28%5Calpha%29%3DT'(%5Calpha%2B%5CDelta%5Calpha)" style="display:inline-block;margin: 0;"/>。再跳跃点之后，易知最优子树<img src="https://math.now.sh?inline=T'%28%5Calpha%E2%80%99%29%5Cin%20T'(%5Calpha)" style="display:inline-block;margin: 0;"/>。</p>
<p>上面可以表述为：将 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/> 从小增大, <img src="https://math.now.sh?inline=0%3D%5Calpha_%7B0%7D%3C" style="display:inline-block;margin: 0;"/><img src="https://math.now.sh?inline=%5Calpha_%7B1%7D%3C%5Ccdots%3C%5Calpha_%7Bn%7D%3C%2B%5Cinfty%2C" style="display:inline-block;margin: 0;"/> 产生一系列的区间 <img src="https://math.now.sh?inline=%5Cleft%5B%5Calpha_%7Bi%7D%2C%20%5Calpha_%7Bi%2B1%7D%5Cright%29%2C%20i%3D0%2C1%2C%20%5Ccdots%2C%20n%20%3B" style="display:inline-block;margin: 0;"/> 剪枝得到的子树序列对应着区间 <img src="https://math.now.sh?inline=%5Calpha%20%5Cin%5Cleft%5B%5Calpha_%7Bi%7D%2C%20%5Calpha_%7Bi%2B1%7D%5Cright%29%2C%20i%3D0%2C1%2C%20%5Ccdots%2C%20n" style="display:inline-block;margin: 0;"/> 的最优子树序列 <img src="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D%2C" style="display:inline-block;margin: 0;"/> <strong>序列中的子树是嵌套的</strong>。</p>
<p>具体来说，从整体树<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=%5Calpha%3D0" style="display:inline-block;margin: 0;"/>开始剪枝。对<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>内的任意内部结点<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>：</p>
<p>以<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>为单结点树的损失函数为</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%28t%29%3DC(t)%2B%5Calpha%0A" /></p><p>以 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 为根结点的子树 <img src="https://math.now.sh?inline=T_%7Bt%7D" style="display:inline-block;margin: 0;"/> 的损失函数为</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%5Cleft%28T_%7Bt%7D%5Cright%29%3DC%5Cleft(T_%7Bt%7D%5Cright)%2B%5Calpha%5Cleft%7CT_%7Bt%7D%5Cright%7C%0A" /></p><p>当 <img src="https://math.now.sh?inline=%5Calpha%3D0" style="display:inline-block;margin: 0;"/> 及 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/> 充分小时，有不等式</p>
<p style=""><img src="https://math.now.sh?from=C_%7B%5Calpha%7D%5Cleft%28T_%7Bt%7D%5Cright%29%3CC_%7B%5Calpha%7D(t)%0A" /></p><p>当 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/> 增大时，在某一 <img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/> 有</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Balign%7D%0AC_%7B%5Calpha%7D%5Cleft%28T_%7Bt%7D%5Cright%29%20%26%3DC_%7B%5Calpha%7D(t)%5C%5C%0A%5Calpha%20%26%3D%5Cfrac%7BC(t)-C%5Cleft(T_%7Bt%7D%5Cright)%7D%7B%5Cleft%7CT_%7Bt%7D%5Cright%7C-1%7D%0A%5Cend%7Balign%7D%0A" /></p><p>**此时<img src="https://math.now.sh?inline=T_t" style="display:inline-block;margin: 0;"/>和<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>有相同的损失函数值，因为<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>的结点更少，所以取<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>，剪去以 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 为根结点的子树 <img src="https://math.now.sh?inline=T_%7Bt%7D" style="display:inline-block;margin: 0;"/> **</p>
<p>根据这个性质，我们可以找到系列区间以及对应的最优子树序列</p>
<p>对<img src="https://math.now.sh?inline=T_0" style="display:inline-block;margin: 0;"/>的每一个内部节点<img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/>，计算</p>
<p style=""><img src="https://math.now.sh?from=g%28t%29%3D%5Cfrac%7BC(t)-C%5Cleft(T_%7Bt%7D%5Cright)%7D%7B%5Cleft%7CT_%7Bt%7D%5Cright%7C-1%7D%0A" /></p><p><img src="https://math.now.sh?inline=g%28t%29" style="display:inline-block;margin: 0;"/>表示剪枝后整体损失函数减小的程度。在 <img src="https://math.now.sh?inline=T_%7B0%7D" style="display:inline-block;margin: 0;"/> 中剪去 <img src="https://math.now.sh?inline=g%28t%29" style="display:inline-block;margin: 0;"/> 最小的 <img src="https://math.now.sh?inline=T_%7Bt%7D%2C" style="display:inline-block;margin: 0;"/> 将得到的子树作为 <img src="https://math.now.sh?inline=T_%7B1%7D%2C" style="display:inline-block;margin: 0;"/> 同时将最小的 <img src="https://math.now.sh?inline=g%28t%29" style="display:inline-block;margin: 0;"/> 设为 <img src="https://math.now.sh?inline=%5Calpha_%7B1%7D" style="display:inline-block;margin: 0;"/>。$ T_{1}$ 为区间 <img src="https://math.now.sh?inline=%5Cleft%5B%5Calpha_%7B1%7D%2C%20%5Calpha_%7B2%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 的最优子树。循环直到得到根结点。</p>
<ul>
<li>在这个过程中，<img src="https://math.now.sh?inline=g%28t%29%3D%5Calpha" style="display:inline-block;margin: 0;"/>是不断增大的？</li>
</ul>
<h4 id="6-2-2-交叉验证">6.2.2 交叉验证</h4>
<p>利用独立的验证数据集，测试子树序列 <img src="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/>中每个子树的平方误差/基尼指数，选择最优决策树<img src="https://math.now.sh?inline=T_%7B%5Calpha%7D" style="display:inline-block;margin: 0;"/>。</p>
<ul>
<li>子树序列 <img src="https://math.now.sh?inline=%5Cleft%5C%7BT_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/>在剪枝的时候是对应<img src="https://math.now.sh?inline=%5Calpha" style="display:inline-block;margin: 0;"/>的最优子序列</li>
</ul>
<h4 id="6-2-3-Algorithm">6.2.3 Algorithm</h4>
<blockquote>
<p>算法 5.7 (CART 剪枝算法)<br>
输入: CART 算法生成的决策树 <img src="https://math.now.sh?inline=T_%7B0%7D" style="display:inline-block;margin: 0;"/>;<br>
输出：最优决策树 <img src="https://math.now.sh?inline=T_%7B%5Calpha%20%5Ccirc%7D" style="display:inline-block;margin: 0;"/><br>
(1) 设 <img src="https://math.now.sh?inline=k%3D0%2C%20T%3DT_%7B0%7D" style="display:inline-block;margin: 0;"/> 。<br>
(2) 设 <img src="https://math.now.sh?inline=%5Calpha%3D%2B%5Cinfty" style="display:inline-block;margin: 0;"/> 。<br>
(3) 自下而上地对各内部结点 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 计算 <img src="https://math.now.sh?inline=C%5Cleft%28T_%7Bt%7D%5Cright%29%2C%5Cleft%7CT_%7Bt%7D%5Cright%7C" style="display:inline-block;margin: 0;"/> 以及</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0A%0Ag%28t%29%20%26%3D%5Cfrac%7BC(t)-C%5Cleft(T_%7Bt%7D%5Cright)%7D%7B%5Cleft%7CT_%7Bt%7D%5Cright%7C-1%7D%20%5C%5C%0A%5Calpha%20%26%3D%5Cmin%20(%5Calpha%2C%20g(t))%0A%5Cend%7Baligned%7D%0A" /></p><p>这里, <img src="https://math.now.sh?inline=T_%7Bt%7D" style="display:inline-block;margin: 0;"/> 表示以 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 为根结点的子树, <img src="https://math.now.sh?inline=C%5Cleft%28T_%7Bt%7D%5Cright%29" style="display:inline-block;margin: 0;"/> 是对训练数据的预测误差, <img src="https://math.now.sh?inline=%5Cleft%7CT_%7Bt%7D%5Cright%7C" style="display:inline-block;margin: 0;"/> 是 <img src="https://math.now.sh?inline=T_%7Bt%7D" style="display:inline-block;margin: 0;"/>的叶结点个数。<br>
(4) 对 <img src="https://math.now.sh?inline=g%28t%29%3D%5Calpha" style="display:inline-block;margin: 0;"/> 的内部结点 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 进行剪枝，并对叶结点 <img src="https://math.now.sh?inline=t" style="display:inline-block;margin: 0;"/> 以多数表决法决定其类，得到树 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 。<br>
(5) 设 <img src="https://math.now.sh?inline=k%3Dk%2B1%2C%20%5Calpha_%7Bk%7D%3D%5Calpha%2C%20T_%7Bk%7D%3DT_%7B%5Ccirc%7D" style="display:inline-block;margin: 0;"/><br>
(6) 如果 <img src="https://math.now.sh?inline=T_%7Bk%7D" style="display:inline-block;margin: 0;"/> 不是由根结点及两个叶结点构成的树，则回到步骤 (2)<img src="https://math.now.sh?inline=%3B" style="display:inline-block;margin: 0;"/> 否则令 <img src="https://math.now.sh?inline=T_%7Bk%7D%3DT_%7Bn%7D" style="display:inline-block;margin: 0;"/><br>
(7) 采用交义验证法在子树序列 <img src="https://math.now.sh?inline=T_%7B0%7D%2C%20T_%7B1%7D%2C%20%5Ccdots%2C%20T_%7Bn%7D" style="display:inline-block;margin: 0;"/> 中选取最优子树 <img src="https://math.now.sh?inline=T_%7B%5Calpha%5E%7B%5Ccirc%7D%7D" style="display:inline-block;margin: 0;"/></p>
</blockquote>
<h2 id="7-Question">7. Question</h2>
<ol>
<li>为什么<img src="https://math.now.sh?inline=C%28T%29" style="display:inline-block;margin: 0;"/>能表示模型对训练数据的预测误差</li>
<li><s>正则化的极大似然估计？</s></li>
<li><s><strong>1.2</strong>中的’构成一个条件概率分布’，不是叶结点咋办？</s></li>
<li><s>6.1.1 启发式算法？</s></li>
<li>CART决策时有没有可能对一个特征二叉再接个二叉，成<img src="https://math.now.sh?inline=2%5E2" style="display:inline-block;margin: 0;"/>个叉？</li>
</ol>
<h2 id="8-Code">8. Code</h2>
<h3 id="8-1-ID3-create-test-visualization">8.1 ID3: create, test, visualization</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">CARTClassification</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">CARTRegression</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ID3_create</span>(<span class="params">self, train_set, features, labels, tol=[<span class="number">0.1</span>, <span class="number">2</span>], visible=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        create ID3 tree</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray. m: samples, n: features</span></span><br><span class="line"><span class="string">        :param features: n size vector</span></span><br><span class="line"><span class="string">        :param labels: m size ndarray</span></span><br><span class="line"><span class="string">        :param tol: tolerate for pre-pruning</span></span><br><span class="line"><span class="string">        :return: ID3 tree in dict type</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        three conditions to stop iteration:</span></span><br><span class="line"><span class="string">          1. all labelss are the same</span></span><br><span class="line"><span class="string">          2. no feature</span></span><br><span class="line"><span class="string">          3. info_gain &lt; tol[0], samples &gt; tol[1]. pre-pruning</span></span><br><span class="line"><span class="string">          4. same training values but different labels</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        train_set = np.array(train_set)</span><br><span class="line">        labels = np.array(labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(np.unique(labels)) == <span class="number">1</span>:  <span class="comment"># condition 1</span></span><br><span class="line">            <span class="keyword">return</span> labels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> train_set.shape[<span class="number">1</span>] == <span class="number">0</span>:  <span class="comment"># condition 2</span></span><br><span class="line">            <span class="keyword">return</span> np.sort(labels)[-<span class="number">1</span>]  <span class="comment"># return the most frequency value</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># condition 3 &amp; 4</span></span><br><span class="line">        <span class="comment"># not finished</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get best feature</span></span><br><span class="line">        best_feature_index, best_info_gain = self.ID3_best_feature(train_set, labels)</span><br><span class="line">        best_feature_name = features[best_feature_index]</span><br><span class="line">        print(<span class="string">&#x27;best selected feature is &#x27;</span>, best_feature_name, <span class="string">&#x27;, its information gain is &#x27;</span>, best_info_gain)</span><br><span class="line"></span><br><span class="line">        ID3Tree = &#123;best_feature_name: &#123;&#125;&#125;  <span class="comment"># return feature name as a dict key</span></span><br><span class="line">        <span class="comment"># return unique values under the feature and as the node(key)</span></span><br><span class="line">        tree_nodes = np.unique(train_set.T[best_feature_index])</span><br><span class="line">        <span class="comment"># small feature set for dealing feature set depending on its index</span></span><br><span class="line">        features = np.delete(features, best_feature_index)</span><br><span class="line">        <span class="comment"># iteration in these nodes</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> tree_nodes:</span><br><span class="line">            train_sample_index = train_set.T[best_feature_index] == node</span><br><span class="line">            node_labels = labels[train_sample_index]</span><br><span class="line">            <span class="comment"># small train set with node feature&#x27;s column equal node value</span></span><br><span class="line">            node_train_set = self.spilt_dataset(train_set, best_feature_index, node)</span><br><span class="line">            <span class="comment"># small train set without node feature</span></span><br><span class="line">            node_train_set = np.delete(node_train_set, best_feature_index, axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># iteration</span></span><br><span class="line">            ID3Tree[best_feature_name][node] = self.ID3_create(node_train_set, features, node_labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> visible <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            treePlotter.ID3_Tree(ID3Tree)</span><br><span class="line">        <span class="keyword">return</span> ID3Tree</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">array</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        calculate bit entropy</span></span><br><span class="line"><span class="string">        :param array: 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: entropy in bit</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        count_array = np.unique(array, return_counts=<span class="literal">True</span>)[<span class="number">1</span>]  <span class="comment"># unique values and its occurrences</span></span><br><span class="line">        probability = count_array / array.size  <span class="comment"># probability of values</span></span><br><span class="line">        h_p = np.dot(-probability, np.log2(probability))  <span class="comment"># entropy</span></span><br><span class="line">        <span class="keyword">return</span> h_p</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conditional_entropy</span>(<span class="params">Y, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        get conditional entropy H(Y|X)</span></span><br><span class="line"><span class="string">        :param Y: random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :param X: given random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: conditional entropy of Y given X, H(Y|X)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        Y = np.array(Y)</span><br><span class="line">        X = np.array(X)</span><br><span class="line">        hY_X = <span class="number">0</span>  <span class="comment"># initialization</span></span><br><span class="line">        X_value, X_count = np.unique(X, return_counts=<span class="literal">True</span>)  <span class="comment"># unique values and its occurrences</span></span><br><span class="line">        <span class="keyword">for</span> xi <span class="keyword">in</span> X_value:</span><br><span class="line">            index = np.argwhere(X == xi)  <span class="comment"># get index of X=xi</span></span><br><span class="line">            p_xi = index.size / X.size  <span class="comment"># P(X=xi)</span></span><br><span class="line">            Yi = Y[index]  <span class="comment"># get yi given xi</span></span><br><span class="line">            hYi_xi = DecisionTree.entropy(np.array(Yi))  <span class="comment"># H(Y|X=xi)</span></span><br><span class="line">            hY_X += p_xi * hYi_xi</span><br><span class="line">        <span class="keyword">return</span> hY_X</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain</span>(<span class="params">Y, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        get information gain G(Y,X)</span></span><br><span class="line"><span class="string">        :param Y: random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :param X: given random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: information gain of Y given X, G(Y|X)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> DecisionTree.entropy(Y) - DecisionTree.conditional_entropy(Y, X)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spilt_dataset</span>(<span class="params">dataset, colume, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dataset with small samples</span></span><br><span class="line"><span class="string">        :param dataset: m*n ndarray</span></span><br><span class="line"><span class="string">        :param colume:  axis</span></span><br><span class="line"><span class="string">        :param value: compared value</span></span><br><span class="line"><span class="string">        :return: l*n ndarray, l&lt;m</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dataset = pd.DataFrame(dataset)</span><br><span class="line">        df = dataset[dataset[colume] == value]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.array(df)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ID3_best_feature</span>(<span class="params">train_set, labels</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        return the feature with the highest infomation gain</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray. m: samples, n: features</span></span><br><span class="line"><span class="string">        :param labels: m size ndarray.</span></span><br><span class="line"><span class="string">        :return: best feature index and its infomation gain</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        features = train_set.shape[<span class="number">1</span>]  <span class="comment"># number of features</span></span><br><span class="line">        tmp = np.ones(features) * -<span class="number">1</span>  <span class="comment"># store info gain</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(features):  <span class="comment"># calculate info gain of each features</span></span><br><span class="line">            feature_list = train_set.T[i]</span><br><span class="line">            gain = DecisionTree.info_gain(labels, feature_list)</span><br><span class="line">            tmp[i] = gain</span><br><span class="line">            print(<span class="string">&quot;the info gain of %d th feature in ID3 is: %.3f&quot;</span> % (i, gain))</span><br><span class="line">        best_feature = np.argmax(tmp)</span><br><span class="line">        best_info_gain = tmp[best_feature]</span><br><span class="line">        <span class="keyword">return</span> best_feature, best_info_gain</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">tree, sample, features</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param tree: dict</span></span><br><span class="line"><span class="string">        :param sample: 1-d ndarray</span></span><br><span class="line"><span class="string">        :param features: 1-d ndarray</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        first_str = <span class="built_in">list</span>(tree.keys())[<span class="number">0</span>] <span class="comment"># root name</span></span><br><span class="line">        small_tree = tree[first_str] <span class="comment">#</span></span><br><span class="line">        feature_index = features.index(first_str)</span><br><span class="line">        label = <span class="string">&#x27;None&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> small_tree.keys():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(sample[feature_index]) == key:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">type</span>(small_tree[key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:</span><br><span class="line">                    label = DecisionTree.classify(small_tree[key], sample, features)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    label = small_tree[key]</span><br><span class="line">        <span class="keyword">return</span> label</span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<h2 id="9-Reference">9. Reference</h2>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/93667566">分类与回归树（classification and regression tree，CART）之回归</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/101721467">回归树</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/93667566">分类与回归树（classification and regression tree，CART）之回归</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Erikfather/Decision_tree-python">Erikfather/Decision_tree-python</a></p>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/27/[SLM-4]%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%20Naive%20Bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/27/%5BSLM-4%5D%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%20Naive%20Bayes/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-27 00:11:26" itemprop="dateCreated datePublished" datetime="2020-11-27T00:11:26+08:00">2020-11-27</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-07 16:11:16" itemprop="dateModified" datetime="2020-12-07T16:11:16+08:00">2020-12-07</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1>朴素贝叶斯 Naive Bayes</h1>
<p>基于Bayes Theorem和特征条件独立假设的分类方法。</p>
<h2 id="0-Bayes-Theorem">0. Bayes Theorem</h2>
<p style=""><img src="https://math.now.sh?from=%5Cunderbrace%7BP%28X%7CY%29%7D_%7Bposterior%7D%3D%5Cfrac%7B%5Coverbrace%7BP(Y%7CX)%7D%5E%7Blikelihood%7D%5Coverbrace%7BP(X)%7D%5E%7Bprior%7D%7D%7B%5Cunderbrace%7BP(Y)%7D_%7Bevidence%7D%7D%3D%5Cfrac%7B%5Coverbrace%7BP(Y%7CX)%7D%5E%7Blikelihood%7D%5Coverbrace%7BP(X)%7D%5E%7Bprior%7D%7D%7B%5Cunderbrace%7B%5Csum%5Climits_x%20P(Y%7CX)P(X)%7D_%7Bevidence%7D%7D%0A" /></p><ul>
<li><img src="https://math.now.sh?inline=P%28X%29" style="display:inline-block;margin: 0;"/>：先验概率prior</li>
<li><img src="https://math.now.sh?inline=P%28X%7CY%29" style="display:inline-block;margin: 0;"/>：后验概率posterior</li>
<li><img src="https://math.now.sh?inline=P%28Y%7CX%29" style="display:inline-block;margin: 0;"/>：似然likelihood。</li>
</ul>
<h2 id="1-Model">1. Model</h2>
<ul>
<li>
<p>输入空间<img src="https://math.now.sh?inline=%5Cmathcal%7BX%7D%20%5Csubseteq%20%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/> 为 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"/> 维向量的集合，输出空间为类集合<img src="https://math.now.sh?inline=%5Cmathcal%7BY%7D%3D%5Cleft%5C%7Bc_%7B1%7D%2C%20c_%7B2%7D%2C%20%5Ccdots%2C%20c_%7BK%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>输入特征向量<img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/>，输出类标记<img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p><img src="https://math.now.sh?inline=X" style="display:inline-block;margin: 0;"/>是输入空间上的随机向量，<img src="https://math.now.sh?inline=Y" style="display:inline-block;margin: 0;"/>是输出空间上的随机变量</p>
</li>
<li>
<p>训练数据集</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0AT%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A" /></p><p>由 <img src="https://math.now.sh?inline=P%28X%2C%20Y%29" style="display:inline-block;margin: 0;"/> 独立同分布产生。</p>
<ul>
<li>这边的独立指的是向量<img src="https://math.now.sh?inline=x_1" style="display:inline-block;margin: 0;"/>与<img src="https://math.now.sh?inline=x_2" style="display:inline-block;margin: 0;"/>等之间的独立，不是特征条件独立。</li>
</ul>
</li>
</ul>
<h3 id="1-1-学习联合概率分布P-X-Y">1.1 学习联合概率分布<img src="https://math.now.sh?inline=P%28X%2CY%29" style="display:inline-block;margin: 0;"/></h3>
<ol>
<li>
<p><img src="https://math.now.sh?inline=P%28X%2C%20Y%29%3DP(Y)%20P(X%20%5Cmid%20Y)%3DP(X)%20P(Y%20%5Cmid%20X)" style="display:inline-block;margin: 0;"/>，用第一个等式</p>
</li>
<li>
<p><strong>先验概率分布</strong></p>
<p style=""><img src="https://math.now.sh?from=P%5Cleft%28Y%3Dc_%7Bk%7D%5Cright%29%2C%20%5Cquad%20k%3D1%2C2%2C%20%5Ccdots%2C%20K%0A" /></p><ul>
<li>由测试集可知，有多少概率/比例的<img src="https://math.now.sh?inline=c_1" style="display:inline-block;margin: 0;"/>标签</li>
</ul>
</li>
<li>
<p><strong>条件概率分布</strong></p>
<p style=""><img src="https://math.now.sh?from=P%5Cleft%28X%3Dx%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright%29%3DP%5Cleft(X%5E%7B(1)%7D%3Dx%5E%7B(1)%7D%2C%20%5Ccdots%2C%20X%5E%7B(n)%7D%3Dx%5E%7B(n)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%2C%5C%5C%20%5Cquad%20k%3D1%2C2%2C%20%5Ccdots%2C%20K%0A" /></p><ul>
<li>
<p>特征向量相等<img src="https://math.now.sh?inline=%5CLongrightarrow" style="display:inline-block;margin: 0;"/>每一维度上都相等</p>
</li>
<li>
<p>朴素贝叶斯法对条件概率做条件独立性假设，即</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Balign%7D%0AP%5Cleft%28X%3Dx%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright%29%20%26%3DP%5Cleft(X%5E%7B(1)%7D%3Dx%5E%7B(1)%7D%2C%20%5Ccdots%2C%20X%5E%7B(n)%7D%3Dx%5E%7B(n)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%5Cnonumber%20%5C%5C%0A%26%3D%5Cprod_%7Bj%3D1%7D%5E%7Bn%7D%20P%5Cleft(X%5E%7B(j)%7D%3Dx%5E%7B(j)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%0A%5Cend%7Balign%7D%0A" /></p></li>
<li>
<p>如果不加独立分布假设，那么(1,1,1,1)和(1,1,1,2)独立，没有可以利用的信息。需要更大的测试集。</p>
</li>
</ul>
</li>
<li>
<p><strong>重复“词语”的处理</strong></p>
<p>我们得到一个垃圾邮件向量<img src="https://math.now.sh?inline=x%3D%5B'%E5%8F%91%E7%A5%A8'%EF%BC%8C%E2%80%98%E5%8F%91%E7%A5%A8%E2%80%99%EF%BC%8C%E2%80%98%E5%8F%91%E7%A5%A8%E2%80%98%EF%BC%8C%E2%80%98%E5%85%85%E5%80%BC%E2%80%99%EF%BC%8C%E2%80%99%E8%B4%AD%E7%89%A9%E2%80%98%5D" style="display:inline-block;margin: 0;"/>，如何处理出现3次的“发票”</p>
<ol>
<li>多项式模型
<ul>
<li>每一个“发票”都是独立的，统计多次，即<img src="https://math.now.sh?inline=P%28X%3D('%E5%8F%91%E7%A5%A8'%EF%BC%8C%E2%80%98%E5%8F%91%E7%A5%A8%E2%80%99%EF%BC%8C%E2%80%98%E5%8F%91%E7%A5%A8%E2%80%98%29%7CY%3Dc_k)%3DP%5E3('%E5%8F%91%E7%A5%A8%7CY%3Dc_k')" style="display:inline-block;margin: 0;"/></li>
</ul>
</li>
<li>伯努利模型
<ul>
<li>只分成出现，不出现的二元情况，即<img src="https://math.now.sh?inline=P%28X%3D('%E5%8F%91%E7%A5%A8'%EF%BC%8C%E2%80%98%E5%8F%91%E7%A5%A8%E2%80%99%EF%BC%8C%E2%80%98%E5%8F%91%E7%A5%A8%E2%80%98%29%7CY%3Dc_k)%3DP('%E5%8F%91%E7%A5%A8%7CY%3Dc_k')" style="display:inline-block;margin: 0;"/></li>
</ul>
</li>
<li>混合模型
<ul>
<li>计算单一句子的概率时用伯努利模型</li>
<li>计算全文词语的概率时用多项式模型</li>
</ul>
</li>
<li>高斯模型
<ul>
<li>适用于连续变量</li>
<li>假设：在给定一个类别<img src="https://math.now.sh?inline=c_k" style="display:inline-block;margin: 0;"/>后，各个特征符合正态分布</li>
<li><img src="https://math.now.sh?inline=P%5Cleft%28x%5E%7B(i%29%7D%20%5Cmid%20y%3Dc_k%5Cright)%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma_%7By%7D%5E%7B2%7D%7D%7D%20%5Cexp%20%5Cleft(-%5Cfrac%7B%5Cleft(x%5E%7B(i)%7D-%5Cmu_%7By%7D%5Cright)%5E%7B2%7D%7D%7B2%20%5Csigma_%7By%7D%5E%7B2%7D%7D%5Cright)" style="display:inline-block;margin: 0;"/>
<ul>
<li><img src="https://math.now.sh?inline=%5Cmu_y%3A" style="display:inline-block;margin: 0;"/> 在类别为 <img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/> 的样本中，特征 <img src="https://math.now.sh?inline=x%5E%7B%28i%29%7D" style="display:inline-block;margin: 0;"/> 的均值。<br>
<img src="https://math.now.sh?inline=%5Csigma_%7By%7D%3A" style="display:inline-block;margin: 0;"/> 在类别为 <img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/> 的样本中，特征 <img src="https://math.now.sh?inline=x%5E%7B%28i%29%7D" style="display:inline-block;margin: 0;"/> 的标准差。</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="1-2-计算后验分布">1.2 计算后验分布</h3>
<p>通过贝叶斯定理，得到在给定了一个特征向量的情况下，标签为<img src="https://math.now.sh?inline=c_k" style="display:inline-block;margin: 0;"/>的概率。</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Balign%7D%0AP%28Y%20%5Cmid%20X%29%26%3D%5Cfrac%7BP(X%2C%20Y)%7D%7BP(X)%7D%3D%5Cfrac%7BP(Y)%20P(X%20%5Cmid%20Y)%7D%7B%5Csum_%7BY%7D%20P(Y)%20P(X%20%5Cmid%20Y)%7D%5C%5C%0AP%5Cleft(Y%3Dc_%7Bk%7D%20%5Cmid%20X%3Dx%5Cright)%26%3D%5Cfrac%7BP%5Cleft(X%3Dx%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%20P%5Cleft(Y%3Dc_%7Bk%7D%5Cright)%7D%7B%5Csum_%7Bk%7D%20P%5Cleft(X%3Dx%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%20P%5Cleft(Y%3Dc_%7Bk%7D%5Cright)%7D%5C%5C%0A%26%3D%5Cfrac%7BP%5Cleft(Y%3Dc_%7Bk%7D%5Cright)%20%5Cprod%20P%5Cleft(X%5E%7B(j)%7D%3Dx%5E%7B(j)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%7D%7B%5Csum_%7Bk%7D%20(P%5Cleft(Y%3Dc_%7Bk%7D%5Cright)%20%5Cprod_%7Bj%7D%20P%5Cleft(X%5E%7B(j)%7D%3Dx%5E%7B(j)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright))%7D%2C%20%5Cquad%20k%3D1%2C2%2C%20%5Ccdots%2C%20K%0A%5Cend%7Balign%7D%0A" /></p><ul>
<li>把(2)带入到(5)，得到等式(6)</li>
</ul>
<h3 id="1-3-后验概率最大化">1.3 后验概率最大化</h3>
<p>得到了概率表达式之后，我们便去寻找使得概率<img src="https://math.now.sh?inline=P%5Cleft%28Y%3Dc_%7Bk%7D%20%5Cmid%20X%3Dx%5Cright%29" style="display:inline-block;margin: 0;"/>最大的标签<img src="https://math.now.sh?inline=c_k" style="display:inline-block;margin: 0;"/></p>
<p>朴素贝叶斯分类器：</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bl%7D%0Ay%3Df%28x%29%3D%5Carg%20%5Cmax%20_%7Bc_%7Bk%7D%7D%20%5Cfrac%7BP%5Cleft(Y%3Dc_%7Bk%7D%5Cright)%20%5Cprod_%7Bj%7D%20P%5Cleft(X%5E%7B(j)%7D%3Dx%5E%7B(j)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%20%5C%5C%20%7D%7B%5Csum_%7Bk%7D%20P%5Cleft(Y%3Dc_%7Bk%7D%5Cright)%20%5Cprod_%7Bj%7D%20P%5Cleft(X%5E%7B(j)%7D%3Dx%5E%7B(j)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%7D%0A%5Cend%7Barray%7D%0A" /></p><p>(7)中的分母本质上为<img src="https://math.now.sh?inline=P%28X%3Dx%29" style="display:inline-block;margin: 0;"/>，与<img src="https://math.now.sh?inline=c_k" style="display:inline-block;margin: 0;"/>的取值无关，故(7)可表示成</p>
<p style=""><img src="https://math.now.sh?from=y%3D%5Carg%20%5Cmax%20_%7Bc_%7Bk%7D%7D%20P%5Cleft%28Y%3Dc_%7Bk%7D%5Cright%29%20%5Cprod_%7Bj%7D%20P%5Cleft(X%5E%7B(j)%7D%3Dx%5E%7B(j)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%0A" /></p><h4 id="1-3-1-含义">1.3.1 含义</h4>
<ul>
<li>后验概率最大化等价于期望风险最小化</li>
</ul>
<h3 id="1-4-朴素贝叶斯法中的参数估计">1.4 朴素贝叶斯法中的参数估计</h3>
<p>为了学习朴素贝叶斯法中的<img src="https://math.now.sh?inline=P%5Cleft%28Y%3Dc_%7Bk%7D%5Cright%29" style="display:inline-block;margin: 0;"/>与<img src="https://math.now.sh?inline=P%5Cleft%28X%3Dx%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright%29" style="display:inline-block;margin: 0;"/></p>
<h4 id="1-4-1-极大似然估计">1.4.1 极大似然估计</h4>
<p>核心思想为根据以有的测试数据集，得到每一类的频率，即为概率</p>
<blockquote>
<p>算法 4.1 (朴素贝叶斯算法 (Naive Bayes algorithm) )</p>
<p>输入: 训练数据 <img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%2C" style="display:inline-block;margin: 0;"/> 其中 <img src="https://math.now.sh?inline=x_%7Bi%7D%3D%5Cleft%28x_%7Bi%7D%5E%7B(1%29%7D%2C%20x_%7Bi%7D%5E%7B(2)%7D%2C%20%5Ccdots%2C%5Cright." style="display:inline-block;margin: 0;"/><br>
<img src="https://math.now.sh?inline=%5Cleft.x_%7Bi%7D%5E%7B%28n%29%7D%5Cright)%5E%7B%5Cmathrm%7BT%7D%7D%2C%20x_%7Bi%7D%5E%7B(j)%7D" style="display:inline-block;margin: 0;"/> 是第 <img src="https://math.now.sh?inline=i" style="display:inline-block;margin: 0;"/> 个样本的第 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/> 个特征, <img src="https://math.now.sh?inline=x_%7Bi%7D%5E%7B%28j%29%7D%20%5Cin%5Cleft%5C%7Ba_%7Bj%201%7D%2C%20a_%7Bj%202%7D%2C%20%5Ccdots%2C%20a_%7Bj%20S_%7Bj%7D%7D%5Cright%5C%7D%2C%20a_%7Bj%20l%7D" style="display:inline-block;margin: 0;"/> 是第 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/> 个特<br>
征可能取的第 <img src="https://math.now.sh?inline=l" style="display:inline-block;margin: 0;"/> 个值, <img src="https://math.now.sh?inline=j%3D1%2C2%2C%20%5Ccdots%2C%20n%2C%20l%3D1%2C2%2C%20%5Ccdots%2C%20S_%7Bj%7D%2C%20y_%7Bi%7D%20%5Cin%5Cleft%5C%7Bc_%7B1%7D%2C%20c_%7B2%7D%2C%20%5Ccdots%2C%20c_%7BK%7D%5Cright%5C%7D%20%3B" style="display:inline-block;margin: 0;"/> 实例 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/>;<br>
输出：实例 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的分类。<br>
（1）计算先验概率及条件概率</p>
<p style=""><img src="https://math.now.sh?from=P%5Cleft%28Y%3Dc_%7Bk%7D%5Cright%29%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20I%5Cleft(y_%7Bi%7D%3Dc_%7Bk%7D%5Cright)%7D%7BN%7D%2C%20%5Cquad%20k%3D1%2C2%2C%20%5Ccdots%2C%20K%0A" /></p><p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bc%7D%0AP%5Cleft%28X%5E%7B(j%29%7D%3Da_%7Bj%20l%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20I%5Cleft(x_%7Bi%7D%5E%7B(j)%7D%3Da_%7Bj%20l%7D%2C%20y_%7Bi%7D%3Dc_%7Bk%7D%5Cright)%20%7D%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20I%5Cleft(y_%7Bi%7D%3Dc_%7Bk%7D%5Cright)%7D%20%5C%5C%0Aj%3D1%2C2%2C%20%5Ccdots%2C%20n%20%3B%20%5Cquad%20l%3D1%2C2%2C%20%5Ccdots%2C%20S_%7Bj%7D%20%3B%20%5Cquad%20k%3D1%2C2%2C%20%5Ccdots%2C%20K%0A%5Cend%7Barray%7D%0A" /></p><p>（2）对于给定的实例 <img src="https://math.now.sh?inline=x%3D%5Cleft%28x%5E%7B(1%29%7D%2C%20x%5E%7B(2)%7D%2C%20%5Ccdots%2C%20x%5E%7B(n)%7D%5Cright)%5E%7B%5Cmathrm%7BT%7D%7D%2C" style="display:inline-block;margin: 0;"/> 计算</p>
<p style=""><img src="https://math.now.sh?from=P%5Cleft%28Y%3Dc_%7Bk%7D%5Cright%29%20%5Cprod_%7Bj%3D1%7D%5E%7Bn%7D%20P%5Cleft(X%5E%7B(j)%7D%3Dx%5E%7B(j)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%2C%20%5Cquad%20k%3D1%2C2%2C%20%5Ccdots%2C%20K%0A" /></p><p>（3）确定实例 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的类</p>
<p style=""><img src="https://math.now.sh?from=y%3D%5Carg%20%5Cmax%20_%7Bc_%7Bk%7D%7D%20P%5Cleft%28Y%3Dc_%7Bk%7D%5Cright%29%20%5Cprod_%7Bj%3D1%7D%5E%7Bn%7D%20P%5Cleft(X%5E%7B(j)%7D%3Dx%5E%7B(j)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)%0A" /></p></blockquote>
<h4 id="1-4-2-贝叶斯估计">1.4.2 贝叶斯估计</h4>
<p><strong>问题</strong>：通过极大似然估计，可能会出现(8)中的几个<img src="https://math.now.sh?inline=P%5Cleft%28X%5E%7B(j%29%7D%3Dx%5E%7B(j)%7D%20%5Cmid%20Y%3Dc_%7Bk%7D%5Cright)" style="display:inline-block;margin: 0;"/>为0（我们把“发票”当成了一个特征，但句子中没有出现“发票”这一词），这个情况很常见</p>
<p><strong>原因</strong>：训练集太少，没有满足大数定律。</p>
<p><strong>解决</strong>：</p>
<p>给定先验概率，赋值为<img src="https://math.now.sh?inline=%5Clambda%20%5Cge0" style="display:inline-block;margin: 0;"/></p>
<ul>
<li>在一个训练集中，对一个本应该为0 的概率赋予一个较小值，那么就会降低其他词语的概率</li>
<li><img src="https://math.now.sh?inline=%5Clambda%3D0" style="display:inline-block;margin: 0;"/>：极大似然估计</li>
<li><img src="https://math.now.sh?inline=%5Clambda%3D1" style="display:inline-block;margin: 0;"/>：Laplace Smoothing</li>
<li>”加上“基础的概率<img src="https://math.now.sh?inline=1%2FK%2C%5C%3B1%2FS_j" style="display:inline-block;margin: 0;"/></li>
</ul>
<p><strong>算法</strong>：</p>
<ul>
<li>
<p>计算先验概率的等式(10)变成</p>
<p style=""><img src="https://math.now.sh?from=P_%7B%5Clambda%7D%5Cleft%28Y%3Dc_%7Bk%7D%5Cright%29%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20I%5Cleft(y_%7Bi%7D%3Dc_%7Bk%7D%5Cright)%2B%5Clambda%7D%7BN%2BK%20%5Clambda%7D%0A" /></p></li>
<li>
<p>计算条件概率的等式(11)变成</p>
<p style=""><img src="https://math.now.sh?from=P_%7B%5Clambda%7D%28X%5E%7B(j%29%7D%3Da_%7Bjl%7D%7CY%3Dc_k)%3D%5Cfrac%7B%5Csum%5Climits_%7Bi%3D1%7D%5ENI(x_i%5E%7Bj%7D%3Da_%7Bjl%7D%2Cy_j%3Dc_k)%2B%5Clambda%7D%7B%5Csum%5Climits_%7Bi%3D1%7D%5ENI(y_j%3Dc_k)%2BS_j%5Clambda%7D%0A" /></p></li>
</ul>
<h2 id="3-Trick">3. Trick</h2>
<h3 id="3-1-Log-function">3.1 Log function</h3>
<p>在等式(12)中，我们可以取对数，把乘法运算转化为加法运算，提高计算速度。</p>
<h2 id="4-Coding">4. Coding</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayes</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.model=<span class="string">&#x27;Naive Bayes&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, Py, Pxi_y, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param Py: 先验概率</span></span><br><span class="line"><span class="string">        :param Pxi_y: 条件概率</span></span><br><span class="line"><span class="string">        :param x: test array</span></span><br><span class="line"><span class="string">        :return: 通过极大似然法得出最大概率的标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        (labelNumber,featureNumber,features)=Pxi_y.shape</span><br><span class="line">        <span class="comment"># p存放所有的似然</span></span><br><span class="line">        p=Py</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labelNumber):</span><br><span class="line">            <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(featureNumber):</span><br><span class="line">                <span class="built_in">sum</span> += Pxi_y[i][j][x[j]] <span class="comment"># 因为转化成了log，所以累乘变累加</span></span><br><span class="line">            p[i] += <span class="built_in">sum</span></span><br><span class="line">        <span class="keyword">return</span> np.argmax(p)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getAllProbability</span>(<span class="params">self, labelArray, labelNumber, trainArray, featureNumber, features, para=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param labelArray: [int,int,...,int] 1*n</span></span><br><span class="line"><span class="string">        :param labelNumber: 一共有N个标签</span></span><br><span class="line"><span class="string">        :param trainArray: train data, ~*m</span></span><br><span class="line"><span class="string">        :param featureNumber: 一共有m个特征标签</span></span><br><span class="line"><span class="string">        :param features: int k, 对第m个特征标签，有k_m个特征。</span></span><br><span class="line"><span class="string">        :param para: lambda</span></span><br><span class="line"><span class="string">        :return: 先验概率Py, 条件概率Pxi_y</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        目前（没有完善load function）对于输入的形式非常的苛刻，也出现了重复的参数，日后完善。</span></span><br><span class="line"><span class="string">        example:</span></span><br><span class="line"><span class="string">        trainArray=[[0,0],[0,1],[0,1],[0,0],[0,0],[1,0],[1,1],[1,1],[1,2],[1,2],[2,2],[2,1],[2,1],[2,2],[2,2]]</span></span><br><span class="line"><span class="string">        trainArray=np.asarray(trainArray)</span></span><br><span class="line"><span class="string">        featureNumber=2</span></span><br><span class="line"><span class="string">        features=3</span></span><br><span class="line"><span class="string">        labelArray=[0,0,1,1,0,0,0,1,1,1,1,1,1,1,0]</span></span><br><span class="line"><span class="string">        labelArray=np.asarray(labelArray)</span></span><br><span class="line"><span class="string">        labelNumber=2</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 一共有N个标签</span></span><br><span class="line">        Py = np.zeros((labelNumber, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 对标签为y，特征为m，第？个特征取值</span></span><br><span class="line">        Pxi_y = np.zeros((labelNumber, featureNumber, features))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算先验概率Py</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labelNumber):</span><br><span class="line">            numerator = np.<span class="built_in">sum</span>(labelArray==i) + para</span><br><span class="line">            denominator = <span class="built_in">len</span>(labelArray) + para * labelNumber</span><br><span class="line">            Py[i] = numerator/denominator</span><br><span class="line">        Py = np.log(Py) <span class="comment"># 取对数加快运算</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算条件概率Pxi_y</span></span><br><span class="line">        (a,b) = trainArray.shape <span class="comment"># a:几条数据  b:几个特征</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(a): <span class="comment"># 第几条数据，选取他们的label和train data array</span></span><br><span class="line">            label = labelArray[i]</span><br><span class="line">            x=trainArray[i]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(featureNumber): <span class="comment"># 对x的每一个标签上的值，出现一次加一次</span></span><br><span class="line">                Pxi_y[label][j][x[j]] +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> <span class="built_in">range</span>(labelNumber): <span class="comment"># 得到次数后计算概念</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(featureNumber):</span><br><span class="line">                denominator = np.<span class="built_in">sum</span>(Pxi_y[label][i]) + features * para</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(features):</span><br><span class="line">                    numerator = Pxi_y[label][i][j] + para</span><br><span class="line">                    Pxi_y[label][i][j]=np.log(numerator/denominator)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Py, Pxi_y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loadData</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">modelTest</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<h2 id="5-Proof">5. Proof</h2>
<h3 id="5-1-用极大似然法推出等式-10">5.1 用极大似然法推出等式(10)</h3>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-4%20Proof%201.png" alt="253e0884011c7a303fda71c8962cfa6"></p>
<h2 id="6-Reference">6. Reference</h2>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a><a target="_blank" rel="noopener" href="https://github.com/Dod-o/Statistical-Learning-Method_Code">Dod-o/Statistical-Learning-Method_Code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Dod-o/Statistical-Learning-Method_Code">Dod-o/Statistical-Learning-Method_Code</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/han_xiaoyang/article/details/50629608">NLP系列(4)_朴素贝叶斯实战与进阶</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">fengdu78/lihang-code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/SmirkCao/Lihang">SmirkCao, Lihang, (2018), GitHub repository</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/23/[SLM-3]%20K%E8%BF%91%E9%82%BB%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/23/%5BSLM-3%5D%20K%E8%BF%91%E9%82%BB%E6%B3%95/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-23 20:51:59" itemprop="dateCreated datePublished" datetime="2020-11-23T20:51:59+08:00">2020-11-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-11 21:43:36" itemprop="dateModified" datetime="2020-12-11T21:43:36+08:00">2020-12-11</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1>k 近邻法</h1>
<p>k-nearest neighbor (KNN)，一种基本的分类与回归方法。这里只介绍分类问题。<strong>k值的选择、距离度量、分类决策规则</strong>是KNN的3个基本要素。</p>
<h2 id="1-Algorithm">1. Algorithm</h2>
<blockquote>
<p><strong>算法 <img src="https://math.now.sh?inline=3.1%28k" style="display:inline-block;margin: 0;"/> 近邻法 <img src="https://math.now.sh?inline=%29" style="display:inline-block;margin: 0;"/></strong><br>
输入: 训练数据集</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0AT%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A" /></p><p>其中， <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%20%5Csubseteq%20%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/> 为实例的特征向量, <img src="https://math.now.sh?inline=y_%7Bi%7D%20%5Cin%20%5Cmathcal%7BY%7D%3D%5Cleft%5C%7Bc_%7B1%7D%2C%20c_%7B2%7D%2C%20%5Ccdots%2C%20c_%7BK%7D%5Cright%5C%7D" style="display:inline-block;margin: 0;"/> 为实例的类别， <img src="https://math.now.sh?inline=i%3D1%2C2%2C%20%5Ccdots%2C%20N%20%3B" style="display:inline-block;margin: 0;"/> 实例特征向量 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/>;<br>
输出：实例 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 所属的类 <img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/>。<br>
(1) 根据给定的距离度量，在训练集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 中找出与 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 最邻近的 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 个点，涵盖这 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 个点的 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的邻域记作 <img src="https://math.now.sh?inline=N_%7Bk%7D%28x%29" style="display:inline-block;margin: 0;"/>;<br>
(2) 在 <img src="https://math.now.sh?inline=N_%7Bk%7D%28x%29" style="display:inline-block;margin: 0;"/> 中根据分类决策规则 (如多数表决) 决定 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的类别 <img src="https://math.now.sh?inline=y" style="display:inline-block;margin: 0;"/> :</p>
<p style=""><img src="https://math.now.sh?from=y%3D%5Carg%20%5Cmax%20_%7Bc_%7Bj%7D%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20N_%7Bk%7D%28x%29%7D%20I%5Cleft(y_%7Bi%7D%3Dc_%7Bj%7D%5Cright)%2C%20%5Cquad%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%20%3B%20j%3D1%2C2%2C%20%5Ccdots%2C%20K%0A" /></p><p>式 (1) 中， <img src="https://math.now.sh?inline=I" style="display:inline-block;margin: 0;"/> 为指示函数，即当 <img src="https://math.now.sh?inline=y_%7Bi%7D%3Dc_%7Bj%7D" style="display:inline-block;margin: 0;"/> 时 <img src="https://math.now.sh?inline=I" style="display:inline-block;margin: 0;"/> 为 <img src="https://math.now.sh?inline=1%2C" style="display:inline-block;margin: 0;"/> 否则 <img src="https://math.now.sh?inline=I" style="display:inline-block;margin: 0;"/> 为 0 。</p>
</blockquote>
<ul>
<li>算法没有显式的学习过程。</li>
<li>例子：
<ol>
<li>S市划分成住宅区、商业区、工业区（K=3），每个区域都有各自地标建筑，共N座。你的坐标为x，选取离你最近的k个地标建筑，k中包括哪类地标建筑最多，你就位于哪个区。</li>
</ol>
</li>
</ul>
<h2 id="2-Model">2. Model</h2>
<ul>
<li>
<p>KNN的模型对应于对特征空间的划分。</p>
</li>
<li>
<p>在训练集、距离度量、k值、分类决策鬼册确定后，对任意新输入实例，所属类别唯一。</p>
</li>
<li>
<p>每一个训练实例点<img src="https://math.now.sh?inline=x_i" style="display:inline-block;margin: 0;"/>，存在一个单元cell，在单元内到此<img src="https://math.now.sh?inline=x_i" style="display:inline-block;margin: 0;"/>的距离最小。所有实例点的单元划分了特征空间。</p>
</li>
</ul>
<h3 id="2-1-距离度量">2.1 距离度量</h3>
<p>计算特征空间中两点的距离。</p>
<p><img src="https://math.now.sh?inline=L_%7Bp%7D" style="display:inline-block;margin: 0;"/> (Minkowski) distance</p>
<p style=""><img src="https://math.now.sh?from=L_%7Bp%7D%5Cleft%28x_%7Bi%7D%2C%20x_%7Bj%7D%5Cright%29%3D%5Cleft(%5Csum_%7Bl%3D1%7D%5E%7Bn%7D%5Cleft%7Cx_%7Bi%7D%5E%7B(l)%7D-x_%7Bj%7D%5E%7B(l)%7D%5Cright%7C%5E%7Bp%7D%5Cright)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%0A" /></p><ul>
<li>
<ul>
<li>设特征空间 <img src="https://math.now.sh?inline=%5Cmathcal%7BX%7D" style="display:inline-block;margin: 0;"/> 是 <img src="https://math.now.sh?inline=n" style="display:inline-block;margin: 0;"/> 维实数向量空间 <img src="https://math.now.sh?inline=x_%7Bi%7D%2C%20x_%7Bj%7D%20%5Cin%20%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20x_%7Bi%7D%3D%5Cleft%28x_%7Bi%7D%5E%7B(1%29%7D%2C%20x_%7Bi%7D%5E%7B(2)%7D%2C%20%5Ccdots%2C%20x_%7Bi%7D%5E%7B(n)%7D%5Cright)%5E%7B%5Cmathrm%7BT%7D%7D%2C" style="display:inline-block;margin: 0;"/><br>
<img src="https://math.now.sh?inline=x_%7Bj%7D%3D%5Cleft%28x_%7Bj%7D%5E%7B(1%29%7D%2C%20x_%7Bj%7D%5E%7B(2)%7D%2C%20%5Ccdots%2C%20x_%7Bj%7D%5E%7B(n)%7D%5Cright)%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/></li>
<li><img src="https://math.now.sh?inline=p%5Cgeq1" style="display:inline-block;margin: 0;"/></li>
</ul>
</li>
<li><img src="https://math.now.sh?inline=p%3D1" style="display:inline-block;margin: 0;"/>，Manhattan distance</li>
<li><img src="https://math.now.sh?inline=p%3D2" style="display:inline-block;margin: 0;"/>， Euclidean distance</li>
<li><img src="https://math.now.sh?inline=p%3D%5Cinfin" style="display:inline-block;margin: 0;"/>，‘Max’ distance</li>
</ul>
<h3 id="2-2-k值选择">2.2 k值选择</h3>
<p>选取一些较小k值，通过<strong>交叉验证法</strong>选取最优k值。</p>
<ul>
<li>小k：
<ul>
<li>近似误差减小，估计误差增大。对近邻实例点非常敏感</li>
<li>模型更复杂，容易过拟合。</li>
</ul>
</li>
<li>大k：
<ul>
<li>近似误差增大，估计误差减小。较远错误标签实例点也能造成影响。</li>
<li>模型更简单。</li>
<li><img src="https://math.now.sh?inline=k%3DN" style="display:inline-block;margin: 0;"/>时，则为测试集最多类。</li>
</ul>
</li>
</ul>
<h3 id="2-3-分类决策规则">2.3 分类决策规则</h3>
<p>一般为<strong>多数表决(majority voting rule)</strong></p>
<p>对给定的实例<img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/>，最近邻的<img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/>个训练实例点存在<img src="https://math.now.sh?inline=c" style="display:inline-block;margin: 0;"/>个类别。为了使得分类错误的概率最小，所以选取类别中最多的那一类。</p>
<h2 id="3-Implement-kd-tree">3. Implement: kd tree</h2>
<p>线性扫描 (linear scan) 要计算实例与每一个训练实例的距离，耗时。</p>
<h3 id="3-1-kd树的构造">3.1 kd树的构造</h3>
<p>选取一个训练实例点，构造一个垂直于某一轴的超平面，使得特征空间被分成左右两个子空间（左小右大）。在2个特征子空间中重复，直到子空间中没有训练实例点。</p>
<h4 id="3-1-1平衡kd树">3.1.1平衡kd树</h4>
<ul>
<li>训练实例点的选取为选定坐标轴上的中位数。</li>
<li>效率不一定最优。</li>
</ul>
<blockquote>
<p>算法 3.2 (构造平衡 <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树）</p>
<p>输入: <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 维空间数据集 <img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7Bx_%7B1%7D%2C%20x_%7B2%7D%2C%20%5Ccdots%2C%20x_%7BN%7D%5Cright%5C%7D%2C" style="display:inline-block;margin: 0;"/> 其中 <img src="https://math.now.sh?inline=x_%7Bi%7D%3D%5Cleft%28x_%7Bi%7D%5E%7B(1%29%7D%2C%20x_%7Bi%7D%5E%7B(2)%7D%2C%20%5Ccdots%2C%20x_%7Bi%7D%5E%7B(k)%7D%5Cright)%5E%7B%5Cmathrm%7BT%7D%7D%2C" style="display:inline-block;margin: 0;"/><br>
<img src="https://math.now.sh?inline=i%3D1%2C2%2C%20%5Ccdots%2C%20N" style="display:inline-block;margin: 0;"/><br>
输出: <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树。<br>
（1）开始：构造根结点，根结点对应于包含 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 的 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 维空间的超矩形区域。<br>
选择 <img src="https://math.now.sh?inline=x%5E%7B%281%29%7D" style="display:inline-block;margin: 0;"/> 为坐标轴，以 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 中所有实例的 <img src="https://math.now.sh?inline=x%5E%7B%281%29%7D" style="display:inline-block;margin: 0;"/> 坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 <img src="https://math.now.sh?inline=x%5E%7B%281%29%7D" style="display:inline-block;margin: 0;"/> 垂直的超平面实现。<br>
由根结点生成深度为 1 的左、右子结点: <strong>左子结点对应坐标 <img src="https://math.now.sh?inline=x%5E%7B%281%29%7D" style="display:inline-block;margin: 0;"/> 小于切分点的子 区域，右子结点对应于坐标 <img src="https://math.now.sh?inline=x%5E%7B%281%29%7D" style="display:inline-block;margin: 0;"/> 大于切分点的子区域</strong>。<br>
将落在切分超平面上的实例点保存在根结点。<br>
（2）重复: 对深度为 <img src="https://math.now.sh?inline=j" style="display:inline-block;margin: 0;"/> 的结点，选择 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D" style="display:inline-block;margin: 0;"/> 为切分的坐标轴， <img src="https://math.now.sh?inline=l%3Dj%28%5Cbmod%20k%29%2B1%2C" style="display:inline-block;margin: 0;"/> 以该结点的区域中所有实例的 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D" style="display:inline-block;margin: 0;"/> 坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D" style="display:inline-block;margin: 0;"/> 垂直的超平面实现。<br>
由该结点生成深度为 <img src="https://math.now.sh?inline=j%2B1" style="display:inline-block;margin: 0;"/> 的左、右子结点: 左子结点对应坐标 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D" style="display:inline-block;margin: 0;"/> 小于切分点 的子区域，右子结点对应坐标 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D" style="display:inline-block;margin: 0;"/> 大于切分点的子区域。<br>
将落在切分超平面上的实例点保存在该结点。</p>
<p>（3）直到两个子区域没有实例存在时停止。从而形成 <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树的区域划分.</p>
</blockquote>
<p><strong>构造过程图示</strong>：</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-3%20%E5%B9%B3%E8%A1%A1kd%E6%A0%91%E5%9B%BE%E7%A4%BA.png" alt="SLM-3 平衡kd树图示"></p>
<ul>
<li>按照黄、绿、蓝、红，垂直x，y，x，y的顺序切分。</li>
<li>左右节点确定：根据在第<img src="https://math.now.sh?inline=l" style="display:inline-block;margin: 0;"/>维度上2个子节点对应坐标大小确定，小左大右。</li>
<li><img src="https://math.now.sh?inline=l%3Dj%28%5Cbmod%20k%29%2B1" style="display:inline-block;margin: 0;"/>：在k维方向都按顺序切上之后，回过头在循环切。
<ul>
<li>公式细节因初始根节点的深度是0或1而有不同</li>
</ul>
</li>
<li>停止条件：所有实例点都位于一个超平面上。</li>
<li>偶数个数的中位数：自己确定，代码上别忘了。</li>
</ul>
<h3 id="3-2-kd树的搜索">3.2 kd树的搜索</h3>
<h4 id="3-2-1-k-1">3.2.1 <img src="https://math.now.sh?inline=k%3D1" style="display:inline-block;margin: 0;"/></h4>
<blockquote>
<p>算法 3.3 (用 <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树的最近邻搜索)<br>
输入: 已构造的 <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树，目标点 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/>;<br>
输出: <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的最近邻，<img src="https://math.now.sh?inline=k%3D1" style="display:inline-block;margin: 0;"/>。<br>
(1) 在 <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树中找出包含目标点 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的叶结点: 从根结点出发，递归地向下访问 <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树。若目标点 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。<br>
(2) 以此叶结点为“当前最近点”。<br>
(3) 递归地向上回退，在每个结点进行以下操作:<br>
(a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点 为“当前最近点”。<br>
(b）当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”问的距离为半径的超球体相交。</p>
<p>​		如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索;<br>
​		如果不相交，向上回退。<br>
(4）当回退到根结点时，搜索结束。最后的“当前最近点&quot;即为 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的最近邻点。</p>
</blockquote>
<h4 id="3-2-2-k-1">3.2.2 <img src="https://math.now.sh?inline=k%3E1" style="display:inline-block;margin: 0;"/></h4>
<blockquote>
<p>算法 3.04 (用 <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树的最近邻搜索)<br>
输入: 已构造的 <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树，目标点 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/>;<br>
输出: <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的k最近邻。</p>
<ol>
<li>
<p>根据p的坐标和kd树的结点向下进行搜索 (如果树的结点是以 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D%3Dc" style="display:inline-block;margin: 0;"/> 来切分的, 那么如果p的 <img src="https://math.now.sh?inline=x%5E%7B%28l%29%7D" style="display:inline-block;margin: 0;"/> 坐标小于c, 则走左子结点, 否则走右子结点)</p>
</li>
<li>
<p>到达叶子结点时，将其标记为已访问。如果S中不足k个点, 则将该结点加入到S中; 如果S不空且当前结点与p点的距离小于S中最长的距离，则用当前结点替换S中离p最远的点</p>
</li>
<li>
<p>如果当前结点不是根节点, 执行（a）; 否则，结束算法</p>
</li>
</ol>
<p>(a). 回退到当前结点的父结点, 此时的结点为当前结点 (回退之后的结点) ，将当前结点标 记为已访问, 执行 (b) 和（c) ; 如果当前结点已经被访过, 再次执行（a）。</p>
<p>(b). 如果此时S中不足k个点, 则将当前结点加入到S中; 如果S中已有k个点, 且当前结点与p 点的距离小于S中最长距离，则用当前结点替换S中距离最远的点。</p>
<p>©. 计算p点和当前结点切分线的距离。如果该距离大于等于S中距离p最远的距离并且S中已 有k个点, 执行3; 如果该距离小于S中最远的距离或S中没有k个点, 从当前结点的另一子节点开始执行1; 如果当前结点没有另一子结点, 执行3。</p>
</blockquote>
<p><strong>算法3.04 (用 <img src="https://math.now.sh?inline=k%20d" style="display:inline-block;margin: 0;"/> 树的最近邻搜索)搜索过程</strong>：目标点P（-1，-5），k=3，S：存储k个近邻点。初始所有点=[0, 0] ，即 [未访问, 不在S中]</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-3%20%E5%B9%B3%E8%A1%A1kd%E6%A0%91%E5%9B%BE%E7%A4%BA.png" alt="SLM-3 平衡kd树图示"></p>
<blockquote>
<ol>
<li>执行算法1：
<ul>
<li>(-1,-5)的-1与A点(6,5)的6小，往左到B</li>
<li>-5&lt;-3，往左到D</li>
<li>D子节点唯一，到H</li>
</ul>
</li>
<li>执行算法2：
<ul>
<li>H=[1, 0]，H=[1, 1]</li>
</ul>
</li>
<li>执行算法3：
<ul>
<li>执行(a)，到D。D=[1, 0]</li>
<li>执行(b)，D=[1, 1]</li>
<li>执行©，P到D的切分线<img src="https://math.now.sh?inline=x%5E1%3D-6" style="display:inline-block;margin: 0;"/>距离为5。S未满。D没有另一子节点。</li>
</ul>
</li>
<li>执行算法3：
<ul>
<li>执行(a)，到B。B=[1, 0]</li>
<li>执行(b)，B=[1,1]，S={H, D, B}</li>
<li>执行©，P到B的切分线<img src="https://math.now.sh?inline=x%5E2%3D-3" style="display:inline-block;margin: 0;"/>距离为2，小于S的最大距离。从B另一子节点E执行算法1.</li>
</ul>
</li>
<li>执行算法1：
<ul>
<li>-1&gt;-2，往右到J</li>
</ul>
</li>
<li>执行算法2：
<ul>
<li>J=[1,0]，d(J, P)大于S的最大距离。不加入</li>
</ul>
</li>
<li>执行算法3：
<ul>
<li>执行(a)，到E，E=[1, 0]</li>
<li>执行(b)，d(E,P)小于S的最大距离。E=[1, 1]，H=[1, 0]，S={E, D, B}</li>
<li>执行©，P到E的切分线<img src="https://math.now.sh?inline=x%5E1%3D-2" style="display:inline-block;margin: 0;"/>距离为1，小于S的最大距离。从E另一子节点I执行算法1.</li>
</ul>
</li>
<li>执行算法1：
<ul>
<li>I是子节点，到I</li>
</ul>
</li>
<li>执行算法2：
<ul>
<li>I=[1, 0]，d(I, P)大于S的最大距离。不加入。</li>
</ul>
</li>
<li>执行算法3：
<ul>
<li>执行若干(a)，到A，A=[1, 0]</li>
<li>执行(b)，d(A, P)大于S的最大距离。</li>
<li>执行©，P到E的切分线<img src="https://math.now.sh?inline=x%5E1%3D6" style="display:inline-block;margin: 0;"/>距离为7，大于最长距离。不加入。</li>
</ul>
</li>
<li>执行算法3：
<ul>
<li>A是根节点。算法结束。已按顺序访问{H, D, B, J, E, I, A}，最终结果S={E, D, B}</li>
</ul>
</li>
</ol>
</blockquote>
<h3 id="3-3-Summary">3.3 Summary</h3>
<ol>
<li>
<p>kd Tree的平均复杂度<img src="https://math.now.sh?inline=O%28logN%29" style="display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"/>为训练实例数。适合训练实例数远大于空间维数的KNN。</p>
</li>
<li>
<p>把k维大空间多次对半分成k维小空间。</p>
</li>
<li>
<p>目标点与实例点的距离<img src="https://math.now.sh?inline=%5Cgeq" style="display:inline-block;margin: 0;"/>目标点与该实例点对应切分超平面的距离。因此：</p>
<ol>
<li>当此实例点当前应该替换S中某点从而加入到S集合中时，另一半空间存在子空间，子空间的点到目标点的距离小于实例点到目标点的距离。
<ol>
<li>此时如果存在实例点位于子空间，那么应该继续循环（例子中没有体现）。</li>
<li>没有实例点在此子空间，那么另一半的空间的实例点都不会加入到集合S中。</li>
</ol>
</li>
<li>这个实例点不应该加入到S中，情况如2.1.2</li>
</ol>
</li>
<li>
<p>动态规划DP的思想？</p>
</li>
<li>
<p>别人笔记的原话：</p>
<p>1、找到叶子结点，看能不能加入到S中</p>
<p>2、回退到父结点，看父结点能不能加入到S中</p>
<p>3、看目标点和回退到的父结点切分线的距离，判断另一子结点能不能加入到S中</p>
</li>
</ol>
<h2 id="4-Question">4. Question</h2>
<ol>
<li>kd方差的代码！
<ul>
<li>有点点难搞-_-</li>
</ul>
</li>
<li>非平衡/方差kd树
<ul>
<li>同上，难搞</li>
</ul>
</li>
</ol>
<h2 id="5-Code">5. Code</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, train_set, label_set</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray, m:dims, n:train points</span></span><br><span class="line"><span class="string">        :param label_set: 1*n int ndarray and from 0 to n-1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.train_set = train_set</span><br><span class="line">        self.label_set = label_set</span><br><span class="line">        self.m, self.n = self.train_set.shape</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">knn_naive</span>(<span class="params">self, test_set, k=<span class="number">1</span>,p=<span class="number">2</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param test_set: m*n ndarray, m:dims, n:test points</span></span><br><span class="line"><span class="string">        :param k: k nearest neighbor</span></span><br><span class="line"><span class="string">        :param p: order of norm</span></span><br><span class="line"><span class="string">        :return: 1*n list for n points</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        (m,n)=test_set.shape</span><br><span class="line">        <span class="comment"># 有几个点，输出对于shape的list</span></span><br><span class="line">        final=[-<span class="number">1</span>]*n</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="comment"># 转置后取点再转置</span></span><br><span class="line">            test_point=test_set.T[i].T</span><br><span class="line">            <span class="comment"># 有些可能是(m,)的ndarray</span></span><br><span class="line">            test_point=test_point.reshape([m,<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 计算距离</span></span><br><span class="line">            distance=np.linalg.norm(self.train_set-test_point,<span class="built_in">ord</span>=p,axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 最近k个点的index</span></span><br><span class="line">            nearestK=np.argsort(distance)[:k]</span><br><span class="line">            <span class="comment"># label是0，1，2，...的顺序，存储对应label出现的总次数</span></span><br><span class="line">            labelList=[<span class="number">0</span>]*(<span class="built_in">max</span>(self.label_set) + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> nearestK:</span><br><span class="line">                labelList[<span class="built_in">int</span>(self.label_set[index])] +=<span class="number">1</span></span><br><span class="line">                point_label=labelList.index(<span class="built_in">max</span>(labelList))</span><br><span class="line">            final[i]=point_label</span><br><span class="line">        <span class="keyword">return</span> final</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">knn_kdtree</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span>(<span class="params">self, points=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        画不同k从而分割成不同子%tb域的图</span></span><br><span class="line"><span class="string">        画结果图</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> self.m != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;Unable to draw a picture&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
<li>Github求start！！！^_^</li>
</ul>
<h2 id="6-Reference">6. Reference</h2>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zzpzm/article/details/88565645">KNN算法和kd树详解（例子+图示）</a></p>
<p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict">sklearn</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">fengdu78/lihang-code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Dod-o/Statistical-Learning-Method_Code">Dod-o/Statistical-Learning-Method_Code</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/23/[SLM-1]%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/23/%5BSLM-1%5D%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-23 16:58:58" itemprop="dateCreated datePublished" datetime="2020-11-23T16:58:58+08:00">2020-11-23</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/22/[SLM-2]%20%E6%84%9F%E7%9F%A5%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CHongfeng Ling 凌崇锋">
      <meta itemprop="description" content="description">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chongfeng Ling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/22/%5BSLM-2%5D%20%E6%84%9F%E7%9F%A5%E6%9C%BA/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-22 10:49:46" itemprop="dateCreated datePublished" datetime="2020-11-22T10:49:46+08:00">2020-11-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-20 00:04:16" itemprop="dateModified" datetime="2020-12-20T00:04:16+08:00">2020-12-20</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1>感知机</h1>
<p>感知机（perceptron）是<strong>二类线性分类</strong>模型。</p>
<h2 id="1-Model">1. Model</h2>
<blockquote>
<p>定义 2.1 (感知机) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/>假设输入空间（特征空间 ) 是 <img src="https://math.now.sh?inline=%5Cmathcal%7BX%7D%20%5Csubseteq%20%5Cmathbf%7BR%7D%5E%7Bn%7D%2C" style="display:inline-block;margin: 0;"/> 输出空间是 <img src="https://math.now.sh?inline=%5Cmathcal%7BY%7D%3D%5C%7B%2B1%2C-1%5C%7D_%7B%5Ccirc%7D" style="display:inline-block;margin: 0;"/> 输入 <img src="https://math.now.sh?inline=x%20%5Cin%20%5Cmathcal%7BX%7D" style="display:inline-block;margin: 0;"/> 表示实例的特征向量，对应于输入空间 ( 特征空间 <img src="https://math.now.sh?inline=%29" style="display:inline-block;margin: 0;"/> 的点; 输出 <img src="https://math.now.sh?inline=y%20%5Cin%20%5Cmathcal%7BY%7D" style="display:inline-block;margin: 0;"/> 表示实例的类别。由输入空间到输出空间的如下函数:</p>
<p style=""><img src="https://math.now.sh?from=f%28x%29%3D%5Coperatorname%7Bsign%7D(w%20%5Ccdot%20x%2Bb)%0A" /></p><p>称为感知机。其中， <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"/> 和 <img src="https://math.now.sh?inline=b" style="display:inline-block;margin: 0;"/> 为感知机模型参数, <img src="https://math.now.sh?inline=w%20%5Cin%20%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/> 叫作权值（weight ) 或权值向量（weight vector) <img src="https://math.now.sh?inline=%2C%20b%20%5Cin%20%5Cmathbf%7BR%7D" style="display:inline-block;margin: 0;"/> 叫作偏置 ( bias <img src="https://math.now.sh?inline=%29%2C%20w%20%5Ccdot%20x" style="display:inline-block;margin: 0;"/> 表示 <img src="https://math.now.sh?inline=w" style="display:inline-block;margin: 0;"/> 和 <img src="https://math.now.sh?inline=x" style="display:inline-block;margin: 0;"/> 的内积。sign 是符号 函数，即</p>
<p style=""><img src="https://math.now.sh?from=%5Coperatorname%7Bsign%7D%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%0A%2B1%2C%20%26%20x%20%5Cgeqslant%200%20%5C%5C%0A-1%2C%20%26%20x%3C0%0A%5Cend%7Barray%7D%5Cright.%0A" /></p></blockquote>
<ul>
<li>
<p>假设空间：特征空间中的所有线性分类模型，即函数集合<img src="https://math.now.sh?inline=%5C%7Bf%7Cf%28x%29%3Dw%20%5Ccdot%20x%2Bb%5C%7D" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>几何解释：线性方程<img src="https://math.now.sh?inline=w%20%5Ccdot%20x%2Bb%3D0" style="display:inline-block;margin: 0;"/>是特征空间<img src="https://math.now.sh?inline=%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/>的一个超平面<img src="https://math.now.sh?inline=%5Cmathbf%7BS%7D" style="display:inline-block;margin: 0;"/>，把特征空间分成2个部分，使得特征向量分别划入正负两类。<img src="https://math.now.sh?inline=%5Cmathbf%7BS%7D" style="display:inline-block;margin: 0;"/>也叫分离超平面（separating hyperplane)</p>
<ul>
<li>超平面<img src="https://math.now.sh?inline=%5Cmathbf%7BS%7D%5Csubseteq%20%5Cmathbf%7BR%7D%5E%7Bn-1%7D" style="display:inline-block;margin: 0;"/>并且截距为0。在这需要把b当成特征而不是截距才说的通。</li>
</ul>
</li>
<li>
<p>例子：</p>
<ul>
<li>通过<img src="https://math.now.sh?inline=%5C%7B%E6%88%BF%E5%B1%8B%E9%9D%A2%E7%A7%AF%EF%BC%8C%E6%88%BF%E9%BE%84%EF%BC%8C...%5C%7D" style="display:inline-block;margin: 0;"/>来判断房子总价是否高于价格<img src="https://math.now.sh?inline=a" style="display:inline-block;margin: 0;"/>
<ul>
<li>假设数据集线性可分</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-Strategy">2. Strategy</h2>
<h3 id="2-1-数据可分性">2.1 数据可分性</h3>
<blockquote>
<p>定义 2.2 (数据集的线性可分性) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/> 给定一个数据集</p>
<p style=""><img src="https://math.now.sh?from=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%0A" /></p><p>其中， <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20y_%7Bi%7D%20%5Cin%20%5Cmathcal%7BY%7D%3D%5C%7B%2B1%2C-1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%2C" style="display:inline-block;margin: 0;"/> 如果存在某个超平面 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=w%20%5Ccdot%20x%2Bb%3D0%0A" /></p><p>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有<br>
<img src="https://math.now.sh?inline=y_%7Bi%7D%3D%2B1" style="display:inline-block;margin: 0;"/> 的实例 <img src="https://math.now.sh?inline=i%2C" style="display:inline-block;margin: 0;"/> 有 <img src="https://math.now.sh?inline=w%20%5Ccdot%20x_%7Bi%7D%2Bb%3E0%2C" style="display:inline-block;margin: 0;"/> 对所有 <img src="https://math.now.sh?inline=y_%7Bi%7D%3D-1" style="display:inline-block;margin: 0;"/> 的实例 <img src="https://math.now.sh?inline=i%2C" style="display:inline-block;margin: 0;"/> 有 <img src="https://math.now.sh?inline=w%20%5Ccdot%20x_%7Bi%7D%2Bb%3C0%2C" style="display:inline-block;margin: 0;"/> 则 称数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 为线性可分数据集（linearly separable data set ) ; 否则，称数据集 <img src="https://math.now.sh?inline=T" style="display:inline-block;margin: 0;"/> 线性<br>
不可分。</p>
</blockquote>
<ul>
<li>存在超平面$\mathbf{S}\Longrightarrow $数据集线性可分</li>
</ul>
<h3 id="2-2-学习策略">2.2 学习策略</h3>
<h4 id="2-2-1-误分类点总数">2.2.1 误分类点总数</h4>
<p>把误分类点总数当成损失函数，不是参数<img src="https://math.now.sh?inline=w%2Cb" style="display:inline-block;margin: 0;"/>的连续可导函数，不易优化</p>
<h4 id="2-2-2-误分类点到超平面的总距离">2.2.2 误分类点到超平面的总距离</h4>
<blockquote>
<p>定理（2.01) <img src="https://math.now.sh?inline=%5Cquad" style="display:inline-block;margin: 0;"/>空间  <img src="https://math.now.sh?inline=%5Cmathbf%7BR%7D%5En" style="display:inline-block;margin: 0;"/>中任意一点<img src="https://math.now.sh?inline=x_0" style="display:inline-block;margin: 0;"/>到超平面  <img src="https://math.now.sh?inline=%5Cmathbf%7BS%7D%3D%5C%7Bx%7Cw%5Ccdot%20x%2Bb%3D0%5C%7D" style="display:inline-block;margin: 0;"/> 的距离为</p>
<p style=""><img src="https://math.now.sh?from=%5Cfrac%7B1%7D%7B%5C%7Cw%5C%7C%7D%5Cleft%7Cw%20%5Ccdot%20x_%7B0%7D%2Bb%5Cright%7C%0A" /></p></blockquote>
<p>对于误分类数据<img src="https://math.now.sh?inline=%28x_i%2Cy_i%29" style="display:inline-block;margin: 0;"/>，M为误分类点集合，误分类点到超平面的总距离为</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Balign%7D%0AD%26%3D%5Cfrac%7B1%7D%7B%5C%7Cw%5C%7C%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20M%7D%20%7Cy_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%7C%5Cnonumber%5C%5C%0A%26%3D-%5Cfrac%7B1%7D%7B%5C%7Cw%5C%7C%7D%20%5Csum_%7Bx_%7Bi%7D%20%5Cin%20M%7D%20y_%7Bi%7D%5Cleft(w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright)%5C%5C%0AL(w%2Cb)%26%3D-%5Csum_%7Bx_%7Bi%7D%20%5Cin%20M%7D%20y_%7Bi%7D%5Cleft(w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright)%0A%5Cend%7Balign%7D%0A" /></p><ul>
<li>对误分类点<img src="https://math.now.sh?inline=%28x_i%2Cy_i%29" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=-y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%20%5Cgeq0" style="display:inline-block;margin: 0;"/></li>
<li>(6)是给定数据集<img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D" style="display:inline-block;margin: 0;"/>，其中<img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20y_%7Bi%7D%20%5Cin%20%5Cmathcal%7BY%7D%3D%5C%7B%2B1%2C-1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N" style="display:inline-block;margin: 0;"/></li>
<li>(7)非负函数，且对于<img src="https://math.now.sh?inline=w%2Cb" style="display:inline-block;margin: 0;"/>连续可导</li>
<li>我们的策略是<strong>极小化损失函数</strong>，即<img src="https://math.now.sh?inline=%5Cmin%20_%7Bw%2C%20b%7D%20L%28w%2C%20b%29%3D-%5Csum_%7Bx_%7Bi%7D%20%5Cin%20M%7D%20y_%7Bi%7D%5Cleft(w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright)" style="display:inline-block;margin: 0;"/></li>
</ul>
<h2 id="3-Algorithm">3. Algorithm</h2>
<p>求解损失函数(7)的最优化问题，通过随机梯度下降(stochastic gradient descent)</p>
<h3 id="3-1-算法原始形式">3.1 算法原始形式</h3>
<blockquote>
<p>算法 2.1（感知机学习算法的原始形式）<br>
输入: 训练数据集 <img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%2C" style="display:inline-block;margin: 0;"/> 其中 <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20y_%7Bi%7D%20%5Cin" style="display:inline-block;margin: 0;"/><img src="https://math.now.sh?inline=%5Cmathcal%7BY%7D%3D%5C%7B-1%2C%2B1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%20%3B" style="display:inline-block;margin: 0;"/> 学习率 <img src="https://math.now.sh?inline=%5Ceta%280%3C%5Ceta%20%5Cleqslant%201%29" style="display:inline-block;margin: 0;"/>;<br>
输出 <img src="https://math.now.sh?inline=%3A%20w%2C%20b%20%3B" style="display:inline-block;margin: 0;"/> 感知机模型 <img src="https://math.now.sh?inline=f%28x%29%3D%5Coperatorname%7Bsign%7D(w%20%5Ccdot%20x%2Bb)" style="display:inline-block;margin: 0;"/> 。<br>
(1) 选取初值 <img src="https://math.now.sh?inline=w_%7B0%7D%2C%20b_%7B0%7D" style="display:inline-block;margin: 0;"/>;<br>
(2)在训练集中随机选取数据<img src="https://math.now.sh?inline=%5Cleft%28x_%7Bi%7D%2C%20y_%7Bi%7D%5Cright%29" style="display:inline-block;margin: 0;"/>;<br>
(3) 如果 <img src="https://math.now.sh?inline=y_%7Bi%7D%5Cleft%28w%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%20%5Cleqslant%200" style="display:inline-block;margin: 0;"/>,</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Balign%7D%0Aw%26%20%5Cleftarrow%20w%2B%5Ceta%20y_%7Bi%7D%20x_%7Bi%7D%20%5C%5C%0Ab%20%26%5Cleftarrow%20b%2B%5Ceta%20y_%7Bi%7D%0A%5Cend%7Balign%7D%0A" /></p><p>(4）转至 (2)，直至训练集中没有误分类点。</p>
</blockquote>
<ul>
<li>
<p>一次随机选取一个误分类点使其梯度下降</p>
</li>
<li>
<p><strong>随机</strong>选取一个误分类点 <img src="https://math.now.sh?inline=%5Cleft%28x_%7Bi%7D%2C%20y_%7Bi%7D%5Cright%29%2C" style="display:inline-block;margin: 0;"/> 对 <img src="https://math.now.sh?inline=w%2C%20b" style="display:inline-block;margin: 0;"/> 进行更新 :</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0Aw%20%26%5Cleftarrow%20w%2B%5Ceta%20y_%7Bi%7D%20x_%7Bi%7D%20%5C%5C%0Ab%20%26%5Cleftarrow%20b%2B%5Ceta%20y_%7Bi%7D%0A%5Cend%7Baligned%7D%0A" /></p></li>
<li>
<p>不同初值，不同分类点选取会影响最后的解</p>
</li>
</ul>
<h3 id="3-2-原始算法的收敛性">3.2 原始算法的收敛性</h3>
<p>证明对一个线性可分的数据集，感知机学习算法的原始形式在有限次迭代后能得到正确的分离超平面与感知机模型</p>
<blockquote>
<p><strong>定理 2.1 (Novikoff)</strong> 设训练数据集 <img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D" style="display:inline-block;margin: 0;"/> 是线 性可分的，其中 <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathcal%7BX%7D%3D%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20y_%7Bi%7D%20%5Cin%20%5Cmathcal%7BY%7D%3D%5C%7B-1%2C%2B1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%2C" style="display:inline-block;margin: 0;"/> 则<br>
(1) 存在满足条件 <img src="https://math.now.sh?inline=%5Cleft%5C%7C%5Chat%7Bw%7D_%7B%5Cmathrm%7Bopt%7D%7D%5Cright%5C%7C%3D1" style="display:inline-block;margin: 0;"/> 的超平面 <img src="https://math.now.sh?inline=%5Chat%7Bw%7D_%7B%5Cmathrm%7Bopt%7D%7D%20%5Ccdot%20%5Chat%7Bx%7D%3Dw_%7B%5Cmathrm%7Bopt%7D%7D%20%5Ccdot%20x%2Bb_%7B%5Cmathrm%7Bopt%7D%7D%3D0" style="display:inline-block;margin: 0;"/> 将训练数据集完全正确分开; 且存在 <img src="https://math.now.sh?inline=%5Cgamma%3E0%2C" style="display:inline-block;margin: 0;"/> 对所有 <img src="https://math.now.sh?inline=i%3D1%2C2%2C%20%5Ccdots%2C%20N" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=y_%7Bi%7D%5Cleft%28%5Chat%7Bw%7D_%7B%5Cmathrm%7Bopt%7D%7D%20%5Ccdot%20%5Chat%7Bx%7D_%7Bi%7D%5Cright%29%3Dy_%7Bi%7D%5Cleft(w_%7B%5Cmathrm%7Bopt%7D%7D%20%5Ccdot%20x_%7Bi%7D%2Bb_%7B%5Cmathrm%7Bopt%7D%7D%5Cright)%20%5Cgeqslant%20%5Cgamma%0A" /></p><p>(2) 令 <img src="https://math.now.sh?inline=R%3D%5Cmax%20_%7B1%20%5Cleqslant%20i%20%5Cleqslant%20N%7D%5Cleft%5C%7C%5Chat%7Bx%7D_%7Bi%7D%5Cright%5C%7C%2C" style="display:inline-block;margin: 0;"/> 则感知机算法 2.1 在训练数据集上的误分类次数 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 满足不等式</p>
<p style=""><img src="https://math.now.sh?from=k%20%5Cleqslant%5Cleft%28%5Cfrac%7BR%7D%7B%5Cgamma%7D%5Cright%29%5E%7B2%7D%0A" /></p></blockquote>
<ul>
<li><img src="https://math.now.sh?inline=%5Chat%7Bw%7D%3D%5Cleft%28w%5E%7B%5Cmathrm%7BT%7D%7D%2C%20b%5Cright%29%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=%5Chat%7Bx%7D%3D%5Cleft%28x%5E%7B%5Cmathrm%7BT%7D%7D%2C%201%5Cright%29%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=%5Chat%7Bx%7D%20%5Cin%20%5Cmathbf%7BR%7D%5E%7Bn%2B1%7D%2C%20%5Chat%7Bw%7D%20%5Cin%20%5Cmathbf%7BR%7D%5E%7Bn%2B1%7D" style="display:inline-block;margin: 0;"/>，<img src="https://math.now.sh?inline=%5Chat%7Bw%7D%20%5Ccdot%20%5Chat%7Bx%7D%3Dw%20%5Ccdot%20x%2Bb" style="display:inline-block;margin: 0;"/></li>
<li>对于线性可分数据集，感知机的解存在但不唯一。</li>
<li>在线性支持向量机中，添加超平面约束条件，从而得到唯一超平面。</li>
</ul>
<h3 id="3-3-算法对偶形式">3.3 算法对偶形式</h3>
<p>我们假设<img src="https://math.now.sh?inline=w%2Cb%3D0" style="display:inline-block;margin: 0;"/>，在原始算法中，对于一个误分类点<img src="https://math.now.sh?inline=%28x_i%2Cy_i%29" style="display:inline-block;margin: 0;"/>，我们进行了<img src="https://math.now.sh?inline=n_i" style="display:inline-block;margin: 0;"/>次更新，对于所有<img src="https://math.now.sh?inline=N" style="display:inline-block;margin: 0;"/>个点，我们有</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0Aw%20%26%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%20x_%7Bi%7D%20%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20n_i%28%5Ceta%20%20y_%7Bi%7D%20x_%7Bi%7D%29%5C%5C%0Ab%20%26%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bi%7D%20y_%7Bi%7D%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20n_i(%5Ceta%20y_i)%0A%5Cend%7Baligned%7D%0A" /></p><p>我们从解w，b转化为<img src="https://math.now.sh?inline=n_i" style="display:inline-block;margin: 0;"/>即第<img src="https://math.now.sh?inline=i" style="display:inline-block;margin: 0;"/>个是实例点由于误分而进行更新的次数。</p>
<p>实例点更新越多，说明距离分离超平面越近，越难分类，对结果影响大，很可能就是支持向量。</p>
<blockquote>
<p>算法 2.2 (感知机学习算法的对偶形式)<br>
输入: 线性可分的数据集 <img src="https://math.now.sh?inline=T%3D%5Cleft%5C%7B%5Cleft%28x_%7B1%7D%2C%20y_%7B1%7D%5Cright%29%2C%5Cleft(x_%7B2%7D%2C%20y_%7B2%7D%5Cright)%2C%20%5Ccdots%2C%5Cleft(x_%7BN%7D%2C%20y_%7BN%7D%5Cright)%5Cright%5C%7D%2C" style="display:inline-block;margin: 0;"/> 其中 <img src="https://math.now.sh?inline=x_%7Bi%7D%20%5Cin%20%5Cmathbf%7BR%7D%5E%7Bn%7D%2C%20y_%7Bi%7D%20%5Cin" style="display:inline-block;margin: 0;"/><br>
<img src="https://math.now.sh?inline=%5C%7B-1%2C%2B1%5C%7D%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20N%20%3B" style="display:inline-block;margin: 0;"/> 学习率 <img src="https://math.now.sh?inline=%5Ceta%280%3C%5Ceta%20%5Cleqslant%201%29%20%3B" style="display:inline-block;margin: 0;"/><br>
输 出: <img src="https://math.now.sh?inline=%5Calpha%2C%20b%20%3B" style="display:inline-block;margin: 0;"/> 感 知机 模 型 <img src="https://math.now.sh?inline=f%28x%29%3D%5Coperatorname%7Bsign%7D%5Cleft(%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bj%7D%20y_%7Bj%7D%20x_%7Bj%7D%20%5Ccdot%20x%2Bb%5Cright)%2C" style="display:inline-block;margin: 0;"/> 其中 <img src="https://math.now.sh?inline=%5Calpha%3D%5Cleft%28%5Calpha_%7B1%7D%2C%20%5Calpha_%7B2%7D%2C%20%5Ccdots%2C%20%5Calpha_%7BN%7D%5Cright%29%5E%7B%5Cmathrm%7BT%7D%7D" style="display:inline-block;margin: 0;"/><br>
(1) <img src="https://math.now.sh?inline=%5Calpha%20%5Cleftarrow%200%2C%20b%20%5Cleftarrow%200%20%3B" style="display:inline-block;margin: 0;"/><br>
(2)在训练集中选取数据 <img src="https://math.now.sh?inline=%5Cleft%28x_%7Bi%7D%2C%20y_%7Bi%7D%5Cright%29" style="display:inline-block;margin: 0;"/>;<br>
(3)如果 <img src="https://math.now.sh?inline=y_%7Bi%7D%5Cleft%28%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20%5Calpha_%7Bj%7D%20y_%7Bj%7D%20x_%7Bj%7D%20%5Ccdot%20x_%7Bi%7D%2Bb%5Cright%29%20%5Cleqslant%200" style="display:inline-block;margin: 0;"/></p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Calpha_%7Bi%7D%20%5Cleftarrow%20%5Calpha_%7Bi%7D%2B%5Ceta%20%5C%5C%0Ab%20%5Cleftarrow%20b%2B%5Ceta%20y_%7Bi%7D%0A%5Cend%7Barray%7D%0A" /></p><p>(4)转至 (2)直到没有误分类数据。</p>
</blockquote>
<ul>
<li>
<p>(12)当<img src="https://math.now.sh?inline=%5Ceta%3D1" style="display:inline-block;margin: 0;"/>，即<img src="https://math.now.sh?inline=n_i%3Dn_i%2B1" style="display:inline-block;margin: 0;"/>，对点更新次数加一</p>
</li>
<li>
<p>对偶算法只涉及到矩阵的内积<img src="https://math.now.sh?inline=x_i%5Ccdot%20x_j" style="display:inline-block;margin: 0;"/>计算，我们可以将内积预先计算存储，即我们计算Gram矩阵<img src="https://math.now.sh?inline=%5Cmathbf%7BG%7D%3D%5Bx_i%20%5Ccdot%20x_j%5D_%7B%5Cmathbf%7BN%7D%5Ctimes%5Cmathbf%7BN%7D%7D" style="display:inline-block;margin: 0;"/></p>
</li>
<li>
<p>和原始形式一样，感知机学习算法的对偶形式迭代是收敛的，且存在多个解。</p>
</li>
</ul>
<h2 id="4-Question">4. Question</h2>
<ol>
<li>
<p>超平面的定义是什么？</p>
</li>
<li>
<p>这么判断数据集可不可分？</p>
<ul>
<li>
<p>证明以下定理： 样本集线性可分的充分必要条件是正实例点集所构成的凸壳CD 与负实例点集所构成的凸壳互不相交。</p>
</li>
<li>
<p>设集合 <img src="https://math.now.sh?inline=S%20%5Csubset%20%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/> 是由 <img src="https://math.now.sh?inline=%5Cmathbf%7BR%7D%5E%7Bn%7D" style="display:inline-block;margin: 0;"/> 中的 <img src="https://math.now.sh?inline=k" style="display:inline-block;margin: 0;"/> 个点所组成的集合, 即 <img src="https://math.now.sh?inline=S%3D%5Cleft%5C%7Bx_%7B1%7D%2C%20x_%7B2%7D%2C%20%5Ccdots%2C%20x_%7Bk%7D%5Cright%5C%7D%20." style="display:inline-block;margin: 0;"/> 定义 <img src="https://math.now.sh?inline=S" style="display:inline-block;margin: 0;"/> 的凸亮<img src="https://math.now.sh?inline=%5Coperatorname%7Bconv%7D%28S%29" style="display:inline-block;margin: 0;"/> 为</p>
<p style=""><img src="https://math.now.sh?from=%5Cbegin%7Baligned%7D%0A%5Coperatorname%7Bconv%7D%28S%29%3D%5Cleft%5C%7Bx%3D%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%20%5Clambda_%7Bi%7D%20x_%7Bi%7D%20%5Cmid%20%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%20%5Clambda_%7Bi%7D%3D1%2C%20%5Clambda_%7Bi%7D%20%5Cgeqslant%200%2C%20i%3D1%2C2%2C%20%5Ccdots%2C%20k%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A" /></p></li>
</ul>
</li>
</ol>
<h2 id="5-Code">5. Code</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">perceptron</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, X_train, Y_train, learning_rate=<span class="number">0.0001</span>, tol=<span class="number">0</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        X_train:Array, default=None</span></span><br><span class="line"><span class="string">            dataset for training</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Y_train:Array, default=None</span></span><br><span class="line"><span class="string">            labelset for training</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        learning_rate:float,default=0.0001</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        tol:int, default=0</span></span><br><span class="line"><span class="string">            the stopping criterion. When the number of misclassification </span></span><br><span class="line"><span class="string">            points small or equal to tol, stop training.</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.X_train=np.mat(X_train)</span><br><span class="line">        self.Y_train=np.mat(Y_train).T</span><br><span class="line">        self.m,self.n=np.shape(self.X_train)</span><br><span class="line">        self.w=np.zeros((<span class="number">1</span>,np.shape(self.X_train)[<span class="number">1</span>]))</span><br><span class="line">        self.b=<span class="number">0</span></span><br><span class="line">        self.learning_rate=learning_rate</span><br><span class="line">        self.tol=tol</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x, w, b</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(x,w.T)+b</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        w: np.mat</span></span><br><span class="line"><span class="string">            weights</span></span><br><span class="line"><span class="string">        b: np.mat</span></span><br><span class="line"><span class="string">            bias        </span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        is_wrong=<span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> is_wrong:</span><br><span class="line">            wrong_count=<span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.m):</span><br><span class="line">                xi=self.X_train[s]</span><br><span class="line">                yi=self.Y_train[s]</span><br><span class="line">                <span class="keyword">if</span> self.calculate(xi,self.w,self.b)*yi &lt;= <span class="number">0</span>:</span><br><span class="line">                    self.w=self.w+self.learning_rate*yi*xi</span><br><span class="line">                    self.b=self.b+self.learning_rate*yi</span><br><span class="line">                    wrong_count+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> wrong_count &lt;=self.tol:</span><br><span class="line">                is_wrong=<span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> self.w,self.b</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<h2 id="6-Proof">6. Proof</h2>
<h3 id="6-1-Theorem-2-01">6.1 Theorem 2.01</h3>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-2%20thm2.01%20distance" alt="image-20201123163513240" style="zoom:50%;" />
<h3 id="6-2-Theorem-2-1">6.2 Theorem 2.1</h3>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-2%20thm2.1%20Novikoff.png" alt="SLM-2 thm2.1 Novikoff" style="zoom:50%;" />
<h2 id="7-Reference">7. Reference</h2>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>
<p><a target="_blank" rel="noopener" href="https://www.pkudodo.com/2018/11/18/1-4/">统计学习方法|感知机原理剖析及实现</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26526858/answer/136577337">如何理解感知机学习算法的对偶形式？ - Zongrong Zheng的回答 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">fengdu78/lihang-code</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>






<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CHongfeng Ling 凌崇锋</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  







  





  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
