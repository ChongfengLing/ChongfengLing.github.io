<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/12/19/hello-world/" class="article-date">
  <time class="dt-published" datetime="2020-12-19T15:09:08.996Z" itemprop="datePublished">2020-12-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2020/12/19/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/12/19/hello-world/" data-id="ckivua5ut0000hsp4cvfrgfni" data-title="Hello World" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-[SLM-7] 支持向量机 SVM" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/12/11/%5BSLM-7%5D%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%20SVM/" class="article-date">
  <time class="dt-published" datetime="2020-12-11T09:06:20.416Z" itemprop="datePublished">2020-12-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="支持向量机-Support-Vector-Machines"><a href="#支持向量机-Support-Vector-Machines" class="headerlink" title="支持向量机 Support Vector Machines"></a>支持向量机 Support Vector Machines</h1><p>支持向量机(Support Vector Machines)是一种<strong>二元分类</strong>模型。<strong>基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。</strong>基本模型是<strong>特征空间上的间隔最大</strong>的线性分类器，区别于感知机。学习策略为间隔最大化，可化为求解凸二次规划convex quadratic programming。学习算法为求解凸二次规划的最优算法。</p>
<h2 id="1-线性可分支持向量机与硬间隔最大化"><a href="#1-线性可分支持向量机与硬间隔最大化" class="headerlink" title="1. 线性可分支持向量机与硬间隔最大化"></a>1. 线性可分支持向量机与硬间隔最大化</h2><h3 id="1-1-Model"><a href="#1-1-Model" class="headerlink" title="1.1 Model"></a>1.1 Model</h3><blockquote>
<p>定义 7.1 (线性可分支持向量机 ) $\quad$ 给定<strong>线性可分</strong>训练数据集<br>$$<br>T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},\nonumber\x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}={+1,-1}, i=1,2, \cdots, N<br>$$<br>通过<strong>间隔最大化或等价地求解相应的凸二次规划问题</strong>学习得到的分离超平面为<br>$$<br>w^{<em>} \cdot x+b^{</em>}=0<br>$$<br>以及相应的分类决策函数<br>$$<br>f(x)=\operatorname{sign}\left(w^{<em>} \cdot x+b^{</em>}\right)<br>$$<br>称为线性可分支持向量机。</p>
</blockquote>
<ul>
<li>对线性可分数据集，存在无穷超平面分离数据。感知机用误分类最小策略，得到的解无穷多个。SVM用间隔最大化策略，得到的解唯一。</li>
</ul>
<h3 id="1-2-间隔"><a href="#1-2-间隔" class="headerlink" title="1.2 间隔"></a>1.2 间隔</h3><ol>
<li><p>点到分离超平面的远近表示对分类预测的确信程度。</p>
</li>
<li><p>$y_{pre}$与$\hat{y}$的符号一致与否表示分类预测的准确性。</p>
</li>
</ol>
<h4 id="1-2-1-函数间隔-Functional-Margin"><a href="#1-2-1-函数间隔-Functional-Margin" class="headerlink" title="1.2.1 函数间隔 Functional Margin"></a>1.2.1 函数间隔 Functional Margin</h4><blockquote>
<p>定义 7.2 (函数间隔) $\quad$ 对于给定的训练数据集 $T$ 和超平面 $(w, b),$ 定义超平面$(w, b)$ <strong>关于样本点 $\left(x_{i}, y_{i}\right)$ 的函数间隔</strong>为<br>$$<br>\hat{\gamma}<em>{i}=y</em>{i}\left(w \cdot x_{i}+b\right)<br>$$<br>定义超平面 $(w, b)$ <strong>关于训练数据集 $T$ 的函数间隔</strong>为超平面 $(w, b)$ 对于 $T$ 中所有样本点 $\left(x_{i}, y_{i}\right)$ 的函数间隔之最小值，即<br>$$<br>\hat{\gamma}=\min <em>{i=1, \cdots, N} \hat{\gamma}</em>{i}<br>$$</p>
</blockquote>
<ul>
<li><p>(3)表示分类的准确性与准确程度</p>
</li>
<li><p>对超平面$w \cdot x+b=0$，成倍的改变$w,;b$不会改变该平面，但是会成倍的改变函数间隔，且<strong>倍数相等</strong></p>
</li>
</ul>
<h4 id="1-2-2-几何间隔-Geometric-Margin"><a href="#1-2-2-几何间隔-Geometric-Margin" class="headerlink" title="1.2.2 几何间隔 Geometric Margin"></a>1.2.2 几何间隔 Geometric Margin</h4><p>对分离超平面$w \cdot x+b=0$的法向量$w$进行正规化，使得$||w’||=\frac{w}{|w|}$（同时对$b$也是）。</p>
<blockquote>
<p>定义 7.3 (几何间隔) $\quad$ 对于给定的训练数据集 $T$ 和超平面 $(w, b),$ 定义超平面$(w, b)$ 关于样本,点 $\left(x_{i}, y_{i}\right)$ 的几何间隔为<br>$$<br>\gamma_{i}=y_{i}\left(\frac{w}{|w|} \cdot x_{i}+\frac{b}{|w|}\right)<br>$$<br>定义超平面 $(w, b)$ 对于训练数据集 $T$ 的几何间隔为超平面 $(w, b)$ 关千 $T$ 中所有样本点 $\left(x_{i}, y_{i}\right)$ 的几何间隔之最小值，即<br>$$<br>\gamma=\min <em>{i=1, \cdots, N} \gamma</em>{i}<br>$$</p>
</blockquote>
<ul>
<li>几何间隔不随参数的改变而改变</li>
</ul>
<h3 id="1-3-间隔最大化"><a href="#1-3-间隔最大化" class="headerlink" title="1.3 间隔最大化"></a>1.3 间隔最大化</h3><ul>
<li>对线性可分数据集，线性可分分离超平面有无穷多个（即感知机），但几何间隔最大的分离超平面唯一。</li>
<li>对线性可分训练集，间隔最大化又称硬间隔最大化。</li>
<li>间隔最大化，即以充分大的确信度，对训练数据进行分类；也就是说，在正负实例分开的同时，对离超平面最近的点也能有足够大的确信度。</li>
</ul>
<h4 id="1-3-1-Algorithm"><a href="#1-3-1-Algorithm" class="headerlink" title="1.3.1 Algorithm"></a>1.3.1 Algorithm</h4><p>由几何间隔的定义可知，硬间隔最大化可以表示为<br>$$<br>\begin{array}{ll}<br>\max <em>{w, b} &amp; \gamma \<br>\text { s.t. } &amp; y</em>{i}\left(\frac{w}{|w|} \cdot x_{i}+\frac{b}{|w|}\right) \geqslant \gamma&gt;0, \quad i=1,2, \cdots, N<br>\end{array}<br>$$<br>由几何间隔和函数间隔的定义，(7)转化成<br>$$<br>\begin{array}{ll}<br>\max <em>{u \cdot b}&amp; \frac{\hat{\gamma}}{|w|} \<br>\text { s.t. } &amp; y</em>{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N<br>\end{array}<br>$$<br>我们要求$w,;b$，使得$\frac{\hat{\gamma}}{|w|}$最大化，在这个目标函数与约束条件中，$\hat{y}$的取值对最后的超平面没有影响。于是我们设$\hat{y}=1$，当成单位1。同时最大化$\frac{1}{|w|}$ 和最小化 $\frac{1}{2}|w|^{2}$等价。(8)转化成<br>$$<br>\begin{array}{ll}<br>\min <em>{w, b} &amp; \frac{1}{2}|w|^{2} \<br>\text { s.t. } &amp; y</em>{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N<br>\end{array}<br>$$</p>
<ul>
<li>(9)为凸二次规划问题convex quadratic programming</li>
</ul>
<p>求出(9)的解$w^*,;b^*$，我们可以得出最大间隔分离超平面$w^{<em>} \cdot x+b^{</em>}=0$ 及分类决策函数$f(x)=\operatorname{sign}\left(w^{<em>} \cdot x+b^{</em>}\right)$，即线性可分向量机模型。</p>
<blockquote>
<p>算法 7.1 (线性可分支持向量机学习算法————最大间隔法)<br>输入: 线性可分训练数据集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right}$，$x_{i}\in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}={-1,+1}, i=1,2, \cdots, N$<br>输出：最大间隔分离超平面和分类决策函数。</p>
<p>（1）构造并求解约束最优化问题:<br>$$<br>\begin{array}{ll}<br>\min <em>{w, b} &amp; \frac{1}{2}|w|^{2} \<br>\text { s.t. } &amp; y</em>{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N<br>\end{array}<br>$$<br>求得最优解 $w^{<em>}, b^{</em>}$ 。<br>（2）由此得到分离超平面:<br>$$<br>w^{<em>} \cdot x+b^{</em>}=0<br>$$<br>分类决策函数<br>$$<br>f(x)=\operatorname{sign}\left(w^{<em>} \cdot x+b^{</em>}\right)<br>$$</p>
</blockquote>
<h4 id="1-3-2-解的存在性与唯一性"><a href="#1-3-2-解的存在性与唯一性" class="headerlink" title="1.3.2 解的存在性与唯一性"></a>1.3.2 解的存在性与唯一性</h4><blockquote>
<p>定理 7.1 (最大间隔分离超平面的存在唯一性) $\quad$ 若训练数据集 $T$ 线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</p>
</blockquote>
<h4 id="1-3-3-支持向量-Support-Vector-amp-间隔边界"><a href="#1-3-3-支持向量-Support-Vector-amp-间隔边界" class="headerlink" title="1.3.3 支持向量 Support Vector &amp; 间隔边界"></a>1.3.3 支持向量 Support Vector &amp; 间隔边界</h4><p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F.png" alt="image-20201217151120228"></p>
<p>对线性可分数据集：</p>
<ul>
<li>支持向量：训练样本点中与分离超平面距离最近的**实例们$(x_i,;b_i)$**，使得约束调节(10)取等号。即位于$H_{1,2}: w \cdot x+b=\pm1$上的两（或更多）点。</li>
<li>间隔边界：$H_1$和$H_2$</li>
</ul>
<p><strong>此模型的结果只由这些少数个支持向量决定</strong>，故此得名。</p>
<h3 id="1-4-对偶算法-Dual-Problem"><a href="#1-4-对偶算法-Dual-Problem" class="headerlink" title="1.4 对偶算法 Dual Problem"></a>1.4 对偶算法 Dual Problem</h3><p>求解最优化问题(10)，我们可以把其当成原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。</p>
<ol>
<li>对偶问题更容易求解。高维甚至无限维的最优化问题难求解。</li>
<li>引入核函数</li>
</ol>
<h4 id="1-4-1-对偶问题的导出"><a href="#1-4-1-对偶问题的导出" class="headerlink" title="1.4.1 对偶问题的导出"></a>1.4.1 对偶问题的导出</h4><p>对于拘束最优化问题(10)<br>$$<br>\begin{array}{ll}<br>\min <em>{w, b} &amp; \frac{1}{2}|w|^{2} \<br>\text { s.t. } &amp; y</em>{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N\nonumber \tag{10}<br>\end{array}<br>$$<br>的每一个约束条件，我们引入一个拉格朗日乘子 Lagrange multiplier $\alpha_{i} \geqslant 0$, $i=1,2, \cdots, N$，定义拉格朗日函数 Generalized Lagrange Function，$\alpha=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}$ 为拉格朗日乘子向量。<br>$$<br>\begin{align}<br>L(w, b, \alpha)&amp;=\frac{1}{2}|w|^{2}-\sum_{i=1}^{N} \alpha_{i}(y_{i}\left(w \cdot x_{i}+b\right)-1)\nonumber<br>\&amp;=\frac{1}{2}|w|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}<br>\end{align}<br>$$<br>带拘束最优化问题(10)可以变成无拘束优化问题(14)<br>$$<br>\begin{array}{l}<br>\min _{w, b} \max <em>{\lambda} \mathcal{L}(w, b, \alpha) \<br>\text { s.t. } \lambda</em>{i} \geqslant 0<br>\end{array}<br>$$</p>
<p>由强对偶关系，拘束问题(14)可以化为无拘束优化问题<br>$$<br>\begin{array}{l}<br>\max _{\lambda} \min <em>{w, b} \mathcal{L}(w, b, \alpha) \<br>\text { s.t. } \lambda</em>{i} \geqslant 0<br>\end{array}<br>$$</p>
<p>由上面的拉格朗日（强）对偶性可得，原始问题(10)的对偶问题是极大极小问题(15)</p>
<ul>
<li>详细推导见：【拉格朗日对偶，等价对偶以及KKT条件】（文件没保存，有缘再续）</li>
</ul>
<h4 id="1-4-2-对偶问题的计算"><a href="#1-4-2-对偶问题的计算" class="headerlink" title="1.4.2 对偶问题的计算"></a>1.4.2 对偶问题的计算</h4><p><strong>1.4.2.1: 求$\min <em>{w, b} L(w, b, \alpha)=\frac{1}{2}|w|^{2}-\sum</em>{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}$</strong></p>
<p>计算$\min <em>{w, b} L(w, b, \alpha)$对变量$w,;b$的偏导为0<br>$$<br>\begin{array}{l}<br>\nabla</em>{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0 \<br>\nabla_{b} L(w, b, \alpha)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0\nonumber\<br>\end{array}<br>$$<br>得<br>$$<br>\begin{array}{l}<br>w&amp;=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}\<br>\sum_{i=1}^{N} \alpha_{i} y_{i}&amp;=0<br>\end{array}<br>$$<br>把(16)带入到(13)，得<br>$$<br>\begin{aligned}<br>L(w, b, \alpha) &amp;=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \<br>&amp;=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}<br>\end{aligned}<br>$$<br>得到结果<br>$$<br>\min <em>{w, b} L(w, b, \alpha)=-\frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}<br>$$<br><strong>求$\min _{w, b} L(w, b, \alpha) \text { 对 } \alpha \text { 的极大 }$</strong><br>$$<br>\begin{aligned}<br>\max <em>{\alpha} &amp; -\frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \<br>\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>\end{aligned}<br>$$<br>调整符号，得与原始问题等价的对偶最优化问题<br>$$<br>\begin{array}{1}<br>\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>\end{array}<br>$$</p>
<ul>
<li>$ \sum_{i=1}^{N} \alpha_{i} y_{i}=0$意味着很大部分的$\alpha=0$，即决定超平面的只是很小一部分的支持向量。</li>
</ul>
<h4 id="1-4-3-对偶问题的解"><a href="#1-4-3-对偶问题的解" class="headerlink" title="1.4.3 对偶问题的解"></a>1.4.3 对偶问题的解</h4><blockquote>
<p>定理 7.2 设 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{l}^{</em>}\right)^{\mathrm{T}}$ 是对偶最优化问题 (7.22)$\sim(7.24)$ 的解，则存在下标 $j,$ 使得 $\alpha_{j}^{<em>}&gt;0,$ 并可按下式求得原始最优化问题 (7.13)$\sim(7.14)$ 的解 $w^{</em>}, b^{<em>}$ :<br>$$<br>\begin{array}{c}<br>w^{</em>}=\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i} x_{i} \<br>b^{</em>}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)<br>\end{array}<br>$$</p>
</blockquote>
<p>分离超平面为$\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i}\left(x \cdot x_{i}\right)+b^{</em>}=0$</p>
<p>分类决策函数为$f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i}\left(x \cdot x_{i}\right)+b^{</em>}\right)$</p>
<h4 id="1-4-4-Algorithm"><a href="#1-4-4-Algorithm" class="headerlink" title="1.4.4 Algorithm"></a>1.4.4 Algorithm</h4><blockquote>
<p>算法 7.2 (线性可分支持向量机学习算法)<br>输入: 线性可分训练集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},$ 其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}$, $y_{i} \in \mathcal{Y}={-1,+1}, i=1,2, \cdots, N$<br>输出：分离超平面和分类决策函数。<br>（1）构造并求解约束最优化问题<br>$$<br>\begin{array}{ll}<br>\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>&amp; \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>\end{array}<br>$$<br>求得最优解 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$ 。<br>(2) 计算<br>$$<br>w^{<em>}=\sum_{i=1}^{N} \alpha_{i}^{</em>} y_{i} x_{i}<br>$$<br>并选择 $\alpha^{<em>}$ 的一个正分量 $\alpha_{j}^{</em>}&gt;0,$ 计算<br>$$<br>b^{<em>}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{</em>} y_{i}\left(x_{i} \cdot x_{j}\right)<br>$$<br>（3）求得分离超平面<br>$$<br>w^{<em>} \cdot x+b^{</em>}=0<br>$$<br>分类决策函数:<br>$$<br>f(x)=\operatorname{sign}\left(w^{<em>} \cdot x+b^{</em>}\right)<br>$$</p>
</blockquote>
<h2 id="2-线性支持向量机与软间隔最大化"><a href="#2-线性支持向量机与软间隔最大化" class="headerlink" title="2. 线性支持向量机与软间隔最大化"></a>2. 线性支持向量机与软间隔最大化</h2><p>对线性不可分数据集，存在一些特异点（outlier）使得不等式不能全部满足，即函数间隔大于1。</p>
<p>因此我们引入松弛变量$\xi_i \geq0$，使得函数间隔大于等于$1-\xi_i$。同时对松弛变量付出一个惩罚参数。</p>
<ul>
<li><p>约束条件<br>$$<br>y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}<br>$$</p>
</li>
<li><p>目标函数<br>$$<br>\frac{1}{2}|w|^{2}+C \sum_{i=1}^{N} \xi_{i}<br>$$</p>
<ul>
<li>$C$为惩罚参数。$C$越大，对误分类的惩罚越大</li>
<li>最小化目标函数，即使$\frac{1}{2}|w|^{2}$尽可能小，间隔尽可能大</li>
</ul>
</li>
</ul>
<h3 id="2-1-原始问题"><a href="#2-1-原始问题" class="headerlink" title="2.1 原始问题"></a>2.1 原始问题</h3><p>$$<br>\begin{array}{ll}<br>\min <em>{w, b, \xi} &amp; \frac{1}{2}|w|^{2}+C \sum</em>{i=1}^{N} \xi_{i} \<br>\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \<br>&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>\end{array}<br>$$</p>
<ul>
<li>凸二次规划问题</li>
<li>$w$的解唯一</li>
<li>$b$的解可能不唯一，而是一个区间</li>
</ul>
<h3 id="2-2-拉格朗日函数"><a href="#2-2-拉格朗日函数" class="headerlink" title="2.2 拉格朗日函数"></a>2.2 拉格朗日函数</h3><p>$$<br>L(w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}|w|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}\<br>\alpha_i\geq 0,;\mu_i\geq0<br>$$</p>
<h3 id="2-3-对偶问题"><a href="#2-3-对偶问题" class="headerlink" title="2.3 对偶问题"></a>2.3 对偶问题</h3><p>原始问题(27)的对偶问题使拉格朗日函数的极大极小问题。<br>$$<br>\begin{array}{ll}<br>\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N<br>\end{array}<br>$$</p>
<h3 id="2-4-解的等价性"><a href="#2-4-解的等价性" class="headerlink" title="2.4 解的等价性"></a>2.4 解的等价性</h3><blockquote>
<p>定理 7.3 设 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$ 是对偶问题的一个解，若存在 $\alpha^{<em>}$ 的一个分量 $\alpha_{j}^{</em>}, 0&lt;\alpha_{j}^{<em>}&lt;C,$ 则原始问题的解 $w^{</em>}, b^{<em>}$ 可按下式 求得:<br>$$<br>\begin{array}{c}<br>w^{</em>}=\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i} x_{i} \<br>b^{</em>}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)<br>\end{array}<br>$$</p>
</blockquote>
<blockquote>
<p>算法 7.3 (线性支持向量机学习算法)<br>输入: 训练数据集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},$ 其中， $x_{i} \in \mathcal{X}=\mathbf{R}^{n}$, $y_{i} \in \mathcal{Y}={-1,+1}, i=1,2, \cdots, N$<br>输出：分离超平面和分类决策函数。<br>（1）选择惩罚参数 $C&gt;0,$ 构造并求解凸二次规划问题<br>$$<br>\begin{array}{ll}<br>\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N<br>\end{array}<br>$$<br>求得最优解 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$<br>（2） 计算 $w^{<em>}=\sum_{i=1}^{N} \alpha_{i}^{</em>} y_{i} x_{i}$<br>选择 $\alpha^{<em>}$ 的一个分量 $\alpha_{j}^{</em>}$ 适合条件 $0&lt;\alpha_{j}^{<em>}&lt;C$ ，计算<br>$$<br>b^{</em>}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{<em>}\left(x_{i} \cdot x_{j}\right)<br>$$<br>（3）求得分离超平面<br>$$<br>w^{</em>} \cdot x+b^{<em>}=0<br>$$<br>分类决策函数:<br>$$<br>f(x)=\operatorname{sign}\left(w^{</em>} \cdot x+b^{*}\right)<br>$$</p>
</blockquote>
<ul>
<li>(32)中，每一个合适的$\alpha$都会得出一个$b$，所以$b$不唯一</li>
</ul>
<h3 id="2-4-支持向量"><a href="#2-4-支持向量" class="headerlink" title="2.4 支持向量"></a>2.4 支持向量</h3><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E8%BD%AF%E9%97%B4%E9%9A%94%E5%88%86%E7%A6%BB%E5%90%91%E9%87%8F.png" alt="image-20201216151045483" style="zoom:50%;" />

<p>虚线是间隔边界，实线是分离超平面。对每一个实例点$(x_i,y_i)$和他们的$\alpha _i^*,;\xi_i$，我们有</p>
<ul>
<li>$\alpha_{i}^{*}&lt;C$，则 $\xi_{i}=0$</li>
<li>$\alpha_{i}^{*}=C, 0&lt;\xi_{i}&lt;1$，则</li>
<li>$\alpha_{i}^{*}=C, \xi_{i}=1$，则</li>
<li>$\alpha_{i}^{*}=C, \xi_{i}&gt;1$，则</li>
<li>没有$\alpha_{i}^{*}=C, 0&lt;\xi_{i}&lt;1$</li>
</ul>
<h3 id="2-5-合页损失函数-Hinge-Loss-Function"><a href="#2-5-合页损失函数-Hinge-Loss-Function" class="headerlink" title="2.5 合页损失函数 Hinge Loss Function"></a>2.5 合页损失函数 Hinge Loss Function</h3><blockquote>
<p>定理 7.4 线性支持向量机原始最优化问题:<br>$$<br>\begin{array}{ll}<br>\min <em>{w, b, \xi} &amp; \frac{1}{2}|w|^{2}+C \sum</em>{i=1}^{N} \xi_{i} \<br>\text { s.t. } &amp; y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \<br>&amp; \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N<br>\end{array}<br>$$<br>等价于最优化问题<br>$$<br>\min <em>{w, b} \sum</em>{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda|w|^{2}<br>$$</p>
</blockquote>
<ul>
<li>$[z]_{+}=\left{\begin{array}{ll}z, &amp; z&gt;0 \ 0, &amp; z \leqslant 0\end{array}\right.$表示取正值的函数。</li>
<li>$L(y(w \cdot x+b))=[1-y(w \cdot x+b)]_{+}$ 经验损失。但实例点被正确分类且函数间隔大于1，损失才是0。位于间隔边界上的实例点损失不为0。</li>
<li>$\lambda|w|^{2}$ 正则化项</li>
</ul>
<h4 id="2-5-1-损失函数对比"><a href="#2-5-1-损失函数对比" class="headerlink" title="2.5.1 损失函数对比"></a>2.5.1 损失函数对比</h4><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-7%20%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt="image-20201216155233841" style="zoom:50%;" />

<p>虚线为感知机的损失函数$\left[-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}$</p>
<ul>
<li>0-1损失函数为二元分类真正的损失函数。<strong>不过因其不连续可导，无法直接优化它所构成的函数</strong>。</li>
<li>合页损失函数是0-1损失函数的上界。<strong>线性支持向量机是优化由0-1损失函数的上界构成的目标函数</strong>。</li>
<li>合页损失函数对学习有更高要求。不但需要分类正确，还要确信度（距离）足够高才为0损失。</li>
</ul>
<h2 id="3-非线性支持向量机与核函数"><a href="#3-非线性支持向量机与核函数" class="headerlink" title="3. 非线性支持向量机与核函数"></a>3. 非线性支持向量机与核函数</h2><p>对于一个非线性数据集，首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。</p>
<p>由于特征空间是高维甚至无限维，加上对偶问题中多组高维向量计算内积。因此引入核函数减少计算量。</p>
<h3 id="3-1-核技巧-Kernel-Trick"><a href="#3-1-核技巧-Kernel-Trick" class="headerlink" title="3.1 核技巧 Kernel Trick"></a>3.1 核技巧 Kernel Trick</h3><p>核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间 (欧氏空间 $\mathbf{R}^{n}$ 或离散集合) 对应于一个特征空间 (希尔伯特空间 $\mathcal{H}$ )，使得在输入空间 $\mathbf{R}^{n}$ 中的超曲面模型对应于特征空间 $\mathcal{H}$ 中的超平面模型支持向量机）。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。</p>
<blockquote>
<p>定义 7.6 (核函数) $\quad$ 设 $\mathcal{X}$ 是输入空间 ( 欧氏空间 $\mathbf{R}^{n}$ 的子集或离散集合 $),$ 又设 $\mathcal{H}$ 为特征空间 $($ 希 尔伯特空间 $),$ 如果存在一个从 $\mathcal{X}$ 到 $\mathcal{H}$ 的映射<br>$$<br>\phi(x): \mathcal{X} \rightarrow \mathcal{H}<br>$$<br>使得对所有 $x, z \in \mathcal{X},$ 函数 $K(x, z)$ 满足条件<br>$$<br>K(x, z)=\phi(x) \cdot \phi(z)<br>$$<br>则称 <strong>$K(x, z)$ 为核函数， $\phi(x)$ 为映射函数</strong>，式中 $\phi(x) \cdot \phi(z)$ 为 $\phi(x)$ 和 $\phi(z)$ 的内积。</p>
</blockquote>
<ul>
<li><strong>在使用过程中只定义核函数$K(x,z)$，不显式定义映射函数$\phi$</strong></li>
<li>特征空间$\mathcal{H}$一般是高维甚至无限维</li>
<li>给定核$K(x,z)$，特征空间和映射函数不唯一</li>
</ul>
<p>在支持向量机中，通过映射函数$\phi$把输入空间变换到新的特征空间，在新的特征空间中训练线性支持向量机。此时对偶问题的目标函数成为$W(\alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}$，分类决策函数成为$f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{<em>} y_{i} \phi\left(x_{i}\right) \cdot \phi(x)+b^{</em>}\right)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{<em>} y_{i} K\left(x_{i}, x\right)+b^{</em>}\right)$</p>
<h3 id="3-2-正定核"><a href="#3-2-正定核" class="headerlink" title="3.2 正定核"></a>3.2 正定核</h3><blockquote>
<p>定理 7.5 (正定核的充要条件) $\quad$ 设 $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbf{R}$ 是对称函数，则 $K(x, z)$ 为正 定核函数的充要条件是对任意 $x_{i} \in \mathcal{X}, i=1,2, \cdots, m, K(x, z)$ 对应的 Gram 矩阵:<br>$$<br>K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}<br>$$<br>是半正定矩阵。</p>
</blockquote>
<blockquote>
<p>定义 7.7 (正定核的等价定义) $\quad$ 设 $\mathcal{X} \subset \mathbf{R}^{n}, K(x, z)$ 是定义在 $\mathcal{X} \times \mathcal{X}$ 上的对称<br>函数, 如果对任意 $x_{i} \in \mathcal{X}, i=1,2, \cdots, m, K(x, z)$ 对应的 Gram 矩阵<br>$$<br>K=\left[K\left(x_{i}, x_{j}\right)\right]_{m \times m}<br>$$</p>
</blockquote>
<h3 id="3-3-常用核函数"><a href="#3-3-常用核函数" class="headerlink" title="3.3 常用核函数"></a>3.3 常用核函数</h3><h4 id="3-3-1-多项式核函数-Polynomial-Kernel-Function"><a href="#3-3-1-多项式核函数-Polynomial-Kernel-Function" class="headerlink" title="3.3.1 多项式核函数 Polynomial Kernel Function"></a>3.3.1 多项式核函数 Polynomial Kernel Function</h4><p>$$<br>K(x, z)=(x \cdot z+1)^{p}<br>$$</p>
<p>对应的支持向量机是一个 $p$ 次多项式分类器。在此情形下，分类决策函数成为<br>$$<br>f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{<em>} y_{i}\left(x_{i} \cdot x+1\right)^{p}+b^{</em>}\right)<br>$$</p>
<h4 id="3-3-2-高斯核函数-Gaussian-Kernel-Function"><a href="#3-3-2-高斯核函数-Gaussian-Kernel-Function" class="headerlink" title="3.3.2 高斯核函数 Gaussian Kernel Function"></a>3.3.2 高斯核函数 Gaussian Kernel Function</h4><p>$$<br>K(x, z)=\exp \left(-\frac{|x-z|^{2}}{2 \sigma^{2}}\right)<br>$$</p>
<p>对应的支持向量机是高斯径向基函数（radial basis function）分类器。在此情形下，分类决策函数成为<br>$$<br>f(x)=\operatorname{sign}\left(\sum_{i=1}^{N_{s}} a_{i}^{<em>} y_{i} \exp \left(-\frac{\left|x-x_{i}\right|^{2}}{2 \sigma^{2}}\right)+b^{</em>}\right)<br>$$</p>
<h4 id="3-3-3-字符串核函数-String-Kernel-Function"><a href="#3-3-3-字符串核函数-String-Kernel-Function" class="headerlink" title="3.3.3 字符串核函数 String Kernel Function"></a>3.3.3 字符串核函数 String Kernel Function</h4><p>定义在离散数据集合上。</p>
<h3 id="3-4-非线性支持向量机"><a href="#3-4-非线性支持向量机" class="headerlink" title="3.4 非线性支持向量机"></a>3.4 非线性支持向量机</h3><blockquote>
<p>定义 7.8 (非线性支持向量机 $) \quad$ 从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划(46)学习得到的分类决策函数<br>$$<br>f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i} K\left(x, x_{i}\right)+b^{</em>}\right)<br>$$<br>称为非线性支持向量机, $K(x, z)$ 是正定核涵数。</p>
</blockquote>
<blockquote>
<p>算法 7.4 (非线性支持向量机学习算法) 输入: 训练数据集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},$ 其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in$$\mathcal{Y}={-1,+1}, i=1,2, \cdots, N$<br>输出：分类决策函数。<br>（1）选取适当的核函数 $K(x, z)$ 和适当的参数 $C,$ 构造并求解最优化问题<br>$$<br>\begin{array}{1}<br>\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N<br>\end{array}<br>$$<br>求得最优解 $\alpha^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \cdots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$ 。<br>（2）选择 $\alpha^{<em>}$ 的一个正分量 $0&lt;\alpha_{j}^{</em>}&lt;C,$ 计算<br>$$<br>b^{<em>}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{</em>} y_{i} K\left(x_{i}, x_{j}\right)<br>$$<br>（3）构造决策函数:<br>$$<br>f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{<em>} y_{i} K\left(x, x_{i}\right)+b^{</em>}\right)<br>$$</p>
</blockquote>
<ul>
<li>$K(x,z)$为正定核函数是，问题(46)是二次规划问题，解存在。</li>
</ul>
<h2 id="4-SMO-Algorithm-sequential-minimal-optimization"><a href="#4-SMO-Algorithm-sequential-minimal-optimization" class="headerlink" title="4. SMO Algorithm (sequential minimal optimization)"></a>4. SMO Algorithm (sequential minimal optimization)</h2><p>序列最小算法用来求解凸二次规划的对偶问题<br>$$<br>\begin{array}{ll}<br>\min <em>{\alpha} &amp; \frac{1}{2} \sum</em>{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \<br>\text { s.t. } &amp; \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \<br>&amp; 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N<br>\end{array}<br>$$</p>
<h2 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h2><p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/12/11/[SLM-7]%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%20SVM/" data-id="ckivv4eku00058cp48u4aassn" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-[SLM-5] 决策树 Decision Tree" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/12/02/%5BSLM-5%5D%20%E5%86%B3%E7%AD%96%E6%A0%91%20Decision%20Tree/" class="article-date">
  <time class="dt-published" datetime="2020-12-02T08:14:56.880Z" itemprop="datePublished">2020-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="决策树-Decision-Tree"><a href="#决策树-Decision-Tree" class="headerlink" title="决策树 Decision Tree"></a>决策树 Decision Tree</h1><p>决策树是一种基本的分类与回归方法，这里主要讨论分类问题。<strong>他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</strong>学习过程主要包括3个步骤：特征选择、决策树的生成、决策树的修建，并根据损失函数最小化的原则建立决策树模型。</p>
<h2 id="1-Model"><a href="#1-Model" class="headerlink" title="1. Model"></a>1. Model</h2><blockquote>
<p>定义 5.1 (决策树) $\quad$ 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点 ( node ) 和有向边 ( directed edge ) 组成。结点有两种类型: 内部结点 ( internal node ) 和叶结,点 ( leaf node ) 。内部结点表示一个特征或属性, 叶结点表示一个类。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">		A--是--&gt;C[叶结点]</span><br><span class="line">		A((根节点))--否--&gt;B((内部结点))</span><br><span class="line">		B--是--&gt;D[叶结点]</span><br><span class="line">		B--否--&gt;E[叶结点]</span><br></pre></td></tr></table></figure>
<h3 id="1-1-决策树与if-then规则"><a href="#1-1-决策树与if-then规则" class="headerlink" title="1.1 决策树与if-then规则"></a>1.1 决策树与if-then规则</h3><ul>
<li>根结点到叶结点的每一条路径是一条规则</li>
<li>路径上的内部节点对应规则的条件</li>
<li>叶结点的类对应规则的结论</li>
<li>if-then规则是<strong>互斥且完备</strong>，即每一个实例有且仅有被一条路径/规则覆盖。</li>
</ul>
<h3 id="1-2-决策树与给定特征条件下的类条件概率分布"><a href="#1-2-决策树与给定特征条件下的类条件概率分布" class="headerlink" title="1.2 决策树与给定特征条件下的类条件概率分布"></a>1.2 决策树与给定特征条件下的类条件概率分布</h3><p>**决策树的生成等价于对特征空间的划分(partition)**，从而划分成互不相交的单元(cell)/区域(region)，再每一个单元定义一个类的概率分布就构成的一个条件概率分布。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。</p>
<p>$P(Y|X),;X:特征的随机变量;;;;Y:类的随机变量$</p>
<p>条件概率$P(Y|X)$往往偏大于某一类$y$。分类时把该节点实例强行分到条件概率大的那一类。</p>
<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLm-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87.png" alt="image-20201202210309048" style="zoom:50%;" />

<h3 id="1-3-决策树的学习"><a href="#1-3-决策树的学习" class="headerlink" title="1.3 决策树的学习"></a>1.3 决策树的学习</h3><p>学习本质：为训练数据集归纳出一组分类规则</p>
<ul>
<li>正确分类训练数据集的决策树可能有很多个，可能没有</li>
<li>需要找到一个与训练数据集的误差较小、泛化能力高的决策树</li>
</ul>
<p>损失函数与策略：最小化正则化后的极大似然函数</p>
<p>算法：递归的选择最优特征。ID3、C4.5、CART</p>
<p>剪枝：树枝多，复杂，泛化能力低。自下而上进行剪枝。树的生成考虑局部最优，树的剪枝考虑全局最优。</p>
<h2 id="2-特征选择-Feature-Selection"><a href="#2-特征选择-Feature-Selection" class="headerlink" title="2. 特征选择 Feature Selection"></a>2. 特征选择 Feature Selection</h2><p>通过信息增益（比）来选取具有分类能力的特征（指那些与随机分类有较大查别的特征），从而提高决策树的学习效率。</p>
<h3 id="2-1-信息增益-Information-Gain"><a href="#2-1-信息增益-Information-Gain" class="headerlink" title="2.1 信息增益 Information Gain"></a>2.1 信息增益 Information Gain</h3><h4 id="2-1-1-熵-Entropy、信息熵"><a href="#2-1-1-熵-Entropy、信息熵" class="headerlink" title="2.1.1 熵 Entropy、信息熵"></a>2.1.1 熵 Entropy、信息熵</h4><blockquote>
<p>定义 5.01（熵）        熵是表示随机变量不确定性的度量。设$X$为离散随机变量，概率分布为<br>$$<br>P\left(X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n<br>$$<br>随机变量$X$的熵为<br>$$<br>H(X)=H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}<br>$$</p>
<p>$$<br>对p_i=0，定义0log0=0\nonumber<br>$$</p>
</blockquote>
<ul>
<li>在(2)中，对数底为2/$e$时，熵单位为比特bit/纳特nat</li>
<li>熵只和$X$的分布有关，与取值$x_i$无关</li>
<li>熵越大，随机变量的不确定性就越大</li>
<li>$0 \leqslant H(p) \leqslant \log n$</li>
</ul>
<h4 id="2-1-2-条件熵-Conditional-Entropy"><a href="#2-1-2-条件熵-Conditional-Entropy" class="headerlink" title="2.1.2 条件熵 Conditional Entropy"></a>2.1.2 条件熵 Conditional Entropy</h4><blockquote>
<p>定义 5.02（条件熵）        对随机变量(X,Y)，联合概率分布为<br>$$<br>P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, n ; \quad j=1,2, \cdots, m\nonumber<br>$$<br>条件熵$H(Y|X)$表示一直随机变量$X$的情况下随机变量$Y$的不确定性。<strong>定义为给定$X$后$Y$的条件概率分布的熵对$X$的数学期望</strong>，即<br>$$<br>H(Y \mid X)=\sum_{i=1}^{n} p_{i} H\left(Y \mid X=x_{i}\right)\<br>p_{i}=P\left(X=x_{i}\right), i=1,2, \cdots, n<br>$$</p>
</blockquote>
<p>当熵和条件熵的概率有极大似然估计得到时，对应的为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy)</p>
<h4 id="2-1-3-信息增益"><a href="#2-1-3-信息增益" class="headerlink" title="2.1.3 信息增益"></a>2.1.3 信息增益</h4><p><strong>信息增益表示得知特征$X$的信息从而使得类$Y$的信息的不确定性减少程度。</strong></p>
<blockquote>
<p>定义 5.2 (信息增益) $\quad$ 特征 $A$ 对训练数据集$D$的信息增益 $g(D, A),$ 定义为集合$D$的经验熵$H(D)$ 与特征 $A$ 给定条件下$D$的经验条件熵 $H(D \mid A)$ 之差，即<br>$$<br>g(D, A)=H(D)-H(D \mid A)\nonumber<br>$$</p>
</blockquote>
<ul>
<li>熵和条件熵之差称为互信息 mutual information，在决策树中即为信息增益</li>
<li>$D$一般为标签</li>
</ul>
<h4 id="2-1-4-信息增益比-information-gain-ratio"><a href="#2-1-4-信息增益比-information-gain-ratio" class="headerlink" title="2.1.4 信息增益比 information gain ratio"></a>2.1.4 信息增益比 information gain ratio</h4><p>在训练集里，某一个特征较多时，信息增益会偏大。因此采用信息增益比来校正。</p>
<blockquote>
<p>定义 5.3 $ (信息增益比) $$\quad$ 特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_{R}(D, A)$ 定义为其信息增益 $g(D, A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_{A}(D)$ 之比，即<br>$$<br>g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}<br>$$<br>其中 $, H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log <em>{2} \frac{\left|D</em>{i}\right|}{|D|}, n$ 是特征 $A$ 取值的个数。</p>
</blockquote>
<h4 id="2-1-5-特征增益的算法"><a href="#2-1-5-特征增益的算法" class="headerlink" title="2.1.5 特征增益的算法"></a>2.1.5 特征增益的算法</h4><p>对数据集$D$，有$k$个类$C_k$，对特征$A$有$n$个取值$D_n$，$D_{i k}=D_{i} \cap C_{k}$，$|D|$为数据集中的样本个数</p>
<blockquote>
<p>算法 5.1 (信息增益的算法)<br>输入: 训练数据集 $D$ 和特征 $A$;<br>输出: 特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$ 。<br>(1) 计算数据集 $D$ 的经验熵$H(D)$<br>$$<br>H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log <em>{2} \frac{\left|C</em>{k}\right|}{|D|}\nonumber<br>$$<br>(2) 计算特征 $A$ 对数据集 $D$ 的经验条件熵 $H(D \mid A)$<br>$$<br>H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log <em>{2} \frac{\left|D</em>{i k}\right|}{\left|D_{i}\right|}\nonumber<br>$$<br>(3) 计算信息增益<br>$$<br>g(D, A)=H(D)-H(D \mid A)\nonumber<br>$$</p>
</blockquote>
<ul>
<li>没有特征B！A是特征的符号</li>
</ul>
<h2 id="3-ID3-Algorithm"><a href="#3-ID3-Algorithm" class="headerlink" title="3. ID3 Algorithm"></a>3. ID3 Algorithm</h2><p>决策树各个节点熵应用信息增益准则选择特征，递归构建决策树。</p>
<p>从根结点开始，计算所有可能的特征，选取信息增益最大的特征作为节点特征。并由该特征的不同取值点建立子节点。递归调用。</p>
<blockquote>
<p>算法 5.2 (ID3 算法)<br>输入: 训练数据集 $D,$ 特征集 $A$ 间值 $\varepsilon ;$$\$</p>
<p>输出：决策树 $T$ 。</p>
<p>(1) 若 $D$ 中所有实例属于同一类 $C_{k},$ 则 $T$ 为单结点树，并将类 $C_{k}$ 作为该结点 的类标记，返回 $T$;<br>(2) 若 $A=\varnothing,$ 则 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_{k}$ 作为该结点的类标记，返回 $T$ :<br>(3) 否则，按算法 5.1 计算 $A$ 中各特征对 $D$ 的信息增益，选择信息增益最大的特征 $A_{g}$ :</p>
<p>(4) 如果 $A_{g}$ 的信息增益小于间值 $\varepsilon,$ 则置 $T$ 为单结点树，并将 $D$ 中实例数最大 的类 $C_{k}$ 作为该结点的类标记，返回 $T$ :<br>(5) 否则，对 $A_{g}$ 的每一可能值 $a_{i},$ 依 $A_{g}=a_{i}$ 将 $D$ 分割为若干非空子集 $D_{i},$ 将 $D_{i}$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T,$ 返回 $T$;<br>(6) 对第 $i$ 个子结点，以 $D_{i}$ 为训练集，以 $A-\left{A_{g}\right}$ 为特征集，逆归地调用步 (1)$\sim$ 步 $(5),$ 得到子树 $T_{i},$ 返回 $T_{i} \circ$</p>
</blockquote>
<ul>
<li>极大似然法进行概率估计？</li>
<li>只有树的生成，没有剪枝，容易过拟合</li>
</ul>
<h2 id="4-C4-5-Algorithm"><a href="#4-C4-5-Algorithm" class="headerlink" title="4. C4.5 Algorithm"></a>4. C4.5 Algorithm</h2><p>ID3算法的改进，用信息增益比来选择特征</p>
<blockquote>
<p>算法 5.3 (C4.5 的生成算法)</p>
<p>输入: 训练数据集 $D$, 特征集 $A$ 间值 $\varepsilon$; </p>
<p>输出：决策树 $T$ 。<br>(1) 如果 $D$ 中所有实例属于同一类 $C_{k},$ 则置 $T$ 为单结点树，并将 $C_{k}$ 作为该结 点的类, 返回 $T$;<br>(2) 如果 $A=\varnothing,$ 则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_{k}$ 作为该结点 的类, 返回 $T$;<br>(3) 否则，按式 (5.10) 计算 $A$ 中各特征对 $D$ 的**<em>信息增益比**</em>, 选择信息增益比最大 的特征 $A_{g}$;<br>(4) 如果 $A_{g}$ 的信息增益比小于间值 $\varepsilon,$ 则置 $T$ 为单结点树，并将 $D$ 中实例数最 大的类 $C_{k}$ 作为该结点的类, 返回 $T$;<br>(5) 否则, 对 $A_{g}$ 的每一可能值 $a_{i},$ 依 $A_{g}=a_{i}$ 将 $D$ 分割为子集若干非空 $D_{i},$ 将 $D_{i}$ 中实例数最大的类作为标记，构建子结点, 由结点及其子结点构成树 $T,$ 返回 $T$;<br>(6) 对结点 $i$, 以 $D_{i}$ 为训练集，以 $A-\left{A_{g}\right}$ 为特征集，递归地调用步 (1) 步 $(5),$ 得到子树 $T_{i},$ 返回 $T_{i}$ 。</p>
</blockquote>
<h2 id="5-Pruning"><a href="#5-Pruning" class="headerlink" title="5. Pruning"></a>5. Pruning</h2><p>考虑树的复杂度，对生成的决策树进行剪枝，减掉子树或叶结点</p>
<p><strong>策略：极小化决策树的损失函数</strong>，即正则化的极大似然估计</p>
<p>设树$T$，叶节点个数$|T|$，树$T$的叶结点$t$，此叶结点的样本点个数$N_t$；此叶结点有$K$类样本点的个数$N_{tk}；;k=1,2,…,K$；此叶结点的经验熵$H_t(T)$，函数参数$\alpha \geq0$。此决策树的损失函数定义为<br>$$<br>C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|<br>$$<br>经验熵$H_t(T)$为<br>$$<br>H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}<br>$$<br>记(6)的左边为$C(T)$<br>$$<br>C(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)=-\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{t k} \log \frac{N_{t k}}{N_{t}}<br>$$<br>最后得到<br>$$<br>C_{\alpha}(T)=C(T)+\alpha|T|<br>$$</p>
<ul>
<li>$C(T)$为样本点个数与经验熵的积，表示模型对训练数据的预测误差</li>
<li>$|T|$为模型复杂度<ul>
<li>越大说明叶结点越多，树越复杂</li>
<li>$\alpha$越大，选择较简单的模型树</li>
</ul>
</li>
</ul>
<h3 id="5-1-Pruning-algorithm"><a href="#5-1-Pruning-algorithm" class="headerlink" title="5.1 Pruning algorithm"></a>5.1 Pruning algorithm</h3><blockquote>
<p>算法 5.4 (树的剪枝算法)<br>输入: 生成算法产生的整个树 $T$, 参数 $\alpha$;<br>输出：修剪后的子树 $T_{\alpha} \circ$<br>（1）计算每个结点的经验熵。<br>（2）递归地从树的叶结点向上回缩。</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D.png" alt="image-20201205222057313"></p>
<p>​    设一组叶结点回缩到其父结点之前与之后的整体树分别为 $T_{B}$ 与 $T_{A}$, 其对应的 损失函数值分别是 $C_{\alpha}\left(T_{B}\right)$ 与 $C_{\alpha}\left(T_{A}\right),$ 如果<br>$$<br>C_{\alpha}\left(T_{A}\right) \leqslant C_{\alpha}\left(T_{B}\right)<br>$$<br>则进行剪枝，即将父结点变为新的叶结点。<br>(3) 返回 ( 2 )，直至不能继续为止，得到损失函数最小的子树 $T_{\alpha^{\circ}}$</p>
</blockquote>
<ul>
<li>动态规划的算法实现</li>
</ul>
<h2 id="6-CART-Algorithm"><a href="#6-CART-Algorithm" class="headerlink" title="6. CART Algorithm"></a>6. CART Algorithm</h2><p>分类与回归树(classification and regression tree)是给定随机变量$X$的条件下给出随机变量$Y$的条件概率分布的学习方法。决策树是二叉树，左分支是“是”分支，右否。<strong>通过递归的二分每个特征，使得特征空间划分成有限个单元</strong>，在这些单元上确定概率分布。</p>
<h3 id="6-1-CART的生成"><a href="#6-1-CART的生成" class="headerlink" title="6.1 CART的生成"></a>6.1 CART的生成</h3><h4 id="6-1-1-回归树生成"><a href="#6-1-1-回归树生成" class="headerlink" title="6.1.1 回归树生成"></a>6.1.1 回归树生成</h4><h5 id="6-1-1-1-Model"><a href="#6-1-1-1-Model" class="headerlink" title="6.1.1.1  Model"></a>6.1.1.1  Model</h5><p>用<strong>平方误差最小化准则</strong>，进行特征选择，递归的构建二叉决策树。</p>
<p>对数据集$D=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right}$，$Y$是连续变量，生成回归树。</p>
<p>回归树对应的是<strong>特征空间的划分</strong>以及<strong>划分单元上的输出值</strong>。</p>
<blockquote>
<p>定义 5.03 （回归树模型）</p>
<p>假设已将输入空间划分为 $M$ 个单元 $R_{1}, R_{2}, \cdots, R_{M},$ 并且在每个单元 $R_{m}$ 上 有一个固定的输出值 $c_{m},$ 于是回归树模型可表示为<br>$$<br>f(x)=\sum_{m=1}^{M} c_{m} I\left(x \in R_{m}\right)<br>$$</p>
</blockquote>
<ul>
<li><p>$x$是特征向量</p>
</li>
<li><p>当输入空间划分确定，利用平方误差最小准则可以确定$c_m$的最优值</p>
<ul>
<li><p>平方误差定义为$\sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}$。注意$x_i\in R_m$</p>
</li>
<li><p>利用平方误差最小原则，单元 $R_{m}$ 上的 $c_{m}$ 的最优值 $\hat{c}<em>{m}$ 是 $R</em>{m}$ 上的所有输入实例 $x_{i}$ 对应的输出 $y_{i}$ 的均值，即<br>$$<br>\hat{c}<em>{m}=\operatorname{ave}\left(y</em>{i} \mid x_{i} \in R_{m}\right)\nonumber<br>$$</p>
</li>
</ul>
</li>
</ul>
<p><strong>问题1：如何划分特征空间，即选择划分点</strong></p>
<p>通过启发式算法，选择第 $j$ 个变量 $x^{(j)}$和它取的特征值 $s$, 作为切分变量（splitting variable）和切分点 (splitting point)，并定义两个区域<br>$$<br>R_{1}(j, s)=\left{x \mid x^{(j)} \leqslant s\right} \quad \text { 和 } \quad R_{2}(j, s)=\left{x \mid x^{(j)}&gt;s\right}<br>$$<br>在一开始的选择基础上，寻找最优切分变量$j$和最优切分点$s$。具体的，求解<br>$$<br>\min <em>{j, s}\left[\min <em>{c</em>{1}} \sum</em>{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min <em>{c</em>{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]<br>$$</p>
<ul>
<li>(11)中，$x^{(j)}$是第$j$个特征，$s$是此特征维度上的值。$x_n$是第$n$个输入向量。<a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/93667566">ref</a></li>
<li>(12)中，括号里的对于$c_1,c_2$取极小值是已知的。</li>
<li>划分特征空间时是把高维矩形划分为2个子高维矩形。</li>
</ul>
<p><strong>问题2：如何确定好输出值$c_m$</strong></p>
<p>特征空间切分好后，根据最小化平方误差准则，得到相应的最优输出值<br>$$<br>\hat{c}<em>{1}=\operatorname{ave}\left(y</em>{i} \mid x_{i} \in R_{1}(j, s)\right) \quad \text { 和 } \quad \hat{c}<em>{2}=\operatorname{ave}\left(y</em>{i} \mid x_{i} \in R_{2}(j, s)\right)<br>$$<br><strong>总结：</strong>一开始，我们遍历所有输入特征，找到最优的切分特征变量$j$，构成一对$(j,s)$，根据这个构成的超平面，讲特征空间划分成2个区域。对每个子区域重复过程，直到满足停止条件。这种回归树称为最小二乘回归树(least squares regression tree)</p>
<h5 id="6-1-1-2-Algorithm"><a href="#6-1-1-2-Algorithm" class="headerlink" title="6.1.1.2 Algorithm"></a>6.1.1.2 Algorithm</h5><blockquote>
<p>算法 5.5 (最小二乘回归树生成算法)<br>输入: 训练数据集 $D$;<br>输出：回归树 $f(x)$ 。<br>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树:<br>（1）选择最优切分变量 $j$ 与切分点 $s$, 求解<br>$$<br>\min <em>{j, s}\left[\min <em>{c</em>{1}} \sum</em>{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min <em>{c</em>{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]<br>$$<br>遍历变量 $j$, 对固定的切分变量 $j$ 扫描切分点 $s$, 选择使式 (14) 达到最小值的对$(j, s)$<br>（2）用选定的对 $(j, s)$ 划分区域并决定相应的输出值:<br>$$<br>\begin{array}{c}<br>R_{1}(j, s)=\left{x \mid x^{(j)} \leqslant s\right}, \quad R_{2}(j, s)=\left{x \mid x^{(j)}&gt;s\right} \<br>\hat{c}<em>{m}=\frac{1}{N</em>{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, \quad x \in R_{m}, \quad m=1,2<br>\end{array}<br>$$<br>（3）继续对两个子区域调用步骤 $(1),(2),$ 直至满足停止条件。<br>（4）将输入空间划分为 $M$ 个区域 $R_{1}, R_{2}, \cdots, R_{M},$ 生成决策树:<br>$$<br>f(x)=\sum_{m=1}^{M} \hat{c}<em>{m} I\left(x \in R</em>{m}\right)<br>$$</p>
</blockquote>
<h5 id="6-1-1-3-Example"><a href="#6-1-1-3-Example" class="headerlink" title="6.1.1.3 Example"></a>6.1.1.3 Example</h5><table>
<thead>
<tr>
<th align="center"></th>
<th align="center">$x^1$</th>
<th align="center">$x^2$</th>
<th align="center">$x^3$</th>
<th align="center">$y$</th>
</tr>
</thead>
<tbody><tr>
<td align="center">$x_1$</td>
<td align="center">1.2</td>
<td align="center">3</td>
<td align="center">2.5</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">$x_2$</td>
<td align="center">1.5</td>
<td align="center">4</td>
<td align="center">3.5</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">$x_3$</td>
<td align="center">1.6.</td>
<td align="center">6</td>
<td align="center">2.75</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">$x_4$</td>
<td align="center">1.8</td>
<td align="center">9</td>
<td align="center">2.25</td>
<td align="center">3</td>
</tr>
</tbody></table>
<p><strong>Q：</strong>对以上数据集基于最小化平方误差生成二叉回归树</p>
<ol>
<li><p>设$j=x^1$，$s=1.5$时</p>
<p>$c_1=1,;c_2=2.5$</p>
<p>$\min <em>{c</em>{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min <em>{c</em>{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}=\ [(1-1)^2+(1-1)^2]+[(2-2.5)^2+(3-2.5)^2]=0.5$</p>
<p>$(j,s)=(x^1,1.6),;0.67$</p>
<p>$(j,s)=(x^1,1.2),;(j,s)=(x^1,1.8)$的结果一定偏大</p>
<p>对固定$j=x^1$，$s=1.5$是最佳切分点，$error=0.5$</p>
</li>
<li><p>设$j=x^2$</p>
<p>最佳切分为$(j,s)=(x^2,4),;error=0.5$</p>
</li>
<li><p>设$j=x^3$</p>
<p>最佳切分为$(j,s)=(x^3,2.75),;error=2$</p>
</li>
<li><p>$\min _{j, s}=\min[0.5,;0.5,;2]=0.5$，选择$(j,s)=(x^1,1.5)$作为最优划分。划分后的子集$R_1,;R_2$为</p>
<table>
<thead>
<tr>
<th align="center"><strong>$R_1$左分支</strong></th>
<th align="center">$x^1$</th>
<th align="center">$x^2$</th>
<th align="center">$x^3$</th>
<th align="center">$y$</th>
</tr>
</thead>
<tbody><tr>
<td align="center">$x_1$</td>
<td align="center">1.2</td>
<td align="center">3</td>
<td align="center">2.5</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">$x_2$</td>
<td align="center">1.5</td>
<td align="center">4</td>
<td align="center">3.5</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center"><strong>$R_2$右分支</strong></td>
<td align="center">$x^1$</td>
<td align="center">$x^2$</td>
<td align="center">$x^3$</td>
<td align="center">$y$</td>
</tr>
<tr>
<td align="center">$x_3$</td>
<td align="center">1.6.</td>
<td align="center">6</td>
<td align="center">2.75</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">$x_4$</td>
<td align="center">1.8</td>
<td align="center">9</td>
<td align="center">2.25</td>
<td align="center">3</td>
</tr>
</tbody></table>
<p>$\hat{c}_1=1,;\hat{c}_2=2.5$</p>
</li>
<li><p>对左右分支继续迭代1-4的步骤，直到满足停止条件</p>
</li>
</ol>
<h4 id="6-1-2-分类树的生成"><a href="#6-1-2-分类树的生成" class="headerlink" title="6.1.2 分类树的生成"></a>6.1.2 分类树的生成</h4><p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<h5 id="6-1-2-1-Gini-Index"><a href="#6-1-2-1-Gini-Index" class="headerlink" title="6.1.2.1 Gini Index"></a>6.1.2.1 Gini Index</h5><blockquote>
<p>定义 5.4 (基尼指数) $\quad$ 分类问题中，假设有 $K$ 个类，样本点属于第 $k$ 类的概率<br>为 $p_{k}$, 则棍率分布的基尼指数定义为<br>$$<br>\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}<br>$$<br>对于二类分类问题, 若样本点属于第 1 个类的概率是 $p,$ 则概率分布的基尼指数为<br>$$<br>\operatorname{Gini}(p)=2 p(1-p)<br>$$<br>对于给定的样本集合 $D,$ 其基尼指数为<br>$$<br>\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}<br>$$<br>这里, $C_{k}$ 是 $D$ 中属于第 $k$ 类的样本子集， $K$ 是类的个数。</p>
<p>如果样本集合 $D$ 根据<strong>特征 $A$</strong> 是否取某一可能值 $a$ 被分割成 $D_{1}$ 和 $D_{2}$ 两部分，即<br>$$<br>D_{1}={(x, y) \in D \mid A(x)=a}, \quad D_{2}=D-D_{1}\nonumber<br>$$<br>则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为<br>$$<br>\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)<br>$$</p>
</blockquote>
<ul>
<li>基尼指数表示集合的不确定性。基尼指数越大，集合的不确定性越大。和熵相似。</li>
<li>二元分类中基尼指数、单位比特熵和分类误差的关系。x轴：概率p。y轴：损失。<img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-5%20%E5%9F%BA%E5%B0%BC%E3%80%81%E7%86%B5%E4%B8%8E%E5%88%86%E7%B1%BB%E8%AF%AF%E5%B7%AE.png" alt="image-20201208142218988" style="zoom:50%;" /></li>
</ul>
<h5 id="6-1-2-2-Algorithm"><a href="#6-1-2-2-Algorithm" class="headerlink" title="6.1.2.2 Algorithm"></a>6.1.2.2 Algorithm</h5><blockquote>
<p>算法 5.6 (CART 生成算法)</p>
<p>输入: 训练数据集 $D,$ 停止计算的条件;<br>输出: CART 决策树。 </p>
<p>根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树:<br>（1） 设结点的训练数据集为 $D$, 计算现有特征对该数据集的基尼指数。此时，对每一个特征 $A,$ 对其可能取的每个值 $a,$ 根据样本点对 $A=a$ 的测试为“是”或“否”，将$D$ 分割成 $D_{1}$ 和 $D_{2}$ 两部分，利用式 (20) 计算 $A=a$ 时的基尼指数。<br>（2）在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择基尼指数最小的<br>持征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现<br>结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。<br>（3）对两个子结点递归地调用 ( 1 )， (2) , 直至满足停止条件。<br>（4）生成 CART 决策树。</p>
</blockquote>
<h3 id="6-2-CART的剪枝"><a href="#6-2-CART的剪枝" class="headerlink" title="6.2 CART的剪枝"></a>6.2 CART的剪枝</h3><p><strong>步骤：</strong></p>
<ol>
<li>对生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成子树序列$\left{T_{0}, T_{1}, \cdots, T_{n}\right}$</li>
<li>通过交叉验证法，在独立的验证数据集上对字数序列进行测试，选择最优子书。</li>
</ol>
<h4 id="6-2-1-剪枝成一个子树序列"><a href="#6-2-1-剪枝成一个子树序列" class="headerlink" title="6.2.1 剪枝成一个子树序列"></a>6.2.1 剪枝成一个子树序列</h4><p>子树的损失函数为<br>$$<br>C_{\alpha}(T)=C(T)+\alpha|T|\nonumber<br>$$</p>
<ul>
<li><p>$C(T)=\sum_{t=1}^{|T|} N_{t}\left(1-\sum_{k=1}^{K}\left(\frac{N_{t k}}{N_{t}}\right)^{2}\right),|T|$ 是叶结点个数，$K$ 是类别个数</p>
</li>
<li><p>定义推导同(5)-(8)</p>
</li>
<li><p>对固定$\alpha$，<strong>唯一存在</strong>最优子树$T_{\alpha}$，使得损失函数$C_{\alpha}(T)$最小。</p>
<ul>
<li>此处“最优”的意义是指使得损失函数最小</li>
<li>$\alpha$越大，最优子树$T_{\alpha}$越小。当 $\alpha \rightarrow \infty$ 时，叶结点不断被剪，根结点组成的单结点树是最优的。</li>
</ul>
</li>
</ul>
<p>对一个整体树$T_0$，它的子树是<strong>有限个</strong>的。因此，对一个连续参数$\alpha$，我们得到了最优子树$T’(\alpha)$。$\alpha$不断增大，在增大到**跳跃点$\alpha’$**之前，$T’(\alpha)$依然是最优子树。即$T’(\alpha)=T’(\alpha+\Delta\alpha)$。再跳跃点之后，易知最优子树$T’(\alpha’)\in T’(\alpha)$。</p>
<p>上面可以表述为：将 $\alpha$ 从小增大, $0=\alpha_{0}&lt;$$\alpha_{1}&lt;\cdots&lt;\alpha_{n}&lt;+\infty,$ 产生一系列的区间 $\left[\alpha_{i}, \alpha_{i+1}\right), i=0,1, \cdots, n ;$ 剪枝得到的子树序列对应着区间 $\alpha \in\left[\alpha_{i}, \alpha_{i+1}\right), i=0,1, \cdots, n$ 的最优子树序列 $\left{T_{0}, T_{1}, \cdots, T_{n}\right},$ <strong>序列中的子树是嵌套的</strong>。</p>
<p>具体来说，从整体树$T_0$，$\alpha=0$开始剪枝。对$T_0$内的任意内部结点$t$：</p>
<p>以$t$为单结点树的损失函数为<br>$$<br>C_{\alpha}(t)=C(t)+\alpha<br>$$<br>以 $t$ 为根结点的子树 $T_{t}$ 的损失函数为<br>$$<br>C_{\alpha}\left(T_{t}\right)=C\left(T_{t}\right)+\alpha\left|T_{t}\right|<br>$$<br>当 $\alpha=0$ 及 $\alpha$ 充分小时，有不等式<br>$$<br>C_{\alpha}\left(T_{t}\right)&lt;C_{\alpha}(t)<br>$$<br>当 $\alpha$ 增大时，在某一 $\alpha$ 有<br>$$<br>\begin{align}<br>C_{\alpha}\left(T_{t}\right) &amp;=C_{\alpha}(t)\<br>\alpha &amp;=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}<br>\end{align}<br>$$</p>
<p>**此时$T_t$和$t$有相同的损失函数值，因为$t$的结点更少，所以取$t$，剪去以 $t$ 为根结点的子树 $T_{t}$ **</p>
<p>根据这个性质，我们可以找到系列区间以及对应的最优子树序列</p>
<p>对$T_0$的每一个内部节点$t$，计算<br>$$<br>g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}<br>$$<br>$g(t)$表示剪枝后整体损失函数减小的程度。在 $T_{0}$ 中剪去 $g(t)$ 最小的 $T_{t},$ 将得到的子树作为 $T_{1},$ 同时将最小的 $g(t)$ 设为 $\alpha_{1}$。$ T_{1}$ 为区间 $\left[\alpha_{1}, \alpha_{2}\right)$ 的最优子树。循环直到得到根结点。</p>
<ul>
<li>在这个过程中，$g(t)=\alpha$是不断增大的？</li>
</ul>
<h4 id="6-2-2-交叉验证"><a href="#6-2-2-交叉验证" class="headerlink" title="6.2.2 交叉验证"></a>6.2.2 交叉验证</h4><p>利用独立的验证数据集，测试子树序列 $\left{T_{0}, T_{1}, \cdots, T_{n}\right}$中每个子树的平方误差/基尼指数，选择最优决策树$T_{\alpha}$。</p>
<ul>
<li>子树序列 $\left{T_{0}, T_{1}, \cdots, T_{n}\right}$在剪枝的时候是对应$\alpha$的最优子序列</li>
</ul>
<h4 id="6-2-3-Algorithm"><a href="#6-2-3-Algorithm" class="headerlink" title="6.2.3 Algorithm"></a>6.2.3 Algorithm</h4><blockquote>
<p>算法 5.7 (CART 剪枝算法)<br>输入: CART 算法生成的决策树 $T_{0}$;<br>输出：最优决策树 $T_{\alpha \circ}$<br>(1) 设 $k=0, T=T_{0}$ 。<br>(2) 设 $\alpha=+\infty$ 。<br>(3) 自下而上地对各内部结点 $t$ 计算 $C\left(T_{t}\right),\left|T_{t}\right|$ 以及<br>$$<br>\begin{aligned}</p>
<p>g(t) &amp;=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1} \<br>\alpha &amp;=\min (\alpha, g(t))<br>\end{aligned}<br>$$<br>这里, $T_{t}$ 表示以 $t$ 为根结点的子树, $C\left(T_{t}\right)$ 是对训练数据的预测误差, $\left|T_{t}\right|$ 是 $T_{t}$的叶结点个数。<br>(4) 对 $g(t)=\alpha$ 的内部结点 $t$ 进行剪枝，并对叶结点 $t$ 以多数表决法决定其类，得到树 $T$ 。<br>(5) 设 $k=k+1, \alpha_{k}=\alpha, T_{k}=T_{\circ}$<br>(6) 如果 $T_{k}$ 不是由根结点及两个叶结点构成的树，则回到步骤 (2)$;$ 否则令 $T_{k}=T_{n}$<br>(7) 采用交义验证法在子树序列 $T_{0}, T_{1}, \cdots, T_{n}$ 中选取最优子树 $T_{\alpha^{\circ}}$</p>
</blockquote>
<h2 id="7-Question"><a href="#7-Question" class="headerlink" title="7. Question"></a>7. Question</h2><ol>
<li>为什么$C(T)$能表示模型对训练数据的预测误差</li>
<li><del>正则化的极大似然估计？</del></li>
<li><del><strong>1.2</strong>中的’构成一个条件概率分布’，不是叶结点咋办？</del></li>
<li><del>6.1.1 启发式算法？</del></li>
<li>CART决策时有没有可能对一个特征二叉再接个二叉，成$2^2$个叉？</li>
</ol>
<h2 id="8-Code"><a href="#8-Code" class="headerlink" title="8. Code"></a>8. Code</h2><h3 id="8-1-ID3-create-test-visualization"><a href="#8-1-ID3-create-test-visualization" class="headerlink" title="8.1 ID3: create, test, visualization"></a>8.1 ID3: create, test, visualization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">CARTClassification</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">CARTRegression</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ID3_create</span>(<span class="params">self, train_set, features, labels, tol=[<span class="number">0.1</span>, <span class="number">2</span>], visible=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        create ID3 tree</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray. m: samples, n: features</span></span><br><span class="line"><span class="string">        :param features: n size vector</span></span><br><span class="line"><span class="string">        :param labels: m size ndarray</span></span><br><span class="line"><span class="string">        :param tol: tolerate for pre-pruning</span></span><br><span class="line"><span class="string">        :return: ID3 tree in dict type</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        three conditions to stop iteration:</span></span><br><span class="line"><span class="string">          1. all labelss are the same</span></span><br><span class="line"><span class="string">          2. no feature</span></span><br><span class="line"><span class="string">          3. info_gain &lt; tol[0], samples &gt; tol[1]. pre-pruning</span></span><br><span class="line"><span class="string">          4. same training values but different labels</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        train_set = np.array(train_set)</span><br><span class="line">        labels = np.array(labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(np.unique(labels)) == <span class="number">1</span>:  <span class="comment"># condition 1</span></span><br><span class="line">            <span class="keyword">return</span> labels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> train_set.shape[<span class="number">1</span>] == <span class="number">0</span>:  <span class="comment"># condition 2</span></span><br><span class="line">            <span class="keyword">return</span> np.sort(labels)[-<span class="number">1</span>]  <span class="comment"># return the most frequency value</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># condition 3 &amp; 4</span></span><br><span class="line">        <span class="comment"># not finished</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get best feature</span></span><br><span class="line">        best_feature_index, best_info_gain = self.ID3_best_feature(train_set, labels)</span><br><span class="line">        best_feature_name = features[best_feature_index]</span><br><span class="line">        print(<span class="string">&#x27;best selected feature is &#x27;</span>, best_feature_name, <span class="string">&#x27;, its information gain is &#x27;</span>, best_info_gain)</span><br><span class="line"></span><br><span class="line">        ID3Tree = &#123;best_feature_name: &#123;&#125;&#125;  <span class="comment"># return feature name as a dict key</span></span><br><span class="line">        <span class="comment"># return unique values under the feature and as the node(key)</span></span><br><span class="line">        tree_nodes = np.unique(train_set.T[best_feature_index])</span><br><span class="line">        <span class="comment"># small feature set for dealing feature set depending on its index</span></span><br><span class="line">        features = np.delete(features, best_feature_index)</span><br><span class="line">        <span class="comment"># iteration in these nodes</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> tree_nodes:</span><br><span class="line">            train_sample_index = train_set.T[best_feature_index] == node</span><br><span class="line">            node_labels = labels[train_sample_index]</span><br><span class="line">            <span class="comment"># small train set with node feature&#x27;s column equal node value</span></span><br><span class="line">            node_train_set = self.spilt_dataset(train_set, best_feature_index, node)</span><br><span class="line">            <span class="comment"># small train set without node feature</span></span><br><span class="line">            node_train_set = np.delete(node_train_set, best_feature_index, axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># iteration</span></span><br><span class="line">            ID3Tree[best_feature_name][node] = self.ID3_create(node_train_set, features, node_labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> visible <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            treePlotter.ID3_Tree(ID3Tree)</span><br><span class="line">        <span class="keyword">return</span> ID3Tree</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">entropy</span>(<span class="params">array</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        calculate bit entropy</span></span><br><span class="line"><span class="string">        :param array: 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: entropy in bit</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        count_array = np.unique(array, return_counts=<span class="literal">True</span>)[<span class="number">1</span>]  <span class="comment"># unique values and its occurrences</span></span><br><span class="line">        probability = count_array / array.size  <span class="comment"># probability of values</span></span><br><span class="line">        h_p = np.dot(-probability, np.log2(probability))  <span class="comment"># entropy</span></span><br><span class="line">        <span class="keyword">return</span> h_p</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conditional_entropy</span>(<span class="params">Y, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        get conditional entropy H(Y|X)</span></span><br><span class="line"><span class="string">        :param Y: random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :param X: given random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: conditional entropy of Y given X, H(Y|X)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        Y = np.array(Y)</span><br><span class="line">        X = np.array(X)</span><br><span class="line">        hY_X = <span class="number">0</span>  <span class="comment"># initialization</span></span><br><span class="line">        X_value, X_count = np.unique(X, return_counts=<span class="literal">True</span>)  <span class="comment"># unique values and its occurrences</span></span><br><span class="line">        <span class="keyword">for</span> xi <span class="keyword">in</span> X_value:</span><br><span class="line">            index = np.argwhere(X == xi)  <span class="comment"># get index of X=xi</span></span><br><span class="line">            p_xi = index.size / X.size  <span class="comment"># P(X=xi)</span></span><br><span class="line">            Yi = Y[index]  <span class="comment"># get yi given xi</span></span><br><span class="line">            hYi_xi = DecisionTree.entropy(np.array(Yi))  <span class="comment"># H(Y|X=xi)</span></span><br><span class="line">            hY_X += p_xi * hYi_xi</span><br><span class="line">        <span class="keyword">return</span> hY_X</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain</span>(<span class="params">Y, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        get information gain G(Y,X)</span></span><br><span class="line"><span class="string">        :param Y: random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :param X: given random variable, 1-D numpy array</span></span><br><span class="line"><span class="string">        :return: information gain of Y given X, G(Y|X)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> DecisionTree.entropy(Y) - DecisionTree.conditional_entropy(Y, X)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spilt_dataset</span>(<span class="params">dataset, colume, value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dataset with small samples</span></span><br><span class="line"><span class="string">        :param dataset: m*n ndarray</span></span><br><span class="line"><span class="string">        :param colume:  axis</span></span><br><span class="line"><span class="string">        :param value: compared value</span></span><br><span class="line"><span class="string">        :return: l*n ndarray, l&lt;m</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dataset = pd.DataFrame(dataset)</span><br><span class="line">        df = dataset[dataset[colume] == value]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.array(df)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ID3_best_feature</span>(<span class="params">train_set, labels</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        return the feature with the highest infomation gain</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray. m: samples, n: features</span></span><br><span class="line"><span class="string">        :param labels: m size ndarray.</span></span><br><span class="line"><span class="string">        :return: best feature index and its infomation gain</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        features = train_set.shape[<span class="number">1</span>]  <span class="comment"># number of features</span></span><br><span class="line">        tmp = np.ones(features) * -<span class="number">1</span>  <span class="comment"># store info gain</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(features):  <span class="comment"># calculate info gain of each features</span></span><br><span class="line">            feature_list = train_set.T[i]</span><br><span class="line">            gain = DecisionTree.info_gain(labels, feature_list)</span><br><span class="line">            tmp[i] = gain</span><br><span class="line">            print(<span class="string">&quot;the info gain of %d th feature in ID3 is: %.3f&quot;</span> % (i, gain))</span><br><span class="line">        best_feature = np.argmax(tmp)</span><br><span class="line">        best_info_gain = tmp[best_feature]</span><br><span class="line">        <span class="keyword">return</span> best_feature, best_info_gain</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">tree, sample, features</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param tree: dict</span></span><br><span class="line"><span class="string">        :param sample: 1-d ndarray</span></span><br><span class="line"><span class="string">        :param features: 1-d ndarray</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        first_str = <span class="built_in">list</span>(tree.keys())[<span class="number">0</span>] <span class="comment"># root name</span></span><br><span class="line">        small_tree = tree[first_str] <span class="comment">#</span></span><br><span class="line">        feature_index = features.index(first_str)</span><br><span class="line">        label = <span class="string">&#x27;None&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> small_tree.keys():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(sample[feature_index]) == key:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">type</span>(small_tree[key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:</span><br><span class="line">                    label = DecisionTree.classify(small_tree[key], sample, features)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    label = small_tree[key]</span><br><span class="line">        <span class="keyword">return</span> label</span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<h2 id="9-Reference"><a href="#9-Reference" class="headerlink" title="9. Reference"></a>9. Reference</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/93667566">分类与回归树（classification and regression tree，CART）之回归</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/101721467">回归树</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012328159/article/details/93667566">分类与回归树（classification and regression tree，CART）之回归</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Erikfather/Decision_tree-python">Erikfather/Decision_tree-python</a></p>
<p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/12/02/[SLM-5]%20%E5%86%B3%E7%AD%96%E6%A0%91%20Decision%20Tree/" data-id="ckivv4ekt00048cp42jfj0vlr" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-[SLM-4] 朴素贝叶斯 Naive Bayes" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/11/27/%5BSLM-4%5D%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%20Naive%20Bayes/" class="article-date">
  <time class="dt-published" datetime="2020-11-26T16:11:26.197Z" itemprop="datePublished">2020-11-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="朴素贝叶斯-Naive-Bayes"><a href="#朴素贝叶斯-Naive-Bayes" class="headerlink" title="朴素贝叶斯 Naive Bayes"></a>朴素贝叶斯 Naive Bayes</h1><p>基于Bayes Theorem和特征条件独立假设的分类方法。</p>
<h2 id="0-Bayes-Theorem"><a href="#0-Bayes-Theorem" class="headerlink" title="0. Bayes Theorem"></a>0. Bayes Theorem</h2><p>$$<br>\underbrace{P(X|Y)}<em>{posterior}=\frac{\overbrace{P(Y|X)}^{likelihood}\overbrace{P(X)}^{prior}}{\underbrace{P(Y)}</em>{evidence}}=\frac{\overbrace{P(Y|X)}^{likelihood}\overbrace{P(X)}^{prior}}{\underbrace{\sum\limits_x P(Y|X)P(X)}_{evidence}}<br>$$</p>
<ul>
<li>$P(X)$：先验概率prior</li>
<li>$P(X|Y)$：后验概率posterior</li>
<li>$P(Y|X)$：似然likelihood。</li>
</ul>
<h2 id="1-Model"><a href="#1-Model" class="headerlink" title="1. Model"></a>1. Model</h2><ul>
<li><p>输入空间$\mathcal{X} \subseteq \mathbf{R}^{n}$ 为 $n$ 维向量的集合，输出空间为类集合$\mathcal{Y}=\left{c_{1}, c_{2}, \cdots, c_{K}\right}$</p>
</li>
<li><p>输入特征向量$x$，输出类标记$y$</p>
</li>
<li><p>$X$是输入空间上的随机向量，$Y$是输出空间上的随机变量</p>
</li>
<li><p>训练数据集<br>$$<br>\begin{aligned}<br>T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right}<br>\end{aligned}<br>$$<br>由 $P(X, Y)$ 独立同分布产生。</p>
<ul>
<li>这边的独立指的是向量$x_1$与$x_2$等之间的独立，不是特征条件独立。</li>
</ul>
</li>
</ul>
<h3 id="1-1-学习联合概率分布-P-X-Y"><a href="#1-1-学习联合概率分布-P-X-Y" class="headerlink" title="1.1 学习联合概率分布$P(X,Y)$"></a>1.1 学习联合概率分布$P(X,Y)$</h3><ol>
<li><p>$P(X, Y)=P(Y) P(X \mid Y)=P(X) P(Y \mid X)$，用第一个等式</p>
</li>
<li><p><strong>先验概率分布</strong><br>$$<br>P\left(Y=c_{k}\right), \quad k=1,2, \cdots, K<br>$$</p>
<ul>
<li>由测试集可知，有多少概率/比例的$c_1$标签</li>
</ul>
</li>
<li><p><strong>条件概率分布</strong><br>$$<br>P\left(X=x \mid Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right),\ \quad k=1,2, \cdots, K<br>$$</p>
<ul>
<li><p>特征向量相等$\Longrightarrow$每一维度上都相等</p>
</li>
<li><p>朴素贝叶斯法对条件概率做条件独立性假设，即<br>$$<br>\begin{align}<br>P\left(X=x \mid Y=c_{k}\right) &amp;=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right)\nonumber \<br>&amp;=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)<br>\end{align}<br>$$</p>
</li>
<li><p>如果不加独立分布假设，那么(1,1,1,1)和(1,1,1,2)独立，没有可以利用的信息。需要更大的测试集。</p>
</li>
</ul>
</li>
<li><p><strong>重复“词语”的处理</strong></p>
<p>我们得到一个垃圾邮件向量$x=[‘发票’，‘发票’，‘发票‘，‘充值’，’购物‘]$，如何处理出现3次的“发票”</p>
<ol>
<li>多项式模型<ul>
<li>每一个“发票”都是独立的，统计多次，即$P(X=(‘发票’，‘发票’，‘发票‘)|Y=c_k)=P^3(‘发票|Y=c_k’)$</li>
</ul>
</li>
<li>伯努利模型<ul>
<li>只分成出现，不出现的二元情况，即$P(X=(‘发票’，‘发票’，‘发票‘)|Y=c_k)=P(‘发票|Y=c_k’)$</li>
</ul>
</li>
<li>混合模型<ul>
<li>计算单一句子的概率时用伯努利模型</li>
<li>计算全文词语的概率时用多项式模型</li>
</ul>
</li>
<li>高斯模型<ul>
<li>适用于连续变量</li>
<li>假设：在给定一个类别$c_k$后，各个特征符合正态分布</li>
<li>$P\left(x^{(i)} \mid y=c_k\right)=\frac{1}{\sqrt{2 \pi \sigma_{y}^{2}}} \exp \left(-\frac{\left(x^{(i)}-\mu_{y}\right)^{2}}{2 \sigma_{y}^{2}}\right)$<ul>
<li>$\mu_y:$ 在类别为 $y$ 的样本中，特征 $x^{(i)}$ 的均值。<br>$\sigma_{y}:$ 在类别为 $y$ 的样本中，特征 $x^{(i)}$ 的标准差。</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="1-2-计算后验分布"><a href="#1-2-计算后验分布" class="headerlink" title="1.2 计算后验分布"></a>1.2 计算后验分布</h3><p>通过贝叶斯定理，得到在给定了一个特征向量的情况下，标签为$c_k$的概率。<br>$$<br>\begin{align}<br>P(Y \mid X)&amp;=\frac{P(X, Y)}{P(X)}=\frac{P(Y) P(X \mid Y)}{\sum_{Y} P(Y) P(X \mid Y)}\<br>P\left(Y=c_{k} \mid X=x\right)&amp;=\frac{P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}\<br>&amp;=\frac{P\left(Y=c_{k}\right) \prod P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum_{k} (P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right))}, \quad k=1,2, \cdots, K<br>\end{align}<br>$$</p>
<ul>
<li>把(2)带入到(5)，得到等式(6)</li>
</ul>
<h3 id="1-3-后验概率最大化"><a href="#1-3-后验概率最大化" class="headerlink" title="1.3 后验概率最大化"></a>1.3 后验概率最大化</h3><p>得到了概率表达式之后，我们便去寻找使得概率$P\left(Y=c_{k} \mid X=x\right)$最大的标签$c_k$</p>
<p>朴素贝叶斯分类器：<br>$$<br>\begin{array}{l}<br>y=f(x)=\arg \max <em>{c</em>{k}} \frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right) \ }{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}<br>\end{array}<br>$$<br>(7)中的分母本质上为$P(X=x)$，与$c_k$的取值无关，故(7)可表示成<br>$$<br>y=\arg \max <em>{c</em>{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)<br>$$</p>
<h4 id="1-3-1-含义"><a href="#1-3-1-含义" class="headerlink" title="1.3.1 含义"></a>1.3.1 含义</h4><ul>
<li>后验概率最大化等价于期望风险最小化</li>
</ul>
<h3 id="1-4-朴素贝叶斯法中的参数估计"><a href="#1-4-朴素贝叶斯法中的参数估计" class="headerlink" title="1.4 朴素贝叶斯法中的参数估计"></a>1.4 朴素贝叶斯法中的参数估计</h3><p>为了学习朴素贝叶斯法中的$P\left(Y=c_{k}\right)$与$P\left(X=x \mid Y=c_{k}\right)$</p>
<h4 id="1-4-1-极大似然估计"><a href="#1-4-1-极大似然估计" class="headerlink" title="1.4.1 极大似然估计"></a>1.4.1 极大似然估计</h4><p>核心思想为根据以有的测试数据集，得到每一类的频率，即为概率</p>
<blockquote>
<p>算法 4.1 (朴素贝叶斯算法 (Naive Bayes algorithm) ) </p>
<p>输入: 训练数据 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},$ 其中 $x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots,\right.$<br>$\left.x_{i}^{(n)}\right)^{\mathrm{T}}, x_{i}^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征, $x_{i}^{(j)} \in\left{a_{j 1}, a_{j 2}, \cdots, a_{j S_{j}}\right}, a_{j l}$ 是第 $j$ 个特<br>征可能取的第 $l$ 个值, $j=1,2, \cdots, n, l=1,2, \cdots, S_{j}, y_{i} \in\left{c_{1}, c_{2}, \cdots, c_{K}\right} ;$ 实例 $x$;<br>输出：实例 $x$ 的分类。<br>（1）计算先验概率及条件概率<br>$$<br>P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K<br>$$</p>
<p>$$<br>\begin{array}{c}<br>P\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right) }{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)} \<br>j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K<br>\end{array}<br>$$<br>（2）对于给定的实例 $x=\left(x^{(1)}, x^{(2)}, \cdots, x^{(n)}\right)^{\mathrm{T}},$ 计算<br>$$<br>P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right), \quad k=1,2, \cdots, K<br>$$<br>（3）确定实例 $x$ 的类<br>$$<br>y=\arg \max <em>{c</em>{k}} P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)<br>$$</p>
</blockquote>
<h4 id="1-4-2-贝叶斯估计"><a href="#1-4-2-贝叶斯估计" class="headerlink" title="1.4.2 贝叶斯估计"></a>1.4.2 贝叶斯估计</h4><p><strong>问题</strong>：通过极大似然估计，可能会出现(8)中的几个$P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)$为0（我们把“发票”当成了一个特征，但句子中没有出现“发票”这一词），这个情况很常见</p>
<p><strong>原因</strong>：训练集太少，没有满足大数定律。</p>
<p><strong>解决</strong>：</p>
<p>给定先验概率，赋值为$\lambda \ge0$</p>
<ul>
<li>在一个训练集中，对一个本应该为0 的概率赋予一个较小值，那么就会降低其他词语的概率</li>
<li>$\lambda=0$：极大似然估计</li>
<li>$\lambda=1$：Laplace Smoothing</li>
<li>”加上“基础的概率$1/K,;1/S_j$</li>
</ul>
<p><strong>算法</strong>：</p>
<ul>
<li><p>计算先验概率的等式(10)变成<br>$$<br>P_{\lambda}\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+\lambda}{N+K \lambda}<br>$$</p>
</li>
<li><p>计算条件概率的等式(11)变成<br>$$<br>P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits_{i=1}^NI(x_i^{j}=a_{jl},y_j=c_k)+\lambda}{\sum\limits_{i=1}^NI(y_j=c_k)+S_j\lambda}<br>$$</p>
</li>
</ul>
<h2 id="3-Trick"><a href="#3-Trick" class="headerlink" title="3. Trick"></a>3. Trick</h2><h3 id="3-1-Log-function"><a href="#3-1-Log-function" class="headerlink" title="3.1 Log function"></a>3.1 Log function</h3><p>在等式(12)中，我们可以取对数，把乘法运算转化为加法运算，提高计算速度。</p>
<h2 id="4-Coding"><a href="#4-Coding" class="headerlink" title="4. Coding"></a>4. Coding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayes</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.model=<span class="string">&#x27;Naive Bayes&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, Py, Pxi_y, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param Py: 先验概率</span></span><br><span class="line"><span class="string">        :param Pxi_y: 条件概率</span></span><br><span class="line"><span class="string">        :param x: test array</span></span><br><span class="line"><span class="string">        :return: 通过极大似然法得出最大概率的标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        (labelNumber,featureNumber,features)=Pxi_y.shape</span><br><span class="line">        <span class="comment"># p存放所有的似然</span></span><br><span class="line">        p=Py</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labelNumber):</span><br><span class="line">            <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(featureNumber):</span><br><span class="line">                <span class="built_in">sum</span> += Pxi_y[i][j][x[j]] <span class="comment"># 因为转化成了log，所以累乘变累加</span></span><br><span class="line">            p[i] += <span class="built_in">sum</span></span><br><span class="line">        <span class="keyword">return</span> np.argmax(p)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getAllProbability</span>(<span class="params">self, labelArray, labelNumber, trainArray, featureNumber, features, para=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param labelArray: [int,int,...,int] 1*n</span></span><br><span class="line"><span class="string">        :param labelNumber: 一共有N个标签</span></span><br><span class="line"><span class="string">        :param trainArray: train data, ~*m</span></span><br><span class="line"><span class="string">        :param featureNumber: 一共有m个特征标签</span></span><br><span class="line"><span class="string">        :param features: int k, 对第m个特征标签，有k_m个特征。</span></span><br><span class="line"><span class="string">        :param para: lambda</span></span><br><span class="line"><span class="string">        :return: 先验概率Py, 条件概率Pxi_y</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        目前（没有完善load function）对于输入的形式非常的苛刻，也出现了重复的参数，日后完善。</span></span><br><span class="line"><span class="string">        example:</span></span><br><span class="line"><span class="string">        trainArray=[[0,0],[0,1],[0,1],[0,0],[0,0],[1,0],[1,1],[1,1],[1,2],[1,2],[2,2],[2,1],[2,1],[2,2],[2,2]]</span></span><br><span class="line"><span class="string">        trainArray=np.asarray(trainArray)</span></span><br><span class="line"><span class="string">        featureNumber=2</span></span><br><span class="line"><span class="string">        features=3</span></span><br><span class="line"><span class="string">        labelArray=[0,0,1,1,0,0,0,1,1,1,1,1,1,1,0]</span></span><br><span class="line"><span class="string">        labelArray=np.asarray(labelArray)</span></span><br><span class="line"><span class="string">        labelNumber=2</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 一共有N个标签</span></span><br><span class="line">        Py = np.zeros((labelNumber, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 对标签为y，特征为m，第？个特征取值</span></span><br><span class="line">        Pxi_y = np.zeros((labelNumber, featureNumber, features))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算先验概率Py</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(labelNumber):</span><br><span class="line">            numerator = np.<span class="built_in">sum</span>(labelArray==i) + para</span><br><span class="line">            denominator = <span class="built_in">len</span>(labelArray) + para * labelNumber</span><br><span class="line">            Py[i] = numerator/denominator</span><br><span class="line">        Py = np.log(Py) <span class="comment"># 取对数加快运算</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算条件概率Pxi_y</span></span><br><span class="line">        (a,b) = trainArray.shape <span class="comment"># a:几条数据  b:几个特征</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(a): <span class="comment"># 第几条数据，选取他们的label和train data array</span></span><br><span class="line">            label = labelArray[i]</span><br><span class="line">            x=trainArray[i]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(featureNumber): <span class="comment"># 对x的每一个标签上的值，出现一次加一次</span></span><br><span class="line">                Pxi_y[label][j][x[j]] +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> <span class="built_in">range</span>(labelNumber): <span class="comment"># 得到次数后计算概念</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(featureNumber):</span><br><span class="line">                denominator = np.<span class="built_in">sum</span>(Pxi_y[label][i]) + features * para</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(features):</span><br><span class="line">                    numerator = Pxi_y[label][i][j] + para</span><br><span class="line">                    Pxi_y[label][i][j]=np.log(numerator/denominator)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Py, Pxi_y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loadData</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">modelTest</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<h2 id="5-Proof"><a href="#5-Proof" class="headerlink" title="5. Proof"></a>5. Proof</h2><h3 id="5-1-用极大似然法推出等式-10"><a href="#5-1-用极大似然法推出等式-10" class="headerlink" title="5.1 用极大似然法推出等式(10)"></a>5.1 用极大似然法推出等式(10)</h3><p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-4%20Proof%201.png" alt="253e0884011c7a303fda71c8962cfa6"></p>
<h2 id="6-Reference"><a href="#6-Reference" class="headerlink" title="6. Reference"></a>6. Reference</h2><p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a><a target="_blank" rel="noopener" href="https://github.com/Dod-o/Statistical-Learning-Method_Code">Dod-o/Statistical-Learning-Method_Code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Dod-o/Statistical-Learning-Method_Code">Dod-o/Statistical-Learning-Method_Code</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/han_xiaoyang/article/details/50629608">NLP系列(4)_朴素贝叶斯实战与进阶</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">fengdu78/lihang-code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/SmirkCao/Lihang">SmirkCao, Lihang, (2018), GitHub repository</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/27/[SLM-4]%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%20Naive%20Bayes/" data-id="ckivv4ekp00038cp4dcs96tdr" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-[SLM-3] K近邻法" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/11/23/%5BSLM-3%5D%20K%E8%BF%91%E9%82%BB%E6%B3%95/" class="article-date">
  <time class="dt-published" datetime="2020-11-23T12:51:59.423Z" itemprop="datePublished">2020-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="k-近邻法"><a href="#k-近邻法" class="headerlink" title="k 近邻法"></a>k 近邻法</h1><p>k-nearest neighbor (KNN)，一种基本的分类与回归方法。这里只介绍分类问题。<strong>k值的选择、距离度量、分类决策规则</strong>是KNN的3个基本要素。</p>
<h2 id="1-Algorithm"><a href="#1-Algorithm" class="headerlink" title="1. Algorithm"></a>1. Algorithm</h2><blockquote>
<p><strong>算法 $3.1(k$ 近邻法 $)$</strong><br>输入: 训练数据集<br>$$<br>\begin{aligned}<br>T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right}<br>\end{aligned}<br>$$<br>其中， $x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$ 为实例的特征向量, $y_{i} \in \mathcal{Y}=\left{c_{1}, c_{2}, \cdots, c_{K}\right}$ 为实例的类别， $i=1,2, \cdots, N ;$ 实例特征向量 $x$;<br>输出：实例 $x$ 所属的类 $y$。<br>(1) 根据给定的距离度量，在训练集 $T$ 中找出与 $x$ 最邻近的 $k$ 个点，涵盖这 $k$ 个点的 $x$ 的邻域记作 $N_{k}(x)$;<br>(2) 在 $N_{k}(x)$ 中根据分类决策规则 (如多数表决) 决定 $x$ 的类别 $y$ :<br>$$<br>y=\arg \max <em>{c</em>{j}} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right), \quad i=1,2, \cdots, N ; j=1,2, \cdots, K<br>$$<br>式 (1) 中， $I$ 为指示函数，即当 $y_{i}=c_{j}$ 时 $I$ 为 $1,$ 否则 $I$ 为 0 。</p>
</blockquote>
<ul>
<li>算法没有显式的学习过程。</li>
<li>例子：<ol>
<li>S市划分成住宅区、商业区、工业区（K=3），每个区域都有各自地标建筑，共N座。你的坐标为x，选取离你最近的k个地标建筑，k中包括哪类地标建筑最多，你就位于哪个区。</li>
</ol>
</li>
</ul>
<h2 id="2-Model"><a href="#2-Model" class="headerlink" title="2. Model"></a>2. Model</h2><ul>
<li><p>KNN的模型对应于对特征空间的划分。</p>
</li>
<li><p>在训练集、距离度量、k值、分类决策鬼册确定后，对任意新输入实例，所属类别唯一。</p>
</li>
<li><p>每一个训练实例点$x_i$，存在一个单元cell，在单元内到此$x_i$的距离最小。所有实例点的单元划分了特征空间。</p>
</li>
</ul>
<h3 id="2-1-距离度量"><a href="#2-1-距离度量" class="headerlink" title="2.1 距离度量"></a>2.1 距离度量</h3><p>计算特征空间中两点的距离。</p>
<p>$L_{p}$ (Minkowski) distance<br>$$<br>L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}<br>$$</p>
<ul>
<li><ul>
<li>设特征空间 $\mathcal{X}$ 是 $n$ 维实数向量空间 $x_{i}, x_{j} \in \mathbf{R}^{n}, x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}},$<br>$x_{j}=\left(x_{j}^{(1)}, x_{j}^{(2)}, \cdots, x_{j}^{(n)}\right)^{\mathrm{T}}$</li>
<li>$p\geq1$</li>
</ul>
</li>
<li>$p=1$，Manhattan distance</li>
<li>$p=2$， Euclidean distance</li>
<li>$p=\infin$，’Max’ distance</li>
</ul>
<h3 id="2-2-k值选择"><a href="#2-2-k值选择" class="headerlink" title="2.2 k值选择"></a>2.2 k值选择</h3><p>选取一些较小k值，通过<strong>交叉验证法</strong>选取最优k值。</p>
<ul>
<li>小k：<ul>
<li>近似误差减小，估计误差增大。对近邻实例点非常敏感</li>
<li>模型更复杂，容易过拟合。</li>
</ul>
</li>
<li>大k：<ul>
<li>近似误差增大，估计误差减小。较远错误标签实例点也能造成影响。</li>
<li>模型更简单。</li>
<li>$k=N$时，则为测试集最多类。</li>
</ul>
</li>
</ul>
<h3 id="2-3-分类决策规则"><a href="#2-3-分类决策规则" class="headerlink" title="2.3 分类决策规则"></a>2.3 分类决策规则</h3><p>一般为<strong>多数表决(majority voting rule)</strong></p>
<p>对给定的实例$x$，最近邻的$k$个训练实例点存在$c$个类别。为了使得分类错误的概率最小，所以选取类别中最多的那一类。</p>
<h2 id="3-Implement-kd-tree"><a href="#3-Implement-kd-tree" class="headerlink" title="3. Implement: kd tree"></a>3. Implement: kd tree</h2><p>线性扫描 (linear scan) 要计算实例与每一个训练实例的距离，耗时。</p>
<h3 id="3-1-kd树的构造"><a href="#3-1-kd树的构造" class="headerlink" title="3.1 kd树的构造"></a>3.1 kd树的构造</h3><p>选取一个训练实例点，构造一个垂直于某一轴的超平面，使得特征空间被分成左右两个子空间（左小右大）。在2个特征子空间中重复，直到子空间中没有训练实例点。</p>
<h4 id="3-1-1平衡kd树"><a href="#3-1-1平衡kd树" class="headerlink" title="3.1.1平衡kd树"></a>3.1.1平衡kd树</h4><ul>
<li>训练实例点的选取为选定坐标轴上的中位数。</li>
<li>效率不一定最优。</li>
</ul>
<blockquote>
<p>算法 3.2 (构造平衡 $k d$ 树） </p>
<p>输入: $k$ 维空间数据集 $T=\left{x_{1}, x_{2}, \cdots, x_{N}\right},$ 其中 $x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(k)}\right)^{\mathrm{T}},$<br>$i=1,2, \cdots, N$<br>输出: $k d$ 树。<br>（1）开始：构造根结点，根结点对应于包含 $T$ 的 $k$ 维空间的超矩形区域。<br>选择 $x^{(1)}$ 为坐标轴，以 $T$ 中所有实例的 $x^{(1)}$ 坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 $x^{(1)}$ 垂直的超平面实现。<br>由根结点生成深度为 1 的左、右子结点: <strong>左子结点对应坐标 $x^{(1)}$ 小于切分点的子 区域，右子结点对应于坐标 $x^{(1)}$ 大于切分点的子区域</strong>。<br>将落在切分超平面上的实例点保存在根结点。<br>（2）重复: 对深度为 $j$ 的结点，选择 $x^{(l)}$ 为切分的坐标轴， $l=j(\bmod k)+1,$ 以该结点的区域中所有实例的 $x^{(l)}$ 坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 $x^{(l)}$ 垂直的超平面实现。<br>由该结点生成深度为 $j+1$ 的左、右子结点: 左子结点对应坐标 $x^{(l)}$ 小于切分点 的子区域，右子结点对应坐标 $x^{(l)}$ 大于切分点的子区域。<br>将落在切分超平面上的实例点保存在该结点。</p>
<p>（3）直到两个子区域没有实例存在时停止。从而形成 $k d$ 树的区域划分.</p>
</blockquote>
<p><strong>构造过程图示</strong>：</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-3%20%E5%B9%B3%E8%A1%A1kd%E6%A0%91%E5%9B%BE%E7%A4%BA.png" alt="SLM-3 平衡kd树图示"></p>
<ul>
<li>按照黄、绿、蓝、红，垂直x，y，x，y的顺序切分。</li>
<li>左右节点确定：根据在第$l$维度上2个子节点对应坐标大小确定，小左大右。</li>
<li>$l=j(\bmod k)+1$：在k维方向都按顺序切上之后，回过头在循环切。<ul>
<li>公式细节因初始根节点的深度是0或1而有不同</li>
</ul>
</li>
<li>停止条件：所有实例点都位于一个超平面上。</li>
<li>偶数个数的中位数：自己确定，代码上别忘了。</li>
</ul>
<h3 id="3-2-kd树的搜索"><a href="#3-2-kd树的搜索" class="headerlink" title="3.2 kd树的搜索"></a>3.2 kd树的搜索</h3><h4 id="3-2-1-k-1"><a href="#3-2-1-k-1" class="headerlink" title="3.2.1 $k=1$"></a>3.2.1 $k=1$</h4><blockquote>
<p>算法 3.3 (用 $k d$ 树的最近邻搜索)<br>输入: 已构造的 $k d$ 树，目标点 $x$;<br>输出: $x$ 的最近邻，$k=1$。<br>(1) 在 $k d$ 树中找出包含目标点 $x$ 的叶结点: 从根结点出发，递归地向下访问 $k d$ 树。若目标点 $x$ 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。<br>(2) 以此叶结点为“当前最近点”。<br>(3) 递归地向上回退，在每个结点进行以下操作:<br>        (a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点 为“当前最近点”。<br>        (b）当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”问的距离为半径的超球体相交。</p>
<p>​        如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索;<br>​        如果不相交，向上回退。<br>(4）当回退到根结点时，搜索结束。最后的“当前最近点”即为 $x$ 的最近邻点。</p>
</blockquote>
<h4 id="3-2-2-k-gt-1"><a href="#3-2-2-k-gt-1" class="headerlink" title="3.2.2 $k&gt;1$"></a>3.2.2 $k&gt;1$</h4><blockquote>
<p>算法 3.04 (用 $k d$ 树的最近邻搜索)<br>输入: 已构造的 $k d$ 树，目标点 $x$;<br>输出: $x$ 的k最近邻。</p>
<ol>
<li><p>根据p的坐标和kd树的结点向下进行搜索 (如果树的结点是以 $x^{(l)}=c$ 来切分的, 那么如果p的 $x^{(l)}$ 坐标小于c, 则走左子结点, 否则走右子结点)</p>
</li>
<li><p>到达叶子结点时，将其标记为已访问。如果S中不足k个点, 则将该结点加入到S中; 如果S不空且当前结点与p点的距离小于S中最长的距离，则用当前结点替换S中离p最远的点</p>
</li>
<li><p>如果当前结点不是根节点, 执行（a）; 否则，结束算法</p>
<p>(a). 回退到当前结点的父结点, 此时的结点为当前结点 (回退之后的结点) ，将当前结点标 记为已访问, 执行 (b) 和（c) ; 如果当前结点已经被访过, 再次执行（a）。</p>
<p>(b). 如果此时S中不足k个点, 则将当前结点加入到S中; 如果S中已有k个点, 且当前结点与p 点的距离小于S中最长距离，则用当前结点替换S中距离最远的点。</p>
<p>(c). 计算p点和当前结点切分线的距离。如果该距离大于等于S中距离p最远的距离并且S中已 有k个点, 执行3; 如果该距离小于S中最远的距离或S中没有k个点, 从当前结点的另一子节点开始执行1; 如果当前结点没有另一子结点, 执行3。</p>
</li>
</ol>
</blockquote>
<p><strong>算法3.04 (用 $k d$ 树的最近邻搜索)搜索过程</strong>：目标点P（-1，-5），k=3，S：存储k个近邻点。初始所有点=[0, 0] ，即 [未访问, 不在S中]</p>
<p><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-3%20%E5%B9%B3%E8%A1%A1kd%E6%A0%91%E5%9B%BE%E7%A4%BA.png" alt="SLM-3 平衡kd树图示"></p>
<blockquote>
<ol>
<li>执行算法1：<ul>
<li>(-1,-5)的-1与A点(6,5)的6小，往左到B</li>
<li>-5&lt;-3，往左到D</li>
<li>D子节点唯一，到H</li>
</ul>
</li>
<li>执行算法2：<ul>
<li>H=[1, 0]，H=[1, 1]</li>
</ul>
</li>
<li>执行算法3：<ul>
<li>执行(a)，到D。D=[1, 0]</li>
<li>执行(b)，D=[1, 1]</li>
<li>执行(c)，P到D的切分线$x^1=-6$距离为5。S未满。D没有另一子节点。</li>
</ul>
</li>
<li>执行算法3：<ul>
<li>执行(a)，到B。B=[1, 0]</li>
<li>执行(b)，B=[1,1]，S={H, D, B}</li>
<li>执行(c)，P到B的切分线$x^2=-3$距离为2，小于S的最大距离。从B另一子节点E执行算法1.</li>
</ul>
</li>
<li>执行算法1：<ul>
<li>-1&gt;-2，往右到J</li>
</ul>
</li>
<li>执行算法2：<ul>
<li>J=[1,0]，d(J, P)大于S的最大距离。不加入</li>
</ul>
</li>
<li>执行算法3：<ul>
<li>执行(a)，到E，E=[1, 0]</li>
<li>执行(b)，d(E,P)小于S的最大距离。E=[1, 1]，H=[1, 0]，S={E, D, B}</li>
<li>执行(c)，P到E的切分线$x^1=-2$距离为1，小于S的最大距离。从E另一子节点I执行算法1.</li>
</ul>
</li>
<li>执行算法1：<ul>
<li>I是子节点，到I</li>
</ul>
</li>
<li>执行算法2：<ul>
<li>I=[1, 0]，d(I, P)大于S的最大距离。不加入。</li>
</ul>
</li>
<li>执行算法3：<ul>
<li>执行若干(a)，到A，A=[1, 0]</li>
<li>执行(b)，d(A, P)大于S的最大距离。</li>
<li>执行(c)，P到E的切分线$x^1=6$距离为7，大于最长距离。不加入。</li>
</ul>
</li>
<li>执行算法3：<ul>
<li>A是根节点。算法结束。已按顺序访问{H, D, B, J, E, I, A}，最终结果S={E, D, B}</li>
</ul>
</li>
</ol>
</blockquote>
<h3 id="3-3-Summary"><a href="#3-3-Summary" class="headerlink" title="3.3 Summary"></a>3.3 Summary</h3><ol>
<li><p>kd Tree的平均复杂度$O(logN)$, $N$为训练实例数。适合训练实例数远大于空间维数的KNN。</p>
</li>
<li><p>把k维大空间多次对半分成k维小空间。</p>
</li>
<li><p>目标点与实例点的距离$\geq$目标点与该实例点对应切分超平面的距离。因此：</p>
<ol>
<li>当此实例点当前应该替换S中某点从而加入到S集合中时，另一半空间存在子空间，子空间的点到目标点的距离小于实例点到目标点的距离。<ol>
<li>此时如果存在实例点位于子空间，那么应该继续循环（例子中没有体现）。</li>
<li>没有实例点在此子空间，那么另一半的空间的实例点都不会加入到集合S中。</li>
</ol>
</li>
<li>这个实例点不应该加入到S中，情况如2.1.2</li>
</ol>
</li>
<li><p>动态规划DP的思想？</p>
</li>
<li><p>别人笔记的原话：</p>
<p>1、找到叶子结点，看能不能加入到S中</p>
<p>2、回退到父结点，看父结点能不能加入到S中</p>
<p>3、看目标点和回退到的父结点切分线的距离，判断另一子结点能不能加入到S中</p>
</li>
</ol>
<h2 id="4-Question"><a href="#4-Question" class="headerlink" title="4. Question"></a>4. Question</h2><ol>
<li>kd方差的代码！<ul>
<li>有点点难搞-_-</li>
</ul>
</li>
<li>非平衡/方差kd树<ul>
<li>同上，难搞</li>
</ul>
</li>
</ol>
<h2 id="5-Code"><a href="#5-Code" class="headerlink" title="5. Code"></a>5. Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, train_set, label_set</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param train_set: m*n ndarray, m:dims, n:train points</span></span><br><span class="line"><span class="string">        :param label_set: 1*n int ndarray and from 0 to n-1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.train_set = train_set</span><br><span class="line">        self.label_set = label_set</span><br><span class="line">        self.m, self.n = self.train_set.shape</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">knn_naive</span>(<span class="params">self, test_set, k=<span class="number">1</span>,p=<span class="number">2</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param test_set: m*n ndarray, m:dims, n:test points</span></span><br><span class="line"><span class="string">        :param k: k nearest neighbor</span></span><br><span class="line"><span class="string">        :param p: order of norm</span></span><br><span class="line"><span class="string">        :return: 1*n list for n points</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        (m,n)=test_set.shape</span><br><span class="line">        <span class="comment"># 有几个点，输出对于shape的list</span></span><br><span class="line">        final=[-<span class="number">1</span>]*n</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="comment"># 转置后取点再转置</span></span><br><span class="line">            test_point=test_set.T[i].T</span><br><span class="line">            <span class="comment"># 有些可能是(m,)的ndarray</span></span><br><span class="line">            test_point=test_point.reshape([m,<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 计算距离</span></span><br><span class="line">            distance=np.linalg.norm(self.train_set-test_point,<span class="built_in">ord</span>=p,axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 最近k个点的index</span></span><br><span class="line">            nearestK=np.argsort(distance)[:k]</span><br><span class="line">            <span class="comment"># label是0，1，2，...的顺序，存储对应label出现的总次数</span></span><br><span class="line">            labelList=[<span class="number">0</span>]*(<span class="built_in">max</span>(self.label_set) + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> nearestK:</span><br><span class="line">                labelList[<span class="built_in">int</span>(self.label_set[index])] +=<span class="number">1</span></span><br><span class="line">                point_label=labelList.index(<span class="built_in">max</span>(labelList))</span><br><span class="line">            final[i]=point_label</span><br><span class="line">        <span class="keyword">return</span> final</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">knn_kdtree</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span>(<span class="params">self, points=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        画不同k从而分割成不同子%tb域的图</span></span><br><span class="line"><span class="string">        画结果图</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> self.m != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;Unable to draw a picture&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
<li>Github求start！！！^_^</li>
</ul>
<h2 id="6-Reference"><a href="#6-Reference" class="headerlink" title="6. Reference"></a>6. Reference</h2><p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zzpzm/article/details/88565645">KNN算法和kd树详解（例子+图示）</a></p>
<p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict">sklearn</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">fengdu78/lihang-code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Dod-o/Statistical-Learning-Method_Code">Dod-o/Statistical-Learning-Method_Code</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/23/[SLM-3]%20K%E8%BF%91%E9%82%BB%E6%B3%95/" data-id="ckivv4eki00018cp43e9sg4xu" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-[SLM-1] 统计学习及监督学习概论" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/11/23/%5BSLM-1%5D%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" class="article-date">
  <time class="dt-published" datetime="2020-11-23T08:58:58.413Z" itemprop="datePublished">2020-11-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/23/[SLM-1]%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" data-id="ckivv4ek200008cp4h913dl26" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-[SLM-2] 感知机" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/11/22/%5BSLM-2%5D%20%E6%84%9F%E7%9F%A5%E6%9C%BA/" class="article-date">
  <time class="dt-published" datetime="2020-11-22T02:49:46.032Z" itemprop="datePublished">2020-11-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><p>感知机（perceptron）是<strong>二类线性分类</strong>模型。</p>
<h2 id="1-Model"><a href="#1-Model" class="headerlink" title="1. Model"></a>1. Model</h2><blockquote>
<p>定义 2.1 (感知机) $\quad$假设输入空间（特征空间 ) 是 $\mathcal{X} \subseteq \mathbf{R}^{n},$ 输出空间是 $\mathcal{Y}={+1,-1}_{\circ}$ 输入 $x \in \mathcal{X}$ 表示实例的特征向量，对应于输入空间 ( 特征空间 $)$ 的点; 输出 $y \in \mathcal{Y}$ 表示实例的类别。由输入空间到输出空间的如下函数:<br>$$<br>f(x)=\operatorname{sign}(w \cdot x+b)<br>$$<br>称为感知机。其中， $w$ 和 $b$ 为感知机模型参数, $w \in \mathbf{R}^{n}$ 叫作权值（weight ) 或权值向量（weight vector) $, b \in \mathbf{R}$ 叫作偏置 ( bias $), w \cdot x$ 表示 $w$ 和 $x$ 的内积。sign 是符号 函数，即<br>$$<br>\operatorname{sign}(x)=\left{\begin{array}{ll}<br>+1, &amp; x \geqslant 0 \<br>-1, &amp; x&lt;0<br>\end{array}\right.<br>$$</p>
</blockquote>
<ul>
<li><p>假设空间：特征空间中的所有线性分类模型，即函数集合${f|f(x)=w \cdot x+b}$</p>
</li>
<li><p>几何解释：线性方程$w \cdot x+b=0$是特征空间$\mathbf{R}^{n}$的一个超平面$\mathbf{S}$，把特征空间分成2个部分，使得特征向量分别划入正负两类。$\mathbf{S}$也叫分离超平面（separating hyperplane)</p>
<ul>
<li>超平面$\mathbf{S}\subseteq \mathbf{R}^{n-1}$并且截距为0。在这需要把b当成特征而不是截距才说的通。</li>
</ul>
</li>
<li><p>例子：</p>
<ul>
<li>通过${房屋面积，房龄，…}$来判断房子总价是否高于价格$a$<ul>
<li>假设数据集线性可分</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-Strategy"><a href="#2-Strategy" class="headerlink" title="2. Strategy"></a>2. Strategy</h2><h3 id="2-1-数据可分性"><a href="#2-1-数据可分性" class="headerlink" title="2.1 数据可分性"></a>2.1 数据可分性</h3><blockquote>
<p>定义 2.2 (数据集的线性可分性) $\quad$ 给定一个数据集<br>$$<br>T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right}<br>$$<br>其中， $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}={+1,-1}, i=1,2, \cdots, N,$ 如果存在某个超平面 $S$<br>$$<br>w \cdot x+b=0<br>$$<br>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有<br>$y_{i}=+1$ 的实例 $i,$ 有 $w \cdot x_{i}+b&gt;0,$ 对所有 $y_{i}=-1$ 的实例 $i,$ 有 $w \cdot x_{i}+b&lt;0,$ 则 称数据集 $T$ 为线性可分数据集（linearly separable data set ) ; 否则，称数据集 $T$ 线性<br>不可分。</p>
</blockquote>
<ul>
<li>存在超平面$\mathbf{S}\Longrightarrow $数据集线性可分</li>
</ul>
<h3 id="2-2-学习策略"><a href="#2-2-学习策略" class="headerlink" title="2.2 学习策略"></a>2.2 学习策略</h3><h4 id="2-2-1-误分类点总数"><a href="#2-2-1-误分类点总数" class="headerlink" title="2.2.1 误分类点总数"></a>2.2.1 误分类点总数</h4><p>把误分类点总数当成损失函数，不是参数$w,b$的连续可导函数，不易优化</p>
<h4 id="2-2-2-误分类点到超平面的总距离"><a href="#2-2-2-误分类点到超平面的总距离" class="headerlink" title="2.2.2 误分类点到超平面的总距离"></a>2.2.2 误分类点到超平面的总距离</h4><blockquote>
<p>定理（2.01) $\quad$空间  $\mathbf{R}^n$中任意一点$x_0$到超平面  $\mathbf{S}={x|w\cdot x+b=0}$ 的距离为<br>$$<br>\frac{1}{|w|}\left|w \cdot x_{0}+b\right|<br>$$</p>
</blockquote>
<p>对于误分类数据$(x_i,y_i)$，M为误分类点集合，误分类点到超平面的总距离为<br>$$<br>\begin{align}<br>D&amp;=\frac{1}{|w|} \sum_{x_{i} \in M} |y_{i}\left(w \cdot x_{i}+b\right)|\nonumber\<br>&amp;=-\frac{1}{|w|} \sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)\<br>L(w,b)&amp;=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)<br>\end{align}<br>$$</p>
<ul>
<li>对误分类点$(x_i,y_i)$，$-y_{i}\left(w \cdot x_{i}+b\right) \geq0$</li>
<li>(6)是给定数据集$T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right}$，其中$x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}={+1,-1}, i=1,2, \cdots, N$</li>
<li>(7)非负函数，且对于$w,b$连续可导</li>
<li>我们的策略是<strong>极小化损失函数</strong>，即$\min <em>{w, b} L(w, b)=-\sum</em>{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)$</li>
</ul>
<h2 id="3-Algorithm"><a href="#3-Algorithm" class="headerlink" title="3. Algorithm"></a>3. Algorithm</h2><p>求解损失函数(7)的最优化问题，通过随机梯度下降(stochastic gradient descent)</p>
<h3 id="3-1-算法原始形式"><a href="#3-1-算法原始形式" class="headerlink" title="3.1 算法原始形式"></a>3.1 算法原始形式</h3><blockquote>
<p>算法 2.1（感知机学习算法的原始形式）<br>输入: 训练数据集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},$ 其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in$$\mathcal{Y}={-1,+1}, i=1,2, \cdots, N ;$ 学习率 $\eta(0&lt;\eta \leqslant 1)$;<br>输出 $: w, b ;$ 感知机模型 $f(x)=\operatorname{sign}(w \cdot x+b)$ 。<br>(1) 选取初值 $w_{0}, b_{0}$;<br>(2)在训练集中随机选取数据$\left(x_{i}, y_{i}\right)$;<br>(3) 如果 $y_{i}\left(w \cdot x_{i}+b\right) \leqslant 0$,<br>$$<br>\begin{align}<br>w&amp; \leftarrow w+\eta y_{i} x_{i} \<br>b &amp;\leftarrow b+\eta y_{i}<br>\end{align}<br>$$<br>(4）转至 (2)，直至训练集中没有误分类点。</p>
</blockquote>
<ul>
<li><p>一次随机选取一个误分类点使其梯度下降</p>
</li>
<li><p><strong>随机</strong>选取一个误分类点 $\left(x_{i}, y_{i}\right),$ 对 $w, b$ 进行更新 :<br>$$<br>\begin{aligned}<br>w &amp;\leftarrow w+\eta y_{i} x_{i} \<br>b &amp;\leftarrow b+\eta y_{i}<br>\end{aligned}<br>$$</p>
</li>
<li><p>不同初值，不同分类点选取会影响最后的解</p>
</li>
</ul>
<h3 id="3-2-原始算法的收敛性"><a href="#3-2-原始算法的收敛性" class="headerlink" title="3.2 原始算法的收敛性"></a>3.2 原始算法的收敛性</h3><p>证明对一个线性可分的数据集，感知机学习算法的原始形式在有限次迭代后能得到正确的分离超平面与感知机模型</p>
<blockquote>
<p><strong>定理 2.1 (Novikoff)</strong> 设训练数据集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right}$ 是线 性可分的，其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}={-1,+1}, i=1,2, \cdots, N,$ 则<br>(1) 存在满足条件 $\left|\hat{w}<em>{\mathrm{opt}}\right|=1$ 的超平面 $\hat{w}</em>{\mathrm{opt}} \cdot \hat{x}=w_{\mathrm{opt}} \cdot x+b_{\mathrm{opt}}=0$ 将训练数据集完全正确分开; 且存在 $\gamma&gt;0,$ 对所有 $i=1,2, \cdots, N$<br>$$<br>y_{i}\left(\hat{w}<em>{\mathrm{opt}} \cdot \hat{x}</em>{i}\right)=y_{i}\left(w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right) \geqslant \gamma<br>$$<br>(2) 令 $R=\max <em>{1 \leqslant i \leqslant N}\left|\hat{x}</em>{i}\right|,$ 则感知机算法 2.1 在训练数据集上的误分类次数 $k$ 满足不等式<br>$$<br>k \leqslant\left(\frac{R}{\gamma}\right)^{2}<br>$$</p>
</blockquote>
<ul>
<li>$\hat{w}=\left(w^{\mathrm{T}}, b\right)^{\mathrm{T}}$，$\hat{x}=\left(x^{\mathrm{T}}, 1\right)^{\mathrm{T}}$，$\hat{x} \in \mathbf{R}^{n+1}, \hat{w} \in \mathbf{R}^{n+1}$，$\hat{w} \cdot \hat{x}=w \cdot x+b$</li>
<li>对于线性可分数据集，感知机的解存在但不唯一。</li>
<li>在线性支持向量机中，添加超平面约束条件，从而得到唯一超平面。</li>
</ul>
<h3 id="3-3-算法对偶形式"><a href="#3-3-算法对偶形式" class="headerlink" title="3.3 算法对偶形式"></a>3.3 算法对偶形式</h3><p>我们假设$w,b=0$，在原始算法中，对于一个误分类点$(x_i,y_i)$，我们进行了$n_i$次更新，对于所有$N$个点，我们有<br>$$<br>\begin{aligned}<br>w &amp;=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} =\sum_{i=1}^{N} n_i(\eta  y_{i} x_{i})\<br>b &amp;=\sum_{i=1}^{N} \alpha_{i} y_{i}=\sum_{i=1}^{N} n_i(\eta y_i)<br>\end{aligned}<br>$$<br>我们从解w，b转化为$n_i$即第$i$个是实例点由于误分而进行更新的次数。</p>
<p>实例点更新越多，说明距离分离超平面越近，越难分类，对结果影响大，很可能就是支持向量。</p>
<blockquote>
<p>算法 2.2 (感知机学习算法的对偶形式)<br>输入: 线性可分的数据集 $T=\left{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right},$ 其中 $x_{i} \in \mathbf{R}^{n}, y_{i} \in$<br>${-1,+1}, i=1,2, \cdots, N ;$ 学习率 $\eta(0&lt;\eta \leqslant 1) ;$<br>输 出: $\alpha, b ;$ 感 知机 模 型 $f(x)=\operatorname{sign}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x+b\right),$ 其中 $\alpha=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}$<br>(1) $\alpha \leftarrow 0, b \leftarrow 0 ;$<br>(2)在训练集中选取数据 $\left(x_{i}, y_{i}\right)$;<br>(3)如果 $y_{i}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i}+b\right) \leqslant 0$<br>$$<br>\begin{array}{l}<br>\alpha_{i} \leftarrow \alpha_{i}+\eta \<br>b \leftarrow b+\eta y_{i}<br>\end{array}<br>$$<br>(4)转至 (2)直到没有误分类数据。</p>
</blockquote>
<ul>
<li><p>(12)当$\eta=1$，即$n_i=n_i+1$，对点更新次数加一</p>
</li>
<li><p>对偶算法只涉及到矩阵的内积$x_i\cdot x_j$计算，我们可以将内积预先计算存储，即我们计算Gram矩阵$\mathbf{G}=[x_i \cdot x_j]_{\mathbf{N}\times\mathbf{N}}$</p>
</li>
<li><p>和原始形式一样，感知机学习算法的对偶形式迭代是收敛的，且存在多个解。</p>
</li>
</ul>
<h2 id="4-Question"><a href="#4-Question" class="headerlink" title="4. Question"></a>4. Question</h2><ol>
<li><p>超平面的定义是什么？</p>
</li>
<li><p>这么判断数据集可不可分？</p>
<ul>
<li><p>证明以下定理： 样本集线性可分的充分必要条件是正实例点集所构成的凸壳CD 与负实例点集所构成的凸壳互不相交。</p>
</li>
<li><p>设集合 $S \subset \mathbf{R}^{n}$ 是由 $\mathbf{R}^{n}$ 中的 $k$ 个点所组成的集合, 即 $S=\left{x_{1}, x_{2}, \cdots, x_{k}\right} .$ 定义 $S$ 的凸亮$\operatorname{conv}(S)$ 为<br>$$<br>\begin{aligned}<br>\operatorname{conv}(S)=\left{x=\sum_{i=1}^{k} \lambda_{i} x_{i} \mid \sum_{i=1}^{k} \lambda_{i}=1, \lambda_{i} \geqslant 0, i=1,2, \cdots, k\right}<br>\end{aligned}<br>$$</p>
</li>
</ul>
</li>
</ol>
<h2 id="5-Code"><a href="#5-Code" class="headerlink" title="5. Code"></a>5. Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">perceptron</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, X_train, Y_train, learning_rate=<span class="number">0.0001</span>, tol=<span class="number">0</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        X_train:Array, default=None</span></span><br><span class="line"><span class="string">            dataset for training</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Y_train:Array, default=None</span></span><br><span class="line"><span class="string">            labelset for training</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        learning_rate:float,default=0.0001</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        tol:int, default=0</span></span><br><span class="line"><span class="string">            the stopping criterion. When the number of misclassification </span></span><br><span class="line"><span class="string">            points small or equal to tol, stop training.</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.X_train=np.mat(X_train)</span><br><span class="line">        self.Y_train=np.mat(Y_train).T</span><br><span class="line">        self.m,self.n=np.shape(self.X_train)</span><br><span class="line">        self.w=np.zeros((<span class="number">1</span>,np.shape(self.X_train)[<span class="number">1</span>]))</span><br><span class="line">        self.b=<span class="number">0</span></span><br><span class="line">        self.learning_rate=learning_rate</span><br><span class="line">        self.tol=tol</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x, w, b</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(x,w.T)+b</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        w: np.mat</span></span><br><span class="line"><span class="string">            weights</span></span><br><span class="line"><span class="string">        b: np.mat</span></span><br><span class="line"><span class="string">            bias        </span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        is_wrong=<span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> is_wrong:</span><br><span class="line">            wrong_count=<span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.m):</span><br><span class="line">                xi=self.X_train[s]</span><br><span class="line">                yi=self.Y_train[s]</span><br><span class="line">                <span class="keyword">if</span> self.calculate(xi,self.w,self.b)*yi &lt;= <span class="number">0</span>:</span><br><span class="line">                    self.w=self.w+self.learning_rate*yi*xi</span><br><span class="line">                    self.b=self.b+self.learning_rate*yi</span><br><span class="line">                    wrong_count+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> wrong_count &lt;=self.tol:</span><br><span class="line">                is_wrong=<span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> self.w,self.b</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChongfengLing/Statistical-Learning-Method-Notes-Code">More details and examples</a></li>
</ul>
<h2 id="6-Proof"><a href="#6-Proof" class="headerlink" title="6. Proof"></a>6. Proof</h2><h3 id="6-1-Theorem-2-01"><a href="#6-1-Theorem-2-01" class="headerlink" title="6.1 Theorem 2.01"></a>6.1 Theorem 2.01</h3><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-2%20thm2.01%20distance" alt="image-20201123163513240" style="zoom:50%;" />

<h3 id="6-2-Theorem-2-1"><a href="#6-2-Theorem-2-1" class="headerlink" title="6.2 Theorem 2.1"></a>6.2 Theorem 2.1</h3><img src="https://raw.githubusercontent.com/ChongfengLing/typora-picBed/main/img/SLM-2%20thm2.1%20Novikoff.png" alt="SLM-2 thm2.1 Novikoff" style="zoom:50%;" />

<h2 id="7-Reference"><a href="#7-Reference" class="headerlink" title="7. Reference"></a>7. Reference</h2><p><a target="_blank" rel="noopener" href="https://book.douban.com/subject/33437381/">统计学习方法（第2版）</a></p>
<p><a target="_blank" rel="noopener" href="https://www.pkudodo.com/2018/11/18/1-4/">统计学习方法|感知机原理剖析及实现</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26526858/answer/136577337">如何理解感知机学习算法的对偶形式？ - Zongrong Zheng的回答 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">fengdu78/lihang-code</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/22/[SLM-2]%20%E6%84%9F%E7%9F%A5%E6%9C%BA/" data-id="ckivv4eko00028cp480ho6jhe" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/12/19/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2020/12/11/%5BSLM-7%5D%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%20SVM/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/12/02/%5BSLM-5%5D%20%E5%86%B3%E7%AD%96%E6%A0%91%20Decision%20Tree/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/11/27/%5BSLM-4%5D%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%20Naive%20Bayes/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/11/23/%5BSLM-3%5D%20K%E8%BF%91%E9%82%BB%E6%B3%95/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>